{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stemmer","stopWordFilter","trimmer"]},"docs":[{"location":"","title":"Prompt engineering &amp; Architecture &amp; SW development","text":"<p>This website is my personal site about prompt engineering, sw architecture and sw development. I will add content shortly.</p>"},{"location":"gemma/install/","title":"How to Run Gemma-7b-it Locally Using llama.cpp and GGUF","text":""},{"location":"gemma/install/#prerequisite","title":"Prerequisite","text":"<p>Some basic knowledge of how to build with CMake and how to clone a repository using Git is required.</p> <p>Python3 must be installed on your local machine.</p> <p>A prerequisite for this tutorial is having llama.cpp built on your local machine. If not, please go here and follow the guides there. Don't forget to install requirements for Python scripts using the <code>requirements.txt</code> file.</p>"},{"location":"gemma/install/#download-gemma-7b-it","title":"Download Gemma-7b-it","text":"<p>To download the Gemma-7b-it model, go to Hugging Face and clone the repository. The repository is located here.</p> <p>Follow these instructions to clone:</p> <pre><code>git lfs install\ngit clone https://huggingface.co/google/gemma-7b-it\n</code></pre> <p>Depending on your Linux distribution, you might need to install Git and Git-lfs. Note that Git LFS (Large File Storage) is dependent on the distribution.</p>"},{"location":"gemma/install/#prompt-format","title":"Prompt Format","text":"<p>For Gemma, you can use the following prompt template:</p> <pre><code>&lt;bos&gt;&lt;start_turn&gt;user\n{prompt}&lt;end_turn&gt;\n&lt;start_turn&gt;model\n</code></pre>"},{"location":"gemma/install/#use-the-provided-fp32-gguf","title":"Use the Provided fp32 GGUF","text":"<p>You can use the provided fp32 file in the repo, e.g., <code>gemma-7b-it.gguf</code>. This is an fp32 model.</p> <p>To run it via the main program, navigate to your build directory in llama.cpp and from the <code>bin</code> directory, run:</p> <pre><code>./main -m &lt;path to gemma dir&gt;/gemma-7b-it.gguf -ngl &lt;number of layers to offload to gpu&gt; -c &lt;context size, max is 8192&gt; -p &lt;prompt&gt;\n</code></pre> <p>To run it as a server:</p> <pre><code>./server -m &lt;path to gemma dir&gt;/gemma-7b-it.gguf -ngl &lt;number of layers to offload to gpu&gt; -c &lt;context size, max is 8192&gt;\n</code></pre> <p>To simulate a client, you can use curl:</p> <pre><code>curl -X POST \"http://localhost:8080/completion\" -H \"Content-Type:application/json\" -d '{\"prompt\":&lt;your prompt&gt;, \"n_predict\":&lt;number of tokens to predict&gt;, \"temperature\":&lt;temperature&gt;}' | jq .content | xargs echo -e\n</code></pre>"},{"location":"gemma/install/#convert-model-to-fp16-from-repo-safetensors","title":"Convert Model to fp16 from Repo Safetensors","text":"<p>To convert it to fp16, use the following Python command from the root llama.cpp directory:</p> <pre><code>python3 convert-hf-to-gguf.py &lt;path to gemma root dir&gt;/gemma-7b-it --outtype f16 --outfile \"&lt;path where you want to store your gguf&gt;/gemma-7b-it-f16.gguf\"\n</code></pre>"},{"location":"gemma/install/#quantize-fp32-fp16-to-q8_0","title":"Quantize fp32, fp16 to Q8_0","text":"<p>To quantize fp32, navigate to the llama.cpp build directory and from the bin directory:</p> <pre><code>./quantize &lt;path to gemma directory&gt;/gemma-7b-it.gguf &lt;output path&gt;/gemma-7b-it-q8.gguf Q8_0\n</code></pre> <p>Similarly, to quantize fp16, navigate to the llama.cpp build directory and from the bin directory:</p> <pre><code>./quantize &lt;path where you stored your fp16 gguf&gt;/gemma-7b-it-f16.gguf &lt;output path&gt;/gemma-7b-it-q8.gguf Q8_0\n</code></pre>"},{"location":"gemma/install/#issues","title":"Issues","text":""},{"location":"gemma/install/#hf-authorization","title":"HF Authorization","text":"<p>You might encounter issues when cloning the repository. To gain access, you must accept the license from Google first. Then, when cloning, you might face an authentication challenge. A helpful solution was to use the Hugging Face Hub CLI and authorize using a Hugging Face token.</p>"},{"location":"gemma/install/#quality","title":"Quality","text":"<p>In general, the quality ranking is f32 &gt; f16 &gt; Q8 &gt; Q...</p>"},{"location":"gemma/install/#speed","title":"Speed","text":"<p>Generally, the speed ranking is Q2 &gt; Q3 &gt; ... &gt; Q8 &gt; f16 &gt; f32</p>"}]}