
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.3, mkdocs-material-9.5.1">
    
    
      
        <title>Usenix atc 2023 v2 - Architect's Insight Hub: Elevate Your Learning Experience</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.45e1311d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#usenix-atc-23-and-osdi-23-joint-keynote-address-sky-computing" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Architect&#39;s Insight Hub: Elevate Your Learning Experience" class="md-header__button md-logo" aria-label="Architect's Insight Hub: Elevate Your Learning Experience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Architect's Insight Hub: Elevate Your Learning Experience
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Usenix atc 2023 v2
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Architect&#39;s Insight Hub: Elevate Your Learning Experience" class="md-nav__button md-logo" aria-label="Architect's Insight Hub: Elevate Your Learning Experience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Architect's Insight Hub: Elevate Your Learning Experience
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../tags.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tags
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-and-osdi-23-joint-keynote-address-sky-computing" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 and OSDI '23 Joint Keynote Address - Sky Computing
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-bifrost-analysis-and-optimization-of-network-io-tax-in-confidential-virtual" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Bifrost: Analysis and Optimization of Network I/O Tax in Confidential Virtual...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-secretflow-spu-a-performant-and-user-friendly-framework-for-privacy-preserving" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - SecretFlow-SPU: A Performant and User-Friendly Framework for Privacy-Preserving...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-portunus-re-imagining-access-control-in-distributed-systems" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Portunus: Re-imagining Access Control in Distributed Systems
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-glogs-interactive-graph-pattern-matching-query-at-large-scale" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - GLogS: Interactive Graph Pattern Matching Query At Large Scale
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-cyclosa-redundancy-free-graph-pattern-mining-via-set-dataflowi" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23: Cyclosa: Redundancy-Free Graph Pattern Mining via Set Dataflowi
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-sowalker-an-io-optimized-out-of-coregraph-processing-system-for-second-order" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - SOWalker: An I/O-Optimized Out-of-CoreGraph Processing System for Second-Order...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-light-dedup-a-light-weight-inline-deduplication-framework-for-non-volatile" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Light-Dedup: A Light-weight Inline Deduplication Framework for Non-Volatile...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-tidedup-a-new-distributed-deduplication-architecture-for-ceph4" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - TiDedup: A New Distributed Deduplication Architecture for Ceph4
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-loopdelta-embedding-locality-aware-opportunistic-delta-compression-in-inline" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - LoopDelta: Embedding Locality-aware Opportunistic Delta Compression in Inline...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-tc-gnn-bridging-sparse-gnn-computation-and-dense-tensor-cores-on-gpus" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-legion-automatically-pushing-the-envelope-of-multi-gpu-system-for-billion-scale" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Legion: Automatically Pushing the Envelope of Multi-GPU System for Billion-Scale...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-bridging-the-gap-between-relational-oltp-and-graph-based-olap" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Bridging the Gap between Relational OLTP and Graph-based OLAP
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-comosum-an-extensible-reconfigurable-and-fault-tolerant-iot-platform-for" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Comosum: An Extensible, Reconfigurable, and Fault-Tolerant IoT Platform for...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-oakestra-a-lightweight-hierarchical-orchestration-framework-for-edge-computing" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Oakestra: A Lightweight Hierarchical Orchestration Framework for Edge Computing
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-explore-data-placement-algorithm-for-balanced-recovery-load-distribution" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Explore Data Placement Algorithm for Balanced Recovery Load Distribution
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-luci-loader-based-dynamic-software-updates-for-off-the-shelf-shared-objects" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Luci: Loader-based Dynamic Software Updates for Off-the-shelf Shared Objects
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-melf-multivariant-executables-for-a-heterogeneous-world" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - MELF: Multivariant Executables for a Heterogeneous World
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-apron-authenticated-and-progressive-system-image-renovation" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - APRON: Authenticated and Progressive System Image Renovation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-zpoline-a-system-call-hook-mechanism-based-on-binary-rewriting" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - zpoline: a system call hook mechanism based on binary rewriting
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-sponge-fast-reactive-scaling-for-stream-processing-with-serverless-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Sponge: Fast Reactive Scaling for Stream Processing with Serverless Frameworks
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-on-demand-container-loading-in-aws-lambda" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - On-demand Container Loading in AWS Lambda
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-decentralized-and-stateful-serverless-computing-on-the-internet-computer-blockchain" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Decentralized and Stateful Serverless Computing on the Internet Computer Blockchain
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-pinolo-detecting-logical-bugs-in-database-management-systems-with-approximate" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Pinolo: Detecting Logical Bugs in Database Management Systems with Approximate...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-autoarts-taxonomy-insights-and-tools-for-root-cause-labelling-of-incidents-in" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - AutoARTS: Taxonomy, Insights and Tools for Root Cause Labelling of Incidents in...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-avoiding-the-ordering-trap-in-systems-performance-measurement" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Avoiding the Ordering Trap in Systems Performance Measurement
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-aware-automate-workload-autoscaling-with-reinforcement-learning-in-production" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - AWARE: Automate Workload Autoscaling with Reinforcement Learning in Production...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-nodens-enabling-resource-efficient-and-fast-qos-recovery-of-dynamic-microservice" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23- Nodens: Enabling Resource Efficient and Fast QoS Recovery of Dynamic Microservice...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-lifting-the-veil-on-metas-microservice-architecture-analyses-of-topology-and" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Lifting the veil on Meta’s microservice architecture: Analyses of topology and...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-tectonic-shift-a-composite-storage-fabric-for-large-scale-ml-training" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Tectonic-Shift: A Composite Storage Fabric for Large-Scale ML Training
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-calcspar-a-contract-aware-lsm-store-for-cloud-storage-with-low-latency-spikes" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Calcspar: A Contract-Aware LSM Store for Cloud Storage with Low Latency Spikes
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-adaptive-online-cache-capacity-optimization-via-lightweight-working-set-size" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Adaptive Online Cache Capacity Optimization via Lightweight Working Set Size...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-sage-software-based-attestation-for-gpu-execution" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - SAGE: Software-based Attestation for GPU Execution
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-confidential-computing-within-an-ai-accelerator" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Confidential Computing within an AI Accelerator
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-arbitor-a-numerically-accurate-hardware-emulation-tool-for-dnn-accelerators" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Arbitor: A Numerically Accurate Hardware Emulation Tool for DNN Accelerators
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-bridging-the-gap-between-qoe-and-qos-in-congestion-control-a-large-scale-mobile" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Bridging the Gap between QoE and QoS in Congestion Control: A Large-scale Mobile...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-farreach-write-back-caching-in-programmable-switches" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - FarReach: Write-back Caching in Programmable Switches
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-cxl-anns-software-hardware-collaborative-memory-disaggregation-and-computation" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - CXL-ANNS: Software-Hardware Collaborative Memory Disaggregation and Computation...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-overcoming-the-memory-wall-with-cxl-enabled-ssd" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Overcoming the Memory Wall with CXL-Enabled SSD
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-styx-exploiting-smartnic-capability-to-reduce-datacenter-memory-tax" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - STYX: Exploiting SmartNIC Capability to Reduce Datacenter Memory Tax
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-change-management-in-physical-network-lifecycle-automation" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Change Management in Physical Network Lifecycle Automation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-aasclepius-monitoring-diagnosing-and-detouring-at-the-internet-peering-edge" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - AAsclepius: Monitoring, Diagnosing, and Detouring at the Internet Peering Edge
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-deploying-user-space-tcp-at-cloud-scale-with-luna" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Deploying User-space TCP at Cloud Scale with LUNA
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-rubbledb-cpu-efficient-replication-with-nvme-of" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - RubbleDB: CPU-Efficient Replication with NVMe-oF
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-distributed-transactions-at-scale-in-amazon-dynamodb" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Distributed Transactions at Scale in Amazon DynamoDB
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-prefix-siphoning-exploiting-lsm-tree-range-filters-for-information-disclosure" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Prefix Siphoning: Exploiting LSM-Tree Range Filters For Information Disclosure
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-epf-evil-packet-filter" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - EPF: Evil Packet Filter
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-translation-pass-through-for-near-native-paging-performance-in-vms" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Translation Pass-Through for Near-Native Paging Performance in VMs
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-efficient-memory-overcommitment-for-io-passthrough-enabled-vms-via-fine-grained" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Efficient Memory Overcommitment for I/O Passthrough Enabled VMs via Fine-grained...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-lpns-scalable-and-latency-predictable-local-storage-virtualization-for" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - LPNS: Scalable and Latency-Predictable Local Storage Virtualization for...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-p2cache-exploring-tiered-memory-for-in-kernel-file-systems-caching" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - P2CACHE: Exploring Tiered Memory for In-Kernel File Systems Caching
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-revisiting-secondary-indexing-in-lsm-based-storage-systems-with-persistent-memory" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Revisiting Secondary Indexing in LSM-based Storage Systems with Persistent Memory
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-zhuque-failure-is-not-an-option-its-an-exception" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Zhuque: Failure is Not an Option, it’s an Exception
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-envpipe-performance-preserving-dnn-training-framework-for-saving-energy" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - EnvPipe: Performance-preserving DNN Training Framework for Saving Energy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-decentralized-application-level-adaptive-scheduling-for-multi-instance-dnns-on" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Decentralized Application-Level Adaptive Scheduling for Multi-Instance DNNs on...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-unfaasener-latency-and-cost-aware-offloading-of-functions-from-serverless-platforms" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23- UnFaaSener: Latency and Cost Aware Offloading of Functions from Serverless Platforms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-llfree-scalable-and-optionally-persistent-page-frame-allocation" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - LLFree: Scalable and Optionally-Persistent Page-Frame Allocation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-singularfs-a-billion-scale-distributed-file-system-using-a-single-metadata-server" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - SingularFS: A Billion-Scale Distributed File System Using a Single Metadata Server
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-the-hitchhikers-guide-to-operating-systems" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - The Hitchhiker's Guide to Operating Systems
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-accelerating-distributed-moe-training-and-inference-with-lina" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Accelerating Distributed MoE Training and Inference with Lina
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-smartmoe-efficiently-training-sparsely-activated-models-through-combiningand" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - SmartMoE: Efficiently Training Sparsely-Activated Models through Combiningand...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-msrl-distributed-reinforcement-learning-with-dataflow-fragments" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - MSRL: Distributed Reinforcement Learning with Dataflow Fragments
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-beware-of-fragmentation-scheduling-gpu-sharing-workloads-with-fragmentation" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Beware of Fragmentation: Scheduling GPU-Sharing Workloads with Fragmentation...
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-towards-iterative-relational-algebra-on-the-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - Towards Iterative Relational Algebra on the GPU
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usenix-atc-23-vectorvisor-a-binary-translation-scheme-for-throughput-oriented-gpu-acceleration" class="md-nav__link">
    <span class="md-ellipsis">
      USENIX ATC '23 - VectorVisor: A Binary Translation Scheme for Throughput-Oriented GPU Acceleration
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>Usenix atc 2023 v2</h1>

<h2 id="usenix-atc-23-and-osdi-23-joint-keynote-address-sky-computing">USENIX ATC '23 and OSDI '23 Joint Keynote Address - Sky Computing</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=AuNfxVLdo0A">https://www.youtube.com/watch?v=AuNfxVLdo0A</a></p>
<ol>
<li>Sky Computing is an initiative to abstract away differences in cloud services and enable applications to run across multiple clouds.</li>
<li>The main challenges include compatibility, cost, latency, and data sovereignty.</li>
<li>Sky aims to provide a uniform layer of abstraction for different cloud services, allowing developers to focus on their application logic without worrying about the underlying infrastructure.</li>
<li>The project is currently in its early stages and has not yet reached full maturity.</li>
<li>Some key components of Sky include intercloud brokers, service catalogs, optimizers, and provisioners.</li>
<li>Intercloud brokers are responsible for mapping application requirements to available cloud services and ensuring that the best possible resources are used.</li>
<li>Service catalogs provide a list of available services across different clouds, along with their associated costs and performance characteristics.</li>
<li>Optimizers help developers specify their application requirements and automatically select the most suitable combination of services and resources.</li>
<li>Provisioners are responsible for allocating and managing resources on behalf of applications.</li>
<li>Sky is designed to work with both existing and new cloud services, leveraging open source software wherever possible.</li>
<li>The project aims to support a wide range of application types, including machine learning, data processing, and web applications.</li>
<li>Sky is still in its early stages and has not yet been widely adopted by the industry, but it has shown promise in early experiments and pilot projects.</li>
</ol>
<h2 id="usenix-atc-23-bifrost-analysis-and-optimization-of-network-io-tax-in-confidential-virtual">USENIX ATC '23 - Bifrost: Analysis and Optimization of Network I/O Tax in Confidential Virtual...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=E47Jgd6zXW8">https://www.youtube.com/watch?v=E47Jgd6zXW8</a></p>
<ol>
<li>3M's Defrost Analysis Optimization Network (DAON) improves network performance by reducing bounce power tax and focusing on security.</li>
<li>DAON leverages first two observations: end-to-end encryption + memory protection, and eliminates payload bouncing through zero copy encryption and deduplication.</li>
<li>Third observation is offloading reassembly logic to the frontend driver, which reduces specific processing types.</li>
<li>The architecture of DAON includes a CBM allocator that creates case Pneuma nodes for dedicated shared memory, and an application layer that decrypts payloads directly from 3D new memory.</li>
<li>Defrost successfully defends against the TLC tutorial attack and has been prototyped with significant performance improvements over traditional VMs.</li>
</ol>
<h2 id="usenix-atc-23-secretflow-spu-a-performant-and-user-friendly-framework-for-privacy-preserving">USENIX ATC '23 - SecretFlow-SPU: A Performant and User-Friendly Framework for Privacy-Preserving...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=X20APC8c9pQ">https://www.youtube.com/watch?v=X20APC8c9pQ</a></p>
<ol>
<li>Secret Flow SPU Framework: A novel framework that provides a solution to the problem of efficiently running machine learning programs while preserving privacy and security.</li>
<li>Design Objectives: The SPU Framework aims for user-friendliness, extensibility, and high performance. It supports multiple ML frameworks without significant code changes, allows for supporting various MPC protocols, and ensures optimized high-performance computation.</li>
<li>Key Components: </li>
<li>Compiler: Converts ML model bytecodes into barcode instructions that are executed by the SPU Framework's runtime.</li>
<li>Executor: Takes code generated by the compiler and executes it using specific MPC protocols, prioritizing performance and extensibility.</li>
<li>Optimization Techniques: The SPU Framework implements several optimization techniques to maximize computational speed and minimize latency, such as vectorization, bundle operation distribution across multicores, and streaming.</li>
<li>Performance Evaluation: Compared to other MPC-based ML frameworks like TF Encrypted, PSPs Krypton, and SPU Solar, the SPU Framework achieves better performance with almost comparable accuracy.</li>
</ol>
<h2 id="usenix-atc-23-portunus-re-imagining-access-control-in-distributed-systems">USENIX ATC '23 - Portunus: Re-imagining Access Control in Distributed Systems</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=J50rieOdMRQ">https://www.youtube.com/watch?v=J50rieOdMRQ</a></p>
<ol>
<li>Portunus is a geographical key management system by Cloudflare, built using attribute-based encryption (ABE) and public-key cryptography.</li>
<li>ABE allows access control with customer key knowledge while avoiding a central party or arbitrator.</li>
<li>TLS termination involves verifying digital signatures to ensure secure communication between the browser and website.</li>
<li>Cloudflare's solution for TLS inspection requires accessing the customer's private key, which is secured using Quicksilver, their globally synchronized key value store.</li>
<li>ABE scheme provides flexible access control with efficient decryption performance and supports negation, which allows excluding certain regions or data centers.</li>
<li>Portunus generates attribute secret keys for each policy and encrypts customer keys using these keys.</li>
<li>Decryption happens at the edge of Cloudflare's network to minimize latency and improve security.</li>
<li>The scheme supports efficient key rotation, which helps maintain security over time.</li>
</ol>
<h2 id="usenix-atc-23-glogs-interactive-graph-pattern-matching-query-at-large-scale">USENIX ATC '23 - GLogS: Interactive Graph Pattern Matching Query At Large Scale</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=qF17EMpEog4">https://www.youtube.com/watch?v=qF17EMpEog4</a></p>
<ol>
<li>GeoLogs is a design for an interactive graph pattern matching system, targeted at non-graph experts.</li>
<li>The goal of the system is to allow users to input arbitrary graph patterns and quickly obtain useful insights.</li>
<li>It aims to overcome the challenges of existing solutions by providing automatic optimization, distributed execution, and user-friendly interfaces.</li>
<li>The system uses a three-module architecture: User Frontend, Backend, and Glog System.</li>
<li>The Glog System is responsible for processing graph pattern queries and generating optimized execution plans. It utilizes a novel graph-based structure called Glock to maintain hardest statistic graphs.</li>
<li>Glocks are capable of handling large clusters and can scale well, providing significant performance gains.</li>
<li>The system supports automatic optimization by defining custom models that estimate the cost of each possible execution plan. This allows for efficient extraction of various pattern frequencies and generates optimized plans given a query.</li>
<li>The paper presents several experiments demonstrating the effectiveness of the proposed system in terms of performance, scalability, and usability.</li>
</ol>
<h2 id="usenix-atc-23-cyclosa-redundancy-free-graph-pattern-mining-via-set-dataflowi">USENIX ATC '23: Cyclosa: Redundancy-Free Graph Pattern Mining via Set Dataflowi</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=ooJvvgOIk34">https://www.youtube.com/watch?v=ooJvvgOIk34</a></p>
<ol>
<li>Introduce a research graph pattern mining system to solve redundant computation problems in graph mining.</li>
<li>Set data flow (SDF) is used for efficient traversal of search space and removal of redundancy.</li>
<li>Pattern-Centric Paradigm guides exploration, avoiding unnecessary partial instances and reducing large workflows.</li>
<li>Existing parent-centric systems focus on search space reduction through tree search methods, but still have redundant computations.</li>
<li>Rewriting optimization is difficult due to many implicit redundancies that cannot be tracked.</li>
<li>Set data flow (SDF) is proposed as a new abstraction level for computing procedures, allowing reuse of candidate vertices and reducing computation repetition.</li>
<li>Cyclosa prototype system uses SDF to eliminate redundancy, analyze patterns, and optimize set operators.</li>
<li>The system includes a pattern analysis module, de-flow creation, and a pattern size estimator.</li>
<li>Set data flow (SDF) is processed in DFS fashion, with set operators reusing allocated set spaces.</li>
<li>Cyclosa efficiently reuses redundant results and outperforms state-of-the-art methods in various situations.</li>
</ol>
<h2 id="usenix-atc-23-sowalker-an-io-optimized-out-of-coregraph-processing-system-for-second-order">USENIX ATC '23 - SOWalker: An I/O-Optimized Out-of-CoreGraph Processing System for Second-Order...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=g7JD1TppoBY">https://www.youtube.com/watch?v=g7JD1TppoBY</a></p>
<ol>
<li>Optimized Auto Core Graph Processing System: A system that improves the efficiency of analyzing graphs by implementing second order random work and considering previous vertex selection for better model accuracy.</li>
<li>Benefit-Aware (BA) IO Model: A model that maximizes IO utilization by loading multiple blocks at once, prioritizing updated works over non-updatable ones.</li>
<li>Block Set Oriented Work Updating Scheme: An approach that allows more efficient work updating by considering the entire block set rather than individual blocks. This scheme also helps in maximizing work updating rate and utilization.</li>
</ol>
<h2 id="usenix-atc-23-light-dedup-a-light-weight-inline-deduplication-framework-for-non-volatile">USENIX ATC '23 - Light-Dedup: A Light-weight Inline Deduplication Framework for Non-Volatile...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=dQzjK--oikw">https://www.youtube.com/watch?v=dQzjK--oikw</a></p>
<ol>
<li>MVM deduplication: Future features will impact deduplication, such as noncryptographic hash-based fingerprinting and byte container comparison.</li>
<li>Latitude system: Aims to fully exploit IO latency by using long-range pointer index (LRPI) for redundancy identification and a region-based layout for locality.</li>
<li>LMT (Light Metadata Table): Maintains locality in the layout, reducing metadata amplification.</li>
<li>CBP (Cross-Block Prefetch): A speculative prefetch technique that improves content comparison performance by leveraging hints and skipping fingerprint calculation when possible.</li>
<li>Lightedub: An efficient deduplication method that combines LRPI, CBP, and LMT to reduce metadata amplification and improve redundancy identification.</li>
</ol>
<h2 id="usenix-atc-23-tidedup-a-new-distributed-deduplication-architecture-for-ceph4">USENIX ATC '23 - TiDedup: A New Distributed Deduplication Architecture for Ceph4</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=UhRRTgebFqE">https://www.youtube.com/watch?v=UhRRTgebFqE</a></p>
<ol>
<li>Background: Conventional deduplication system using fingerprint algorithm and indexing for efficient data storage.</li>
<li>Proposed architecture: Centralized architecture with flexible, scalable design to address limitations in prior approaches.</li>
<li>Key design elements: Selective level crossover, crawling, event-driven architecture, content-defined chunking, wide shared reference, and yield reference minimization.</li>
<li>Demonstration of architecture: Tidy Divs concept, clear chunk tea, and Snapchat example for snapshot creation and deletion.</li>
<li>Evaluation: Four system experiments to measure distribution impact, constant performance, and throughput rate.</li>
<li>K3K Design: Aiming to provide reasonable performance with source code available on personal repository, welcoming feedback.</li>
</ol>
<h2 id="usenix-atc-23-loopdelta-embedding-locality-aware-opportunistic-delta-compression-in-inline">USENIX ATC '23 - LoopDelta: Embedding Locality-aware Opportunistic Delta Compression in Inline...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=OAHFt4KCyr0">https://www.youtube.com/watch?v=OAHFt4KCyr0</a></p>
<ol>
<li>Research paper presented on "Loop third date backup critical method fire protection"</li>
<li>Focuses on reducing storage cost and improving data redundancy techniques such as data deduplication, data compression, and eliminating rebound dates.</li>
<li>Introduces a new data reduction approach called "Loop Dirt".</li>
<li>Loop Dirt combines data compression, duplication-based backup systems, and inverse data congression to achieve high compression ratio, high I/O throughput, and improved restore performance.</li>
<li>It proposes a catch-aware filter for identifying similar chunks and catching potential semi-chunks.</li>
<li>The approach leverages both logic locality (similarity within consecutive backups) and physical locality (physical proximity of chunks in storage).</li>
<li>Four data sets are used to evaluate the performance of Loop Dirt: Markham SITC, greedy two-version, Look Dirt, and the Rapture dataset.</li>
<li>The results show that the Loop Dirt approach achieves higher compression ratios compared to other methods and significantly improves restore performance and backup throughput.</li>
</ol>
<h2 id="usenix-atc-23-tc-gnn-bridging-sparse-gnn-computation-and-dense-tensor-cores-on-gpus">USENIX ATC '23 - TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=Et-kBEnry5Y">https://www.youtube.com/watch?v=Et-kBEnry5Y</a></p>
<ol>
<li>TCGN: A Tensor Code-based Gene (Sparse Graph Neural Network) Acceleration Design<ul>
<li>Designed for efficient GPU utilization and high computation efficiency</li>
<li>Focuses on accelerating Sparse Graph Neural Networks (GNNs) using Tensor Cores in modern GPUs</li>
<li>Consists of three main components: input level delay, new sparse graph translation technique, and efficient processing</li>
</ul>
</li>
<li>Challenges in GNN Computation<ul>
<li>Existing frameworks like PyTorch and TensorFlow primarily focus on deep learning operations, not on GNN-specific optimizations</li>
<li>Irregular nature of gene computation makes it hard to harvest real performance gains</li>
<li>Underutilization of GPU resources due to lack of efficient sparse operator support</li>
</ul>
</li>
<li>Design Approach for TCGN<ul>
<li>Leverages Tensor Cores in modern GPUs to improve computation intensity and reduce memory consumption</li>
<li>Integrates a comprehensive algorithmic system that includes input level delay, new sparse graph translation technique, and efficient processing</li>
</ul>
</li>
<li>Sparse Graph Translation Technique<ul>
<li>Aims to compress the smart graph into fewer number tiles, leveraging high-performance Tensor Cores for acceleration</li>
</ul>
</li>
<li>Efficient Processing in TCGN<ul>
<li>Focuses on optimizing the computation of sparse GNNs using Tensor Cores</li>
</ul>
</li>
<li>Future Work and Optimizations<ul>
<li>Exploring ways to further accelerate preprocessing tasks, focusing on CPU-based OpenMP optimization</li>
<li>Enhancing tensor core compatibility for different precision levels (FP32, TF32)</li>
<li>Implementing kernel fusion techniques to improve the efficiency of inference scenarios</li>
</ul>
</li>
</ol>
<h2 id="usenix-atc-23-legion-automatically-pushing-the-envelope-of-multi-gpu-system-for-billion-scale">USENIX ATC '23 - Legion: Automatically Pushing the Envelope of Multi-GPU System for Billion-Scale...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=_5EXingIu1U">https://www.youtube.com/watch?v=_5EXingIu1U</a></p>
<ol>
<li>Legion is a system designed to efficiently train large-scale graphs using modern multi-GPU systems, specifically addressing the challenges of poor multigpu cache stability and managing graph topology data.</li>
<li>Legion proposes three key design outlines: hierarchical graph partitioning for improved multigpu cache stability, hotness-aware unified caching to manage graph topology data, and automatic cash management for optimal memory utilization.</li>
<li>Legion's hierarchical graph partitioning aims to maintain a balance between GPU interconnects while minimizing cache replication, resulting in decreased PCA traffic and improved load balancing.</li>
<li>Hotness-aware unified caching is designed to minimize overlapping partitions and manage both the topology data and feature data within the same cache system. This helps to reduce PCI traffic generated during graph sampling and extraction processes.</li>
<li>Legion's automatic cash management automatically determines optimal topology feature cache sizes, maximizing overall training throughput while minimizing the impact of different cache size variances on system performance.</li>
<li>Legion has been evaluated using real-world billion-scale graphs and has demonstrated significant speed improvements compared to state-of-the-art graph neural network libraries like DGL UVA and DJL UVA, with a reduction in PCI traffic by up to 80%.</li>
</ol>
<h2 id="usenix-atc-23-bridging-the-gap-between-relational-oltp-and-graph-based-olap">USENIX ATC '23 - Bridging the Gap between Relational OLTP and Graph-based OLAP</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=QVZfKw4XyB0">https://www.youtube.com/watch?v=QVZfKw4XyB0</a></p>
<ol>
<li>Introduce work on gap relational OLTP graph-based AP and cable tape work at Shanghai Jiao Tong University, Alibaba Group, and Shanghai AI Laboratory.</li>
<li>The JP algorithm is used for analyzing social media networks, with improvements made to better understand relationships between users.</li>
<li>Graph traversal and neural network techniques are used in data analysis.</li>
<li>Dynamic graph processing on relational data sets can be inefficient due to the need to rewrite graph operations and perform costly join operations.</li>
<li>Two existing solutions for dynamic graph analytical processing are presented: a data lab approach that combines an OLTP system with a graph-specific system, and a graph database system that stores data in a native graph model.</li>
<li>The new 4J Tiger Graph System solution aims to achieve good freshness while maintaining high performance.</li>
<li>A hybrid transactional analytical processing (HTAP) approach is introduced as an alternative for dynamic JP workloads, which allows real-time querying and guarantees data freshness.</li>
<li>The HDGut system extends the htap architecture to support graph processing in OLTP systems, providing a more efficient solution for dynamic JP workloads.</li>
<li>Key design goals of the HDGut system are: transparent data model conversion and efficient Dynamic graph storage with read/write performance and high data freshness.</li>
<li>The HDGut system uses a combination of RG mapping and efficient Dynamic storage to support concurrent reading and writing operations while maintaining locality.</li>
<li>The evaluation of the HDGut system shows that it achieves comparable performance to other existing solutions, with improvements in certain areas such as topology access property time computation.</li>
</ol>
<h2 id="usenix-atc-23-comosum-an-extensible-reconfigurable-and-fault-tolerant-iot-platform-for">USENIX ATC '23 - Comosum: An Extensible, Reconfigurable, and Fault-Tolerant IoT Platform for...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=E7L5lP1EmrI">https://www.youtube.com/watch?v=E7L5lP1EmrI</a></p>
<ol>
<li>Kamasan: An Extensible, Reconfigurable Full-Tolerant Platform for Digital Agriculture</li>
<li>Four-year experience in deploying a digital agriculture platform across various farms and cloud providers</li>
<li>Challenges: Limited connectivity, unreliable power, sparse population, and geography</li>
<li>Kamasan Design: Software Defined Farm (SDF) with reconfigurable data plane and control plane</li>
<li>Data Plane: Sensor data processing and storage, analytics module for decision-making support</li>
<li>Control Plane: Configured using a reusable template</li>
<li>Modular Objects: Compute, Storage, Analytics, Telemetry</li>
<li>RPC Protocol: Ensures module network agnosticism</li>
<li>Deployment Experiences: Water Guard (water stress detection), Vineyard Disease Detection</li>
<li>Lessons Learned: Extensibility and reconfigurability challenges, cloud complications, failure tolerance improvement</li>
<li>Future Work: Explore enabling scalable data analytics at the edge using an edge-centric version of Kubernetes in collaboration with IBM Research</li>
</ol>
<h2 id="usenix-atc-23-oakestra-a-lightweight-hierarchical-orchestration-framework-for-edge-computing">USENIX ATC '23 - Oakestra: A Lightweight Hierarchical Orchestration Framework for Edge Computing</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=tdwTqPh8lH0">https://www.youtube.com/watch?v=tdwTqPh8lH0</a></p>
<ol>
<li>Introduction to Orchestra:</li>
<li>Lightweight, hierarchical orchestration framework for Edge Computing</li>
<li>Addresses heterogeneity and constraints of edge devices</li>
<li>Problem statement:</li>
<li>Managing applications on constrained edge devices</li>
<li>Solution overview:</li>
<li>System design that supports latency-critical applications</li>
<li>Key components of Orchestra:</li>
<li>Root orchestrator</li>
<li>Multiple clusters</li>
<li>Worker nodes</li>
<li>Node engine</li>
<li>Net manager</li>
<li>Cluster concept:</li>
<li>Composed of multiple worker nodes</li>
<li>Managed by cluster manager</li>
<li>Delegated scheduling:</li>
<li>Resource scale and placement decisions are delegated to the root orchestrator</li>
<li>Latency-aware deployment:</li>
<li>Aimed at respecting latency requirements across services</li>
<li>Balancing traffic:</li>
<li>Uses round-robin or closest IP address balancing policies</li>
<li>Networking performance:</li>
<li>Optimized for low overhead and efficient resource consumption in constrained environments</li>
<li>Implementation:<ul>
<li>Open-source, modular design with 18,000 lines of code</li>
</ul>
</li>
<li>Performance results:<ul>
<li>Six times lower CPU percentage usage on worker nodes compared to Kubernetes</li>
<li>Ten times lower CPU percentage usage on clusters</li>
</ul>
</li>
<li>Application performance:<ul>
<li>AR application processed around 100 times faster with Orchestra than without it</li>
</ul>
</li>
</ol>
<h2 id="usenix-atc-23-explore-data-placement-algorithm-for-balanced-recovery-load-distribution">USENIX ATC '23 - Explore Data Placement Algorithm for Balanced Recovery Load Distribution</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=I9WrwbNS6pM">https://www.youtube.com/watch?v=I9WrwbNS6pM</a></p>
<ol>
<li>Junction Laboratory researchers present a data placement algorithm for balanced recovery load distribution in distributed storage systems.</li>
<li>The approach addresses the issue of data placement and aims to balance the workload among various nodes during the recovery process.</li>
<li>The system, called GFS Round Club System, splits data into smaller units called Toyota Units and replicates them for reliability.</li>
<li>Data is placed in specific locations based on a mapping system that considers the unit-specific location of each piece of information.</li>
<li>In case of lost data, recovery processes are initiated across placement groups, with correlated fields noted to help repair the loss.</li>
<li>The researchers found that highly paralyzed systems can dramatically improve their recovery speed by distributing the load more evenly among nodes.</li>
<li>They also discovered that limiting clusters' bandwidth could further improve overall reliability but might cause congestion in certain nodes, prolonging the recovery time.</li>
<li>To address this issue, they propose a simple solution called "finger" that aims to distribute sufficient numbers of data units across various nodes to balance the load.</li>
<li>The algorithm uses randomization and Franklin's recovery approach for effective load balancing, increasing the probability of data loss while minimizing it.</li>
<li>The researchers also address the issue of system expansion, which can disrupt the entire process if not handled properly.</li>
<li>They propose a new method called "Greedy Data Placement Algorithm" that significantly reduces recovery time and improves overall performance.</li>
<li>The algorithm is found to be 17-24 times more efficient than random data distribution in terms of recovery performance.</li>
</ol>
<h2 id="usenix-atc-23-luci-loader-based-dynamic-software-updates-for-off-the-shelf-shared-objects">USENIX ATC '23 - Luci: Loader-based Dynamic Software Updates for Off-the-shelf Shared Objects</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=h9-vHlND9ZY">https://www.youtube.com/watch?v=h9-vHlND9ZY</a></p>
<ol>
<li>Dynamic software update (DSU) enables changes to running processes without restarting applications.</li>
<li>Lucy's approach leverages the existing dynamic linker and its ability to handle shared object binary forms, adapting it for automatic updates.</li>
<li>The focus is on libraries, which are commonly used across multiple applications and have frequent vulnerabilities.</li>
<li>The DSU approach works by intercepting active code in libraries, performing necessary relocations, and handling control transfers between functions.</li>
<li>It supports runtime execution of tasks, automatic detection of updates, and seamless installation of new versions without user interaction or application downtime.</li>
<li>Lucy's DSU is designed for compatibility with various programming languages and compilers, such as GCC and LLVM.</li>
<li>The approach uses designated relocation interfaces, which simplify the process and make it safer compared to modifying machine instructions directly.</li>
<li>It can automatically track modifications in loaded shared libraries and handle incompatibilities by notifying users and requiring manual restart if necessary.</li>
<li>Lucy's DSU is compatible with popular Linux distributions like Debian and Ubuntu, which are used in various production environments.</li>
</ol>
<h2 id="usenix-atc-23-melf-multivariant-executables-for-a-heterogeneous-world">USENIX ATC '23 - MELF: Multivariant Executables for a Heterogeneous World</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=bkkwaur2GCk">https://www.youtube.com/watch?v=bkkwaur2GCk</a></p>
<ol>
<li>Mighty Variant ELF (MVELF) approach for multivariant executables in a heterogeneous world</li>
<li>Incorporates multiple compile-time variants and function granularity</li>
<li>Uses memory overlaying technique to exchange sections at runtime</li>
<li>Overlay manager allows concurrent use of multiple variants across the application</li>
<li>Evaluated via four case studies: performance isolation, profiling map, heterogeneous ISO setup, and MILF adaptability</li>
<li>Heterogeneous ISO setup demonstrated optimal thread pool dispatching for specific ISA jobs</li>
<li>MILF adaptability showed dynamic selection of assertions and address sanitization</li>
<li>Four case studies provided real-world examples of applying the MVELF approach</li>
<li>Researchers recommend considering the MVELF approach to optimize performance in heterogeneous environments</li>
</ol>
<h2 id="usenix-atc-23-apron-authenticated-and-progressive-system-image-renovation">USENIX ATC '23 - APRON: Authenticated and Progressive System Image Renovation</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=aZn8UMDuzfw">https://www.youtube.com/watch?v=aZn8UMDuzfw</a></p>
<ol>
<li>Apron is an authenticated proactive system for efficient physical device recovery.</li>
<li>It ensures secure, immediate system availability and covers everyday workloads like document sending and text messaging.</li>
<li>Apron consists of 5 stages: due region, system correlation, system corrosion recognized, solution make system enter recovered environment, and finally, recovery.</li>
<li>Apron's primary focus is on minimizing system downtime during recovery and ensuring data integrity.</li>
<li>The presentation highlights the limitations of existing recovery systems and proposes improvements using a background prefetcher feature.</li>
<li>The background prefetcher fetches equivalent blocks from storage equipment, reducing recovery time and network overhead.</li>
<li>Apron's evaluation shows that it significantly reduces system downtime during recovery and improves overall system availability.</li>
</ol>
<h2 id="usenix-atc-23-zpoline-a-system-call-hook-mechanism-based-on-binary-rewriting">USENIX ATC '23 - zpoline: a system call hook mechanism based on binary rewriting</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=aC_X0WU-tGM">https://www.youtube.com/watch?v=aC_X0WU-tGM</a></p>
<ol>
<li>Presentation on binary writing work and system config mechanism</li>
<li>Existing issues with adaptation and user space programs</li>
<li>Overview of DPDK (Data Plane Development Kit) for high-performance networking applications</li>
<li>Introducing a new system config mechanism, P Trace, to improve performance and avoid modifying existing programs</li>
<li>Use case: AWP (Advanced Web Protocol) portable network stack with DPDK, achieving five times better throughput compared to standard Linux TCP stack</li>
<li>Discussion on the challenges of binary writing and the importance of a proper system call mechanism</li>
<li>Explanation of Cisco C Center instruction triggering system calls and the concept of user-defined hooks</li>
<li>Demonstration of using two-byte instructions for jumping to user-defined hook functions</li>
<li>Introducing a new calling convention to improve system call execution efficiency</li>
<li>Discussion on detecting and preventing termination bugs in binary writing</li>
<li>Overview of the "Zeppelin" mechanism for handling memory accesses and protecting system code</li>
<li>Demonstration of applying the system config mechanism transparently to existing applications (e.g., AWP, DPDK)</li>
<li>Summary: The presented plane system config mechanism is exciting and has the potential to improve CPU-based writing performance while maintaining compatibility with existing user space programs.</li>
</ol>
<h2 id="usenix-atc-23-sponge-fast-reactive-scaling-for-stream-processing-with-serverless-frameworks">USENIX ATC '23 - Sponge: Fast Reactive Scaling for Stream Processing with Serverless Frameworks</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=mhx0aWhzP6w">https://www.youtube.com/watch?v=mhx0aWhzP6w</a></p>
<ol>
<li>Sponge: A Distributed Stream Processing System</li>
<li>Sponge addresses scaling stateful and stateless operators in a real-time stream processing system</li>
<li>Uses serverless frameworks for fast, scalable computation</li>
<li>Redirects bursty input data to prevent latency peaks and CPU bottlenecks</li>
<li>Introduces a router operator to coordinate data flow among service instances</li>
<li>Calculates required number of additional cores based on system throughput and input rate</li>
<li>Achieves 88% reduction in 99th percentile tail latency compared to VM scaling methods</li>
<li>Reduces cost by up to 17% compared to provisioning methods while maintaining comparable performance</li>
</ol>
<h2 id="usenix-atc-23-on-demand-container-loading-in-aws-lambda">USENIX ATC '23 - On-demand Container Loading in AWS Lambda</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=Wden61jKWvs">https://www.youtube.com/watch?v=Wden61jKWvs</a></p>
<ol>
<li>AWS Lambda function service offering since 2015</li>
<li>Support for deploying code using zip files and raw code files</li>
<li>250MB file size limit increased to 10GB (40x increase)</li>
<li>Extended support for code file deployment in zip format, as well as Docker-style container images</li>
<li>Customer concern: increasing package size without increasing cold start latency</li>
<li>Lambda's challenge: making cold start faster while allowing larger package sizes</li>
<li>Customer feedback changed over time - from difficult to complex, and now more focused on ease of use and simplicity</li>
<li>Introduction of Docker-style container images for deploying applications on AWS Lambda</li>
<li>Firecracker micro VM technology used in Lambda workers</li>
<li>On-demand loading of containers with accelerated cold start times (up to 15x faster)</li>
<li>Lazy loading allows for acceleration of container loading, and efficient use of system resources</li>
<li>Deduplication of container images leads to significant cost savings and improved performance</li>
<li>Use of consistent hashing and distributed caching provides a fault-tolerant and scalable solution for data storage and retrieval</li>
<li>Erasure coding technique used in place of replication for increased resilience and reduced cost</li>
<li>Tiered cache structure (L1, L2, L3) improves performance by reducing latency and improving cache hit rates</li>
<li>The future of serverless applications will involve improved packaging techniques and better support for virtualization technologies</li>
<li>AWS Lambda's "snapstart" feature helps reduce cold start times and improve the overall performance of Java-based applications</li>
</ol>
<h2 id="usenix-atc-23-decentralized-and-stateful-serverless-computing-on-the-internet-computer-blockchain">USENIX ATC '23 - Decentralized and Stateful Serverless Computing on the Internet Computer Blockchain</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=y4rVM_dXD5s">https://www.youtube.com/watch?v=y4rVM_dXD5s</a></p>
<ol>
<li>Internet Computer (IC) platform aims to run computation in a decentralized manner.</li>
<li>The platform is designed to be efficient and scalable, with a focus on execution environment optimization.</li>
<li>IC consists of replicated State machines running independently across multiple data centers, partitioned into subnets.</li>
<li>Programmers can write smart contracts (canisters) in languages like Rust, Python, or JavaScript, which are compiled to WebAssembly bytecode and executed by the IC platform.</li>
<li>The platform has a four-layer architecture: networking, consensus, message routing, and execution environment layers.</li>
<li>Statefulness is achieved through canister smart contracts, which allow for persistent data storage and fast query times.</li>
<li>Deterministic scheduling ensures that computation order and results are consistent across replicated State machines.</li>
<li>Billing is based on instruction count rather than time, making it more fair and efficient for developers.</li>
<li>The platform offers serverless capabilities with a focus on performance and scalability.</li>
<li>IC has been in production since May 2021 and continues to grow, with ongoing improvements and developments.</li>
</ol>
<h2 id="usenix-atc-23-pinolo-detecting-logical-bugs-in-database-management-systems-with-approximate">USENIX ATC '23 - Pinolo: Detecting Logical Bugs in Database Management Systems with Approximate...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=mirh27_Pwmw">https://www.youtube.com/watch?v=mirh27_Pwmw</a></p>
<ol>
<li>Background: Importance of Dbms in modern Enterprise applications and their potential impact on security</li>
<li>Key Findings: </li>
<li>Discovery of a challenging logical bug in Dbms that can result in incorrect results without any runtime errors</li>
<li>Identification of three main lines of work to address the issue: Dbms testing, differential testing, and metamorphic testing</li>
<li>Challenges:</li>
<li>Handling expressive SQL statements and their dialects</li>
<li>Detecting deeply hidden bugs in Dbms</li>
<li>Proposed Solutions: </li>
<li>Advanced version of metamorphic testing to generate generalized equivalence relations and detect logical bugs</li>
<li>Lexis 3 mutual database system for selecting PQS and Rank TLP state art approach for comparison</li>
<li>Research Findings:</li>
<li>Discovery of 41 unique logical bugs in 24-hour Dbms settings, with 39 confirmed by developers</li>
<li>Limitations:</li>
<li>Restrictions on satisfying given syntax due to advanced features of Dbms (e.g., aggregate functions, window functions)</li>
<li>Future Work:</li>
<li>Exploring the impact of seed query generation on final results and coverage data evaluation</li>
<li>Conclusion:</li>
<li>The presentation introduces an interesting concept called "approximation relation" to resolve test article problems in detecting logical bugs in Dbms</li>
</ol>
<h2 id="usenix-atc-23-autoarts-taxonomy-insights-and-tools-for-root-cause-labelling-of-incidents-in">USENIX ATC '23 - AutoARTS: Taxonomy, Insights and Tools for Root Cause Labelling of Incidents in...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=sGsDeJTuhEI">https://www.youtube.com/watch?v=sGsDeJTuhEI</a></p>
<ol>
<li>Microsoft Azure automates root cause labeling in incident management using tools like Auto Arts.</li>
<li>The goal is to improve the efficiency and accuracy of analyzing post-incident reports and identifying contributing factors.</li>
<li>Manual labeling of incidents is often error-prone, time-consuming, and subjective due to linguistic differences among engineers.</li>
<li>A hierarchical taxonomy called Arts is built to facilitate easy navigation across hundreds of contributing factors.</li>
<li>Auto Arts uses machine learning models to automatically label post-incident reports and generate context for better understanding of incidents.</li>
<li>The tool aims to improve the quality of annotations, reduce uninformed guessing, and enable experts to discover new tags based on context extracted from the reports.</li>
</ol>
<h2 id="usenix-atc-23-avoiding-the-ordering-trap-in-systems-performance-measurement">USENIX ATC '23 - Avoiding the Ordering Trap in Systems Performance Measurement</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=h3CM16m_Jq4">https://www.youtube.com/watch?v=h3CM16m_Jq4</a></p>
<ol>
<li>Avoid the "ordering trap" in performance testing: Pay attention to the order of test execution, as it can significantly affect results and lead to incorrect or unreliable conclusions.</li>
<li>Follow a systematic methodology for experimentation: Define a baseline order, implement a reset procedure, run experiments with fixed and random orders, and perform statistical analysis using nonparametric tests like the Crystal Wallace test.</li>
<li>Use the "OrderSmart" tool to help manage the experiment process, including running individual tests in fixed or random orders, collecting results, and performing analyses.</li>
<li>Consider the impact of ordering on long-term performance data sets: Analyzing a large volume of trial results (e.g., 2 million) can reveal significant differences due to test order execution.</li>
<li>Implement rigorous performance analysis practices: Include order testing in your experiment design, as this factor can lead to accurate conclusions and avoid biasing factors.</li>
<li>Share resources for further exploration: The "OrderSmart" tool, along with the collected data sets and research papers, are available to help orchestrate experiments and learn more about the impact of ordering on performance testing.</li>
</ol>
<h2 id="usenix-atc-23-aware-automate-workload-autoscaling-with-reinforcement-learning-in-production">USENIX ATC '23 - AWARE: Automate Workload Autoscaling with Reinforcement Learning in Production...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=gnOmCb1CG1s">https://www.youtube.com/watch?v=gnOmCb1CG1s</a></p>
<ol>
<li>IBM Research presents work on Auto-scaling and Reinforced Learning in Cloud systems.</li>
<li>The talk focuses on three main challenges: fast model adaptation to new workloads, reliable online policy serving during workload changes, and robust early-stage policy training.</li>
<li>Aware system framework is proposed as a solution which includes Multidimensional Part Auto Scaler (MPA), Boost Trapper module, Retraining Detection Trigger module, and Meta Learning module.</li>
<li>MPA uses offline and online training for the Iowa agent to learn optimal policies for workload management.</li>
<li>The Boost Trapper module combines offline and online learning stages to improve performance.</li>
<li>The Retraining Detection Trigger module continuously monitors changes in the environment and triggers retraining when necessary.</li>
<li>Meta Learning module enables fast adaptation of models to new workloads by using a base learner and a meta-learner.</li>
<li>The talk presents several case studies, including kubernetes cluster management, and demonstrates the effectiveness of the proposed framework in improving resource utilization and reducing SLO violations.</li>
</ol>
<h2 id="usenix-atc-23-nodens-enabling-resource-efficient-and-fast-qos-recovery-of-dynamic-microservice">USENIX ATC '23- Nodens: Enabling Resource Efficient and Fast QoS Recovery of Dynamic Microservice...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=YKkyh6oAqEY">https://www.youtube.com/watch?v=YKkyh6oAqEY</a></p>
<ol>
<li>Microservice Dynamics: The focus is on efficient skill recovery and maintaining high result efficiency in a dynamic medical service application.</li>
<li>Monitoring and Load Management: Implementing methods for quick monitoring of low change microservices, upper Network traffic, and load management based on traffic data.</li>
<li>Blocking Relationships: Capture blocking relationships among different microservices by constructing an execution blocking graph based on call dependency and core order. Update actual load in real-time.</li>
<li>Efficient Career Journey: Allocate enough recessive results to enable fastened result efficiency, ensuring quick query processing and maintaining high result efficiency.</li>
</ol>
<h2 id="usenix-atc-23-lifting-the-veil-on-metas-microservice-architecture-analyses-of-topology-and">USENIX ATC '23 - Lifting the veil on Meta’s microservice architecture: Analyses of topology and...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=vfsdGdVAwag">https://www.youtube.com/watch?v=vfsdGdVAwag</a></p>
<ol>
<li>Research on metas microservice architecture:</li>
<li>Analyzed 22 months of data from 400+ services, with daily churn in deployed services</li>
<li>Found that inference platform (IP) services had a stable deployment rate compared to regular services</li>
<li>Infrastructure-related tasks such as multitenancy and data placement were considered important for service granularity management</li>
<li>Request workflow analysis:</li>
<li>Investigated the behavior of different child services called by parent services, finding that some children are more likely to be called concurrently than others</li>
<li>Found that clusters in call frequency can indicate specific child sets with similar behavior</li>
<li>Concurrency rate analysis:</li>
<li>Observed significant variation in concurrency rates across different service endpoints and child sets</li>
<li>Identified that certain services consistently had high or low concurrency rates, suggesting well-defined control data dependencies</li>
<li>Implications for microservice architecture:</li>
<li>Highlighted the need for extended tooling support to handle heterogeneity and significant churn in topology and workflows</li>
<li>Suggested that future research should expand understanding of micros services by expanding abstraction support for different types of architectures, and by releasing data via GitHub.</li>
</ol>
<h2 id="usenix-atc-23-tectonic-shift-a-composite-storage-fabric-for-large-scale-ml-training">USENIX ATC '23 - Tectonic-Shift: A Composite Storage Fabric for Large-Scale ML Training</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=F1ffpj8NkzE">https://www.youtube.com/watch?v=F1ffpj8NkzE</a></p>
<ol>
<li>Industry trend towards scaling ML infrastructure</li>
<li>Need for storage and IOP capacity to support large, complex models</li>
<li>Tectonic shift: Composite storage fabric designed for ML training</li>
<li>Combines tectonic cluster with flash-based shift cluster</li>
<li>Maximizes IOP absorption using intelligent policies leveraging historic and future access information derived from application semantics</li>
<li>Significant power savings across multiple data centers</li>
</ol>
<h2 id="usenix-atc-23-calcspar-a-contract-aware-lsm-store-for-cloud-storage-with-low-latency-spikes">USENIX ATC '23 - Calcspar: A Contract-Aware LSM Store for Cloud Storage with Low Latency Spikes</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=esHi3z0HdRE">https://www.youtube.com/watch?v=esHi3z0HdRE</a></p>
<ol>
<li>Presentation on cloud block storage</li>
<li>Importance of latency in cloud storage systems</li>
<li>AWS EBS as a popular platform for cloud block storage</li>
<li>Latency analysis and optimization techniques</li>
<li>Discussion on EBS types, Cloud windows, and LPS control</li>
<li>Brief mention of RSM Store and other databases</li>
<li>Proposal for tax forms to avoid latency battles</li>
<li>Emphasis on using multiple sources and general reality tables</li>
<li>Call to reduce 99 percentile latency for better performance</li>
</ol>
<h2 id="usenix-atc-23-adaptive-online-cache-capacity-optimization-via-lightweight-working-set-size">USENIX ATC '23 - Adaptive Online Cache Capacity Optimization via Lightweight Working Set Size...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=NHDM8bd_260">https://www.youtube.com/watch?v=NHDM8bd_260</a></p>
<ol>
<li>Adaptive Cache Capacity Optimization<ul>
<li>Lightweight Walking Set Size Estimation: Accurate item repetition ratio estimation</li>
<li>Fine-granted Concurrency Control Measure: Use of optimistic aging and segment block technique</li>
<li>Cookie-based Online Catch Capacity Tuning Framework: Cookie design and adaptive capacity tuning mechanism</li>
<li>Evaluation Performance: Compare approaches, evaluate accuracy, working set size, site acceleration</li>
<li>Real-world System Practice: Show effectiveness in real-world applications using cookie environment</li>
</ul>
</li>
<li>Key Contributions of the Proposed Methods<ul>
<li>Lightweight Walking Set Size: Music ratio estimation, fine-granted concurrency control measure</li>
<li>Cookie-based Online Catch Capacity Tuning Framework: Use of cookies together with lifting catch system</li>
</ul>
</li>
<li>Challenges and Solutions<ul>
<li>Challenge 1: Properly estimating working set size in dynamic online workloads<ul>
<li>Solution: Implement automated atomic operations, cookie data structure design</li>
</ul>
</li>
<li>Challenge 2: Ensuring cache layer efficiency in high concurrency scenarios<ul>
<li>Solution: Cookie's lightweight architecture and optimistic aging strategy</li>
</ul>
</li>
<li>Challenge 3: Accurately determining catch capacity status in various scenarios<ul>
<li>Solution: Propose a structure for cash status, use existing methods to generate optimal catch size</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="usenix-atc-23-sage-software-based-attestation-for-gpu-execution">USENIX ATC '23 - SAGE: Software-based Attestation for GPU Execution</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=vpsNlotLJIM">https://www.youtube.com/watch?v=vpsNlotLJIM</a></p>
<ol>
<li>SAGE: A Software Approach for Secure GPU Execution</li>
<li>Researchers propose a software-based approach to provide integrity and secrecy guarantees when executing code on untrusted GPUs.</li>
<li>The proposed solution is called "Sage" and aims to bridge the gap between traditional CPU security models and emerging GPU accelerators.</li>
<li>Sage provides a verifiable execution environment for GPU devices, ensuring that code executed on an untrusted GPU device remains unmodified even in the presence of malicious actors.</li>
<li>SAGE Architecture</li>
<li>Sage is composed of two main components: the Trusted Verifier and the GPU Execution Environment.</li>
<li>The Trusted Verifier is responsible for kicking off software primitives that run on an untrusted system, while the GPU Execution Environment establishes a root trust and sets up a shared secret key for encrypted authenticated communication between the verifier and the GPU.</li>
<li>SAGE Components</li>
<li>The Verification Function is responsible for computing checksum values for code running on an untrusted system, ensuring that any modifications made to the code are detectable.</li>
<li>The Trusted Verifier Check function computes correct checksum values returned by the GPU and verifies their correctness.</li>
<li>Challenges in Implementing SAGE</li>
<li>Designing a verification function for GPUs is challenging due to the lack of fundamental documentation on GPU architecture and the need for optimal GPU utilization and predictable execution times.</li>
<li>Practical Demonstration of SAGE</li>
<li>The researchers demonstrate the feasibility of their approach by implementing a multilayer perceptron neural network using Sage, showing that the overhead introduced by the software-based security mechanism can be constrained to reasonable levels.</li>
<li>Future Work and Conclusion</li>
<li>The researchers suggest that future work could involve combining software and hardware solutions to provide multiple layers of security defense depth, leveraging the strengths of each approach while mitigating their respective weaknesses.</li>
</ol>
<h2 id="usenix-atc-23-confidential-computing-within-an-ai-accelerator">USENIX ATC '23 - Confidential Computing within an AI Accelerator</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=nTVP6IvFlDY">https://www.youtube.com/watch?v=nTVP6IvFlDY</a></p>
<ol>
<li>Presentation introduced the concept of Confessional Computing and its importance in preserving AI privacy in cloud environments.</li>
<li>Confessional Computing introduces a Trusted Execution Environment (TEE) to provide strong protection against privileged attacks, confidentiality requirements, and data theft.</li>
<li>The speaker discussed the need for hardware-level remote attestation mechanisms to ensure trust between tenant and host environments.</li>
<li>A new development board was introduced that employed TSMc 7nm technology to provide a secure AI computing environment.</li>
<li>The speaker highlighted the importance of isolating entire device hosts using remote attestation and discussed the concept of Trust Extension (TE) to further enhance security.</li>
<li>The presentation covered the Intellectual Property Units (IPUs) architecture, which is designed for massively parallel processing and has a flat memory space with core-private SRAM for enhanced security.</li>
<li>The speaker emphasized the need for low Total Cost of Ownership (TCOP) and introduced the concept of an optimized Trust Extension (TE) that can be deployed in hardware to provide strong security guarantees without trusting the CPU.</li>
<li>The presentation highlighted the importance of efficient encryption protocols, such as GCM, to protect data from tampering or replay attacks.</li>
<li>The speaker demonstrated the performance benefits of using IPUs for AI workloads and showcased a graph plot comparing the performance of baseline IPU with VM-based TEE.</li>
<li>The presentation concluded by emphasizing the need for confidential Compute Hardware in the future to support hyperscale privacy-preserving AI workloads, while also providing strong security guarantees without trusting the CPU.</li>
</ol>
<h2 id="usenix-atc-23-arbitor-a-numerically-accurate-hardware-emulation-tool-for-dnn-accelerators">USENIX ATC '23 - Arbitor: A Numerically Accurate Hardware Emulation Tool for DNN Accelerators</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=WvIHZMkGD7o">https://www.youtube.com/watch?v=WvIHZMkGD7o</a></p>
<ol>
<li>Hardware accelerators are a popular research area, leveraging various hardware-level optimizations to speed up the training process and support low precision arithmetic and sparsity processing.</li>
<li>However, optimization may have nontrivial effects on model accuracy, introducing new problems in estimating its impact.</li>
<li>The presenter proposes an "Arbiter" tool that provides a fast, easy-to-use emulator for DNA accelerators, offering numerically accurate emulation and aiding machine learning compiler conduct extensive case studies using arbitrary popular benchmark data sets.</li>
<li>The use of low precision data formats like fixed-point posit can provide higher processing power with lower memory footprint but still requires trade-offs in range and precision values.</li>
<li>Approximate computing techniques, such as replacing nonlinear operations with approximate versions, could improve performance while reducing chip area.</li>
<li>The Arbiter tool is designed to offer fine-grained numerically accurate emulation by taking a compiler-based approach that can support wide ranges of arbitrary numerical formats and customize operation implementations.</li>
<li>It also enables the exploration of sparsity processing, allowing users to specify an arbitrary n2m sparsity pattern and configure a scoring function for pruning values.</li>
<li>The Arbiter tool is built on top of modern machine learning compilers like XLA, TVM, and Glow, which transform computation graphs into hardware-independent IRs, perform certain analyses and optimizations, and ultimately generate lower-level kernel code.</li>
<li>Evaluation results show that the Arbiter tool can accurately mimic numerical behavior while achieving feasible overhead compared to traditional software-based simulators. It also allows researchers to conduct accurate analysis of model convergence accuracy with low Precision training.</li>
</ol>
<h2 id="usenix-atc-23-bridging-the-gap-between-qoe-and-qos-in-congestion-control-a-large-scale-mobile">USENIX ATC '23 - Bridging the Gap between QoE and QoS in Congestion Control: A Large-scale Mobile...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=arT_YQmLKJ0">https://www.youtube.com/watch?v=arT_YQmLKJ0</a></p>
<ol>
<li>Congestion control (CC) in large-scale mobile web services is crucial for optimizing user experience and service quality.</li>
<li>The QE metric focuses on the end-to-end performance of a web request, while traditional QS metrics concentrate on transport layer performance.</li>
<li>Introducing a fluid-based CC selection framework can help improve QE by selecting the best CCA for specific scenarios and application requirements.</li>
<li>The Flu system includes a monitor selector to continuously track application and network states, then selects an appropriate CCA based on these factors.</li>
<li>Reinforcement learning is used to build prediction models and selection policies that adapt to changing network conditions and user preferences.</li>
<li>A key challenge in CC switching is ensuring smooth transitions without impacting performance or introducing additional design complexities.</li>
<li>The Flu system addresses this issue by allowing for seamless CCA migration, including variable migration parameters that help maintain consistent performance during transitions.</li>
<li>Results from a real-world experiment with a popular mobile web service demonstrated that the Flu system can improve QE and throughput while reducing latency and improving user experience.</li>
</ol>
<h2 id="usenix-atc-23-farreach-write-back-caching-in-programmable-switches">USENIX ATC '23 - FarReach: Write-back Caching in Programmable Switches</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=VVP0J8l9wlI">https://www.youtube.com/watch?v=VVP0J8l9wlI</a></p>
<ol>
<li>Recent study aims to improve right performance of key-value stores, specifically for intensive workloads.</li>
<li>Facebook's Trivial Pair production workload challenges achieving high right performance due to latency issues and server imbalances.</li>
<li>A programmable switch architecture is proposed to address these challenges, with a control plane and data plane working together.</li>
<li>Within the data plane, multiple pipelines process requests and responses within switch memory.</li>
<li>The write policy allows for whole records to be cached in-switch memory instead of updating servers immediately, improving right performance.</li>
<li>Three main challenges faced by the proposed architecture: performance, availability, and reliability.</li>
<li>To fix the performance challenge, a nonblocking cache admission system is introduced, allowing subsequent rights to be processed server side without blocking.</li>
<li>The availability issue is addressed by introducing a non-blocking eviction method that conservatively reads from servers to maintain consistency.</li>
<li>A crash consistent snapshot generation and loss recovery mechanism is proposed to ensure data reliability in case of switch failures.</li>
<li>Evaluation was done using Tofino switches, with Farish achieving 66 times better performance compared to baseline systems in a large-scale server rotation setup.</li>
<li>The performance of the snapshot generation feature was also evaluated, showing that Farish can maintain relatively low control plane bandwidth usage.</li>
</ol>
<h2 id="usenix-atc-23-cxl-anns-software-hardware-collaborative-memory-disaggregation-and-computation">USENIX ATC '23 - CXL-ANNS: Software-Hardware Collaborative Memory Disaggregation and Computation...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=fa66gZiiF58">https://www.youtube.com/watch?v=fa66gZiiF58</a></p>
<ol>
<li>CXL-based ANN (Approximate Nearest Neighbor) search for billion-scale data sets</li>
<li>Collaborative software and hardware approach to overcome CXL's limitations</li>
<li>Leverage local caching at endpoint side, accelerate distance calculation, reduce slow storage access</li>
<li>Implemented in a FPGA-based rail system prototype with one CXL CPU and four endpoints via a switch</li>
<li>Evaluation results: 38x better performance compared to Oracle system, 59% hit rate for local caching graph on a billion-point data set, 21x less data transfer overhead using acceleration endpoint</li>
<li>Recommends XLRNS (Xtreme Large-scale Retrieval Network System) for high scalability and efficient data retrieval</li>
</ol>
<h2 id="usenix-atc-23-overcoming-the-memory-wall-with-cxl-enabled-ssd">USENIX ATC '23 - Overcoming the Memory Wall with CXL-Enabled SSD</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=dQDCqHYS1Lk">https://www.youtube.com/watch?v=dQDCqHYS1Lk</a></p>
<ol>
<li>Overcoming the memory wall with CXL-enabled SSDs</li>
<li>CXL (Compute Express Link) - a PCIe-based interconnect enabling direct memory access and CPU endpoint via low-store instruction</li>
<li>Potential of flash memory as a memory expansion option using CXL</li>
<li>Challenges in handling intense memory requests by flash memory: granularity mismatch, latency, and limited endurance</li>
<li>Research focus on creating a method to overcome these challenges using CXL-based flash memory design tools and physical memory tracers</li>
<li>Evaluation of existing optimization techniques for CXL-flash performance improvement, including DRM cache, MSHR prefetcher, and mishandled latency</li>
<li>Experimentation with synthetic workloads to create a trace-driven simulator model for CXL-flash devices</li>
<li>Evaluating the effectiveness of existing caching policies and prefetching algorithms in improving device performance</li>
<li>Investigation into the impact of virtual-to-physical address translation on device performance</li>
<li>Future work directions: wear leveling, gouge collection within flash memory, and studying end-to-end performance</li>
</ol>
<h2 id="usenix-atc-23-styx-exploiting-smartnic-capability-to-reduce-datacenter-memory-tax">USENIX ATC '23 - STYX: Exploiting SmartNIC Capability to Reduce Datacenter Memory Tax</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=lfupho5_iNA">https://www.youtube.com/watch?v=lfupho5_iNA</a></p>
<ol>
<li>Data center memory trends and challenges</li>
<li>Stagnant DRAM technology</li>
<li>Increase in memory demand</li>
<li>Memory cost dominates data center costs</li>
<li>Memory optimization control features</li>
<li>Kernel Same-page Merging (KSM)</li>
<li>Compressed Test Swap Pages (OneSwapCompress)</li>
<li>Evaluation of OneSwapCompress and Stacks framework</li>
<li>Reduction in 99th percentile latency by up to 624x for KSM and 111x for OneSwapCompress</li>
<li>Significant CPU cycle reduction (up to 47%)</li>
<li>Stacks effectively leverages smarting capabilities (compute, RDMA)</li>
<li>SmartInk offloads memory-intensive operations, reducing interference and CPU cycles</li>
<li>Future work and impact</li>
<li>Further optimization of remote execution phase</li>
<li>Evaluate stack's impact on current applications</li>
<li>Utilize smarting resources efficiently in data centers</li>
</ol>
<h2 id="usenix-atc-23-change-management-in-physical-network-lifecycle-automation">USENIX ATC '23 - Change Management in Physical Network Lifecycle Automation</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=dmX_UXavC5o">https://www.youtube.com/watch?v=dmX_UXavC5o</a></p>
<ol>
<li>Google Jupiter B4 Network Architecture</li>
<li>Challenges faced in managing scale, safety, deployment efficiency, and interoperability</li>
<li>MALT: Model-based Abstraction for Lifecycle Topology</li>
<li>Entity-Relationship Graph Model</li>
<li>Versioned Immutable Sharded Consistent Collection Model</li>
<li>Four key model generation systems:</li>
<li>Design Service<ul>
<li>Mutates current and future network models</li>
<li>Asynchronous Point of Workflow Steps</li>
<li>Resource Reservation</li>
<li>Build Service<ul>
<li>Distributed Data Flow Graph Execution Engine</li>
<li>Caching for efficient execution</li>
</ul>
</li>
<li>Query Service<ul>
<li>Understands network evolution and new technology concepts</li>
<li>Semantic Canned Queries</li>
</ul>
</li>
</ul>
</li>
<li>Topo Plan: Manage future network topology<ul>
<li>Similar to software version control system</li>
<li>Supports branching, committing, rebasing</li>
</ul>
</li>
<li>Improved overall deployment time through project concurrency, dependency tracking, and flexible planning</li>
<li>MALT's common modeling language and consistent authoritative view of network topology at any given point in time</li>
</ol>
<h2 id="usenix-atc-23-aasclepius-monitoring-diagnosing-and-detouring-at-the-internet-peering-edge">USENIX ATC '23 - AAsclepius: Monitoring, Diagnosing, and Detouring at the Internet Peering Edge</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=Fb0dCFZMLlU">https://www.youtube.com/watch?v=Fb0dCFZMLlU</a></p>
<ol>
<li>Common Internet Faults:</li>
<li>Link Congestion</li>
<li>Weak Cut Cloud Service</li>
<li>High Package Loss Rate</li>
<li>Key Components of the Solution:</li>
<li>Monitoring and Real-time Processing</li>
<li>Automated Processes for Reducing Packet Loss Rate</li>
<li>Classification of Faults:</li>
<li>Cloud Faults (Easiest to Detect)</li>
<li>Client Faults (Client Force)</li>
<li>Middle Fold Faults (Internet Network)</li>
<li>Key Strategies for Dealing with Faults:</li>
<li>Localizing the Fault</li>
<li>Identifying the Direction of the Fault</li>
<li>Deterring Traffic to Affected Areas</li>
<li>Existing Solutions:</li>
<li>Step-by-Step Process</li>
<li>Photo Localization</li>
<li>Auto Article Appeal (Automates Entire Process)</li>
<li>Future Plans for Improvement:</li>
<li>Huawei Cloud Network's Three-Year Plan to Protect Against Major Accidents</li>
<li>Key Components of the Automated System:</li>
<li>Monitor Subsystem</li>
<li>Fault Monitoring and Diagnosis System</li>
<li>Deterrent Subsystem</li>
<li>Key Features of Each Component:</li>
<li>Monitor Subsystem: Active IP Collector, Qs Monitor</li>
<li>Fault Monitoring and Diagnosis System: Decision Tree Instance, four-classifier, four-debugger</li>
<li>Deterrent Subsystem: Traffic Detouring Mechanism for Cloud Side, Client Side, Middle Fold Fault Debugging</li>
<li>Key Findings from the Study:</li>
<li>Identifying and Filtering Transient Faults</li>
<li>Effective Default Classification of Victim Assets</li>
<li>Detection of Ambiguous Faults (Account 7%)</li>
<li>Performance Evaluation Metrics:<ul>
<li>Average Packet Loss Rate</li>
<li>Time to Detect and Resolve Faults</li>
</ul>
</li>
</ol>
<h2 id="usenix-atc-23-deploying-user-space-tcp-at-cloud-scale-with-luna">USENIX ATC '23 - Deploying User-space TCP at Cloud Scale with LUNA</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=TK1vEDOopX4">https://www.youtube.com/watch?v=TK1vEDOopX4</a></p>
<ol>
<li>Luna is a user-space TCP implementation designed for Alibaba Cloud Storage.</li>
<li>It was introduced to address the limitations of kernel TCP in cloud storage scenarios, such as high overhead and interruption.</li>
<li>Key features of Luna include:</li>
<li>Run completion mode</li>
<li>Transparent 4-stack zero called "pay application"</li>
<li>User space memory slab debuff</li>
<li>Z Buff memory pool</li>
<li>Careful memory address arrangement to gather metadata buffer addresses</li>
<li>Collaboration with the kernel Network stack using leaf complex error-prone control plans and away flow director technology</li>
<li>Luna outperforms existing user TCP solutions, including mtcp and OPP, in terms of latency and throughput.</li>
<li>Production deployment of Luna has shown improvements in EBS throughput (50%) and MTS end-to-end latency (reduced by 50% to 68ms).</li>
</ol>
<h2 id="usenix-atc-23-rubbledb-cpu-efficient-replication-with-nvme-of">USENIX ATC '23 - RubbleDB: CPU-Efficient Replication with NVMe-oF</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=1Sjf79QyvrM">https://www.youtube.com/watch?v=1Sjf79QyvrM</a></p>
<ol>
<li>CPU efficiency improvements in replicated key-value stores</li>
<li>Log structure merge tree example and its background compaction job</li>
<li>Redundant compaction in replication groups can be removed, leading to a more efficient system</li>
<li>NVMeOF protocol helps remove redundant compaction and save CPU cost</li>
<li>Nvme call Target offloading feature reduces remote disk traffic</li>
<li>SSD file preallocation in RoboDB ensures data consistency when shipping SD files</li>
<li>Bitmap records help manage unused space in the file pool</li>
<li>Evaluation results show that RoboDB improves performance and reduces tail latency, especially for read-heavy workloads</li>
<li>Trade-offs between CPU usage and network traffic should be considered when designing replicated storage systems</li>
</ol>
<h2 id="usenix-atc-23-distributed-transactions-at-scale-in-amazon-dynamodb">USENIX ATC '23 - Distributed Transactions at Scale in Amazon DynamoDB</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=3OpEIMR-ml0">https://www.youtube.com/watch?v=3OpEIMR-ml0</a></p>
<ol>
<li>DynamoDB is a NoSQL database that supports full asset transactions, providing predictable performance and unbounded scale.</li>
<li>A key feature of DynamoDB is its ability to handle high traffic applications with consistent performance, even as workloads increase.</li>
<li>The speaker discusses the implementation of transactions in DynamoDB using two new APIs: Transact Get Item and Transact Put Item.</li>
<li>These APIs allow developers to model complex transactions involving multiple items within a single request, providing greater flexibility and control.</li>
<li>The speaker also highlights the importance of check item operations, which enable developers to verify conditions before executing a transaction.</li>
<li>DynamoDB's two-phase commit protocol ensures that transactions are executed in a serialized manner, preventing conflicts and maintaining data consistency.</li>
<li>The use of timestamp ordering helps to ensure that transactions are processed in the correct order, even when multiple coordinators are involved.</li>
<li>The speaker emphasizes the importance of latency optimization and the impact of non-transactional workloads on overall performance.</li>
<li>DynamoDB's support for full asset transactions allows developers to build highly scalable applications with predictable performance, even under heavy load.</li>
</ol>
<h2 id="usenix-atc-23-prefix-siphoning-exploiting-lsm-tree-range-filters-for-information-disclosure">USENIX ATC '23 - Prefix Siphoning: Exploiting LSM-Tree Range Filters For Information Disclosure</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=NgVVmu39u7g">https://www.youtube.com/watch?v=NgVVmu39u7g</a></p>
<ol>
<li>Presentation on perfect siphoning information disclosure attack using LSM tree with range filter</li>
<li>Attack leverages internal mechanism of key-value stores and reveals sensitive data</li>
<li>Range filter is a crucial part in the attack, as it can be exploited to retrieve full keys and expose sensitive information</li>
<li>Prefix siphoning is an effective way to extract full keys by exploiting range filters</li>
<li>The attack consists of finding false positive keys, identifying shared prefixes, and scanning possible suffixes</li>
<li>Demonstrated using RoxDB, a popular LSM tree implementation</li>
<li>Attack showed significant performance improvement but highlighted the security vulnerabilities in range filters</li>
<li>Mitigation strategies discussed include securing key-value stores, limiting user requests, and improving filter designs</li>
</ol>
<h2 id="usenix-atc-23-epf-evil-packet-filter">USENIX ATC '23 - EPF: Evil Packet Filter</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=pkR5kJkFGkc">https://www.youtube.com/watch?v=pkR5kJkFGkc</a></p>
<ol>
<li>EPF pack filter joint work with Takis and Vasilias from Brown University</li>
<li>Background: Current security exploitation, BPF enables new attack, Kernel Security is a lucrative target for attackers</li>
<li>Packet Filter (PPF) allows attackers to escalate privileges by hijacking control flow in kernel space</li>
<li>Red User type attack: Attackers can create large amounts of content in kernel space and use BPF programs to reuse kernel code</li>
<li>Classic PPF and Extended PPF (EBPF) are available in the Linux kernel, with EBPF offering newer functionality</li>
<li>BPF background: Used for packet filtering, system call filtering, network routing, etc.; two types of BPF programs exist - classic and extended</li>
<li>Attack using BPF code reuse payload: Attackers create a BPF program that they can control, then use it to hijack the BPF interpreter's flow and execute malicious code in kernel space</li>
<li>Code reusing BPF interpreter way: By controlling the immediate field of a BPF instruction, attackers can reuse unverified, unsafe code inside a verified BPF program</li>
<li>Attack escalates privilege by overwriting an attacker process's credentials</li>
<li>Three defense mechanisms are proposed: BPF ISR (Instruction Set Randomization), PPX NX (Nonexecutable Memory), and BPF CFI (Control Flow Integrity)</li>
</ol>
<h2 id="usenix-atc-23-translation-pass-through-for-near-native-paging-performance-in-vms">USENIX ATC '23 - Translation Pass-Through for Near-Native Paging Performance in VMs</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=e1DrrIKU0Ko">https://www.youtube.com/watch?v=e1DrrIKU0Ko</a></p>
<ol>
<li>Virtual machine (VM) performance optimization is crucial in virtualized systems.</li>
<li>Memory address translation overhead is a significant factor affecting VM performance.</li>
<li>Traditional nested paging and Shadow paging techniques cause 24x memory access overhead for single guest virtual address to host physical address translation.</li>
<li>Translation Passthrough (TPT) is proposed as an efficient solution to manage memory translations in VMs.</li>
<li>TPT aims to provide self-managed, direct guest VM-to-host memory translation with native performance and efficient protection isolation for VMs.</li>
<li>The design of TPT includes a read-only map (VM Readonly Map) that directly maps guest physical addresses to host physical addresses, allowing the virtual machine to construct its passthrough page table and perform translations directly.</li>
<li>TPT also enforces efficient protection isolation by utilizing hardware components like Page Frame Permission Tag Maps to ensure that VMs only access allocated physical pages.</li>
<li>TPT has been evaluated on various workloads, including KC Bench (kernel compilation) and Pagerank benchmarks, demonstrating its ability to achieve efficient memory translation and page table management with minimal overhead compared to traditional nested paging and Shadow paging techniques.</li>
</ol>
<h2 id="usenix-atc-23-efficient-memory-overcommitment-for-io-passthrough-enabled-vms-via-fine-grained">USENIX ATC '23 - Efficient Memory Overcommitment for I/O Passthrough Enabled VMs via Fine-grained...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=IeDlXZLyXaA">https://www.youtube.com/watch?v=IeDlXZLyXaA</a></p>
<ol>
<li>Introduced efficient approach to memory commitment using IO Passthrough (Ill-Pas) in VMs</li>
<li>Illustrated background of memory commitment and its issues in VM environments</li>
<li>Explained the concept of IO Passthrough, which allows guest OS direct interaction with underlying hardware without CPU intervention</li>
<li>Discussed two main solutions to address the contradiction between memory commitment and I/O Passthrough: <ul>
<li>Adding paid support for hardware devices that trigger page port similar to how CPUs do</li>
<li>Monitoring DMA buffer allocation within guests and notifying the hypervisor when a page is reclaimed</li>
</ul>
</li>
<li>Provided an overview of the overall architecture of Wiprobe, which includes three main components: Data Manager, Memory Formation Routine, and Wave Probe Injector</li>
<li>Detailed the process of registering helper functions for page metadata layout detection and verification</li>
<li>Described the process of scanning memory to identify free pages and reclaim them using a hypothetical example</li>
<li>Explained the concept of fine-grained page metadata management, which helps avoid DMA failures and maintain high service level objectives</li>
<li>Discussed the challenges faced in implementing this solution, such as ensuring data consistency and avoiding fragmentation</li>
<li>Presented experimental results comparing the performance of Wiprobe with baseline methods and other memory reclamation techniques</li>
</ol>
<h2 id="usenix-atc-23-lpns-scalable-and-latency-predictable-local-storage-virtualization-for">USENIX ATC '23 - LPNS: Scalable and Latency-Predictable Local Storage Virtualization for...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=iD4CdPmtagU">https://www.youtube.com/watch?v=iD4CdPmtagU</a></p>
<ol>
<li>Background and Motivation:</li>
<li>Emphasis on NVMe virtualization in cloud environments to provide latency predictability and QoS control.</li>
<li>LPNs Design and Implementation:</li>
<li>Scalable architecture with LPNs as the key component.</li>
<li>Use of RPM (Resource Pool Manager) model for collaborative work with original NVMe drivers.</li>
<li>VM-level latency predictor to monitor workload performance data and perform self-feedback.</li>
<li>QoS Control and Latency Predictability:</li>
<li>LPNs provide reliable latency, predictable QoS control, and efficient virtualization scenario management.</li>
<li>Evaluation and Overhead Analysis:</li>
<li>Use of microbenchmarks and application benchmarks to demonstrate LPNs' performance.</li>
<li>Show that LPNs have a much smaller overhead compared to traditional software-level virtualization designs.</li>
<li>Key Contributions:</li>
<li>Flexible, scalable polling mechanism for better latency control.</li>
<li>Hardware Cube Pool and QoS resource management for efficient load handling.</li>
<li>Comparison with Other Solutions:</li>
<li>LPNs outperform IBM's SVM and other virtual storage solutions in terms of latency predictability and QoS control.</li>
<li>Future Work:</li>
<li>Further enhancements to address random read/write performance issues and improve overall performance.</li>
</ol>
<h2 id="usenix-atc-23-p2cache-exploring-tiered-memory-for-in-kernel-file-systems-caching">USENIX ATC '23 - P2CACHE: Exploring Tiered Memory for In-Kernel File Systems Caching</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=oqm207ZscPg">https://www.youtube.com/watch?v=oqm207ZscPg</a></p>
<ol>
<li>PSquare Cache: A novel kernel casting mechanism for persistent memory (PM) specialty systems, leveraging read-write distinguishable memory hierarchy and lightweight Operation Log for efficient data locking.</li>
<li>Three main design choices of PSquare Cache: </li>
<li>Rewrite Memory Hierarchy: Distinguishes read/write operations in PM DRAM memory hierarchy.</li>
<li>Lightweight Operation Log: Efficiently captures system updates, ensuring instant data and metadata durability with atomic update lockdown pointer.</li>
<li>Leveraging PM's light adjustability: Persistently updates data efficiently, especially for small right-hand (SRH) operations.</li>
<li>PSquare Cache significantly outperforms legacy kernel file systems in performance tests, demonstrating a threefold improvement over the Nova solution and a 10x improvement over the EST4 system.</li>
<li>PSquare Cache is open-source on GitHub, with four main design choices: Rewrite Memory Hierarchy, Lightweight Operation Log, Leveraging PM's light adjustability, and Persistent Cache.</li>
</ol>
<h2 id="usenix-atc-23-revisiting-secondary-indexing-in-lsm-based-storage-systems-with-persistent-memory">USENIX ATC '23 - Revisiting Secondary Indexing in LSM-based Storage Systems with Persistent Memory</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=oqQlKjBiwM4">https://www.youtube.com/watch?v=oqQlKjBiwM4</a></p>
<ol>
<li>LSM3 storage engine and Kerala Store Database System</li>
<li>Highrise performance with SM Tree, new calculator pair, blind white manner</li>
<li>Secondary indexing inefficiencies and RSM3's advantages and disadvantages</li>
<li>Problems with synchronous strategy and previous validation strategies</li>
<li>PM-based secondary indexes: pros and cons, three approaches to implement them</li>
<li>PS3 (Persistent Sphere): a promising approach that combines LSM tree and hash table for secondary indexing</li>
<li>Two optimization techniques for non-index queries: region error trace feature and peaky group design</li>
<li>Hybrid PM/DM Hash Table based validation approach</li>
<li>Long index query set searching primary table optimization</li>
<li>Worker active scheme for better resource utilization in multi-threaded scenarios</li>
<li>Evaluation results comparing PS3, LSM3, and other secondary indexing methods</li>
</ol>
<h2 id="usenix-atc-23-zhuque-failure-is-not-an-option-its-an-exception">USENIX ATC '23 - Zhuque: Failure is Not an Option, it’s an Exception</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=yC_oMzOb02Q">https://www.youtube.com/watch?v=yC_oMzOb02Q</a></p>
<ol>
<li>Introduced new programming model for flush-fail persistent memory systems</li>
<li>Developed a simple pre-programming model using flush-fail semantics</li>
<li>Built and tested an ellipse-based prototype implementation</li>
<li>Found Zookeeper (ZooKeeper) outperforms state-of-the-art programming PM systems</li>
<li>Proposed a new programming model called "whole process persistence" for user-space applications</li>
<li>Demonstrated the need for platform support to ensure cache flushing during power failures</li>
<li>Presented performance results comparing ZooKeeper with Memcached D, showing improved throughput and minimal overhead</li>
<li>Showcased the benefits of using a flush-fail persistent memory system in a wide range of applications</li>
</ol>
<h2 id="usenix-atc-23-envpipe-performance-preserving-dnn-training-framework-for-saving-energy">USENIX ATC '23 - EnvPipe: Performance-preserving DNN Training Framework for Saving Energy</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=RfAq1JbHbXI">https://www.youtube.com/watch?v=RfAq1JbHbXI</a></p>
<ol>
<li>Climate change and AI energy consumption are major concerns.</li>
<li>DNN training framework, MVpipe, is designed to save energy while maintaining accuracy and performance.</li>
<li>MVpipe uses pipeline parallelism to efficiently train large models without sacrificing performance or accuracy.</li>
<li>Key components of MVpipe include a profiler, scheduler, and frequency planner.</li>
<li>The profiler measures performance and energy trends for each stage of the pipeline.</li>
<li>The scheduler maximizes the use of "bubbles" (idle time) in the pipeline to save energy without degrading performance.</li>
<li>The frequency planner adjusts GPU frequencies to minimize performance degradation while maximizing energy savings.</li>
<li>MVpipe saves 28% energy and has only 1% performance degradation compared to baseline methods.</li>
</ol>
<h2 id="usenix-atc-23-decentralized-application-level-adaptive-scheduling-for-multi-instance-dnns-on">USENIX ATC '23 - Decentralized Application-Level Adaptive Scheduling for Multi-Instance DNNs on...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=6ToXzky1Dl4">https://www.youtube.com/watch?v=6ToXzky1Dl4</a></p>
<ol>
<li>Introduced decentralized application Level adaptive scheduling multiinstance DNS</li>
<li>Discussed challenges in mobile devices: limited resources, compatibility issues, performance degradation, and privacy concerns</li>
<li>Profiled different DNS configurations on standalone devices</li>
<li>Presented TF-Learn model code generation for GPU accelerator usage</li>
<li>Focused on application-specific configuration control and their impact on user experience and device performance</li>
<li>Explained the need to consider OS level scheduling and its influence on underlying computing units</li>
<li>Discussed the importance of a centralized scheduler that can adapt to dynamic environments and strong isolation among apps</li>
<li>Proposed a semisupervised deep reinforcement learning path algorithm for app adoption, which could learn from the environment and predict optimal actions</li>
<li>Presented a decentralized DQN-based scheduler design that can select corresponding actions based on specific code generation configurations</li>
<li>Introduced two major concerns in running DNA models on mobile devices: latency and power consumption</li>
<li>Proposed a user-level solution that deals with the effect of schedulers underlining operating systems</li>
<li>Discussed how OS environment changes can affect agent flexibility and handle private features</li>
<li>Presented an evaluation software and hardware setup for testing DNS models on mobile devices</li>
<li>Described three profiling experiments conducted on different devices and their results</li>
<li>Introduced a machine learning-based decision model that takes time to learn the dynamic nature of mobile systems, but converges to optimal solutions eventually</li>
<li>Showcased how decentralized scheduling can match speed and stability in various scenarios</li>
<li>Raised the question of whether solutions can work without uncontrolled apps and proposed a way to handle such situations using the "detox" method</li>
<li>Concluded by highlighting the contributions of their work, which include creating decentralized application-level scheduling, ensuring privacy and performance for Edge apps, and providing insights into OS-level scheduling and its influence on underlying computing units</li>
</ol>
<h2 id="usenix-atc-23-unfaasener-latency-and-cost-aware-offloading-of-functions-from-serverless-platforms">USENIX ATC '23- UnFaaSener: Latency and Cost Aware Offloading of Functions from Serverless Platforms</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=Yk9N5Ui6oaI">https://www.youtube.com/watch?v=Yk9N5Ui6oaI</a></p>
<ol>
<li>Serverless computing is popular and complex, with a growing market.</li>
<li>Azure Workload Characterization (2019-2020) shows majority applications are using functions.</li>
<li>Proposal: floating function to leverage unused capacity in VMs across different clouds.</li>
<li>Serverless functions are stateless and can execute anywhere.</li>
<li>Execution time of serverless functions is getting shorter, with median execution times around 16 ms in 2020.</li>
<li>Design applications for disaggregated compute, storage, and database.</li>
<li>Use Popup Messaging System to communicate between scheduler, publishers, and subscribers.</li>
<li>Scheduling goal: opportunistic harvesting of resources.</li>
<li>Optimization problem: minimize cost while considering latency, execution delay, and resource availability.</li>
<li>Cost model includes VMS costs, data movement costs, and cloud provider costs.</li>
<li>Use Azure Data Studio to analyze and optimize prediction scores for resource allocation.</li>
<li>Offloading functions can happen using Google Cloud CLI or AWS CLI.</li>
<li>Using Popup Messaging System allows for fault tolerance and efficient communication between hosts.</li>
<li>Orchestration systems like Google Workflow and AWS Step Functions have cost implications.</li>
<li>Some applications offer significant cost savings by moving to a serverless architecture, such as video analytics (90% cost saving).</li>
</ol>
<h2 id="usenix-atc-23-llfree-scalable-and-optionally-persistent-page-frame-allocation">USENIX ATC '23 - LLFree: Scalable and Optionally-Persistent Page-Frame Allocation</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=yvd3D5VOHc8">https://www.youtube.com/watch?v=yvd3D5VOHc8</a></p>
<ol>
<li>Problem: Traditional memory management subsystems struggle with parallelism and scalability, leading to bottlenecks in modern systems with multiple cores and large memories.</li>
<li>Solution: Develop a new allocator (LA3) that is scalable, optionally persistent, and handles huge pages efficiently.</li>
<li>Collaboration: Joint project between Hamburg University of Technology and Let's Fork Vienna (LFV).</li>
<li>Design Principles: </li>
<li>Respect Hardware: Consider page size, cache line granularity, and other hardware-specific factors.</li>
<li>Avoid Memory Sharing: Minimize contention and conflicts between CPUs accessing the same memory regions.</li>
<li>Careful Redundancy: Implement efficient mechanisms for handling persistence and crash consistency.</li>
<li>Architecture Overview: </li>
<li>Large Bit Field: Use a large bit field to represent allocated and free pages, allowing efficient parallel access.</li>
<li>Index-like Data Structure: Group bits into blocks and use an index-like data structure for efficient searching and allocation.</li>
<li>Huge Page Entry: Allocate huge pages using a single atomic instruction, reducing fragmentation and improving performance.</li>
<li>Persistency and Recovery: </li>
<li>Atomic Instructions: Use atomic instructions to update allocator state directly, ensuring consistent crash recovery.</li>
<li>Log-based Approach: Implement a log-based approach for efficient allocation and deallocation of persistent memory.</li>
<li>Performance Evaluation: </li>
<li>Benchmark Results: LA3 outperforms Linux allocator in terms of raw performance, parallelism, and fragmentation management.</li>
<li>Future Work: Continue researching and optimizing the LA3 allocator for improved scalability, persistence, and efficiency in modern computing systems.</li>
</ol>
<h2 id="usenix-atc-23-singularfs-a-billion-scale-distributed-file-system-using-a-single-metadata-server">USENIX ATC '23 - SingularFS: A Billion-Scale Distributed File System Using a Single Metadata Server</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=bjzU3RhvBXg">https://www.youtube.com/watch?v=bjzU3RhvBXg</a></p>
<ol>
<li>Background motivation: Distributed file systems dominate modern data centers, and single-medium server solutions are desirable for large-scale file systems.</li>
<li>Challenges of existing solutions: Crash consistency overhead, concurrency control issues, and scalability limitations create performance gaps in existing distributed file systems.</li>
<li>Singular FS design goals: Optimize medium storage performance, leverage log-free method operation, and utilize hierarchical concurrency control for better parallelism.</li>
<li>Hybrid anode partition: Combines single-node operations with double-node operations to ensure locality and minimize critical areas.</li>
<li>Log-free method operation: Eliminates transactional overhead by converting non-transactional KV operations into a log-free format, ensuring crash consistency without additional costs.</li>
<li>Hierarchical concurrency control: Minimizes critical areas by classifying operations related to inodes into three categories (updater, reader, writer) and implementing optimistic concurrency control based on timestamps.</li>
<li>Evaluation results: Singular FS shows lower latency and higher throughput compared to local PM file systems and distributed file systems, making it a viable solution for large-scale file systems.</li>
</ol>
<h2 id="usenix-atc-23-the-hitchhikers-guide-to-operating-systems">USENIX ATC '23 - The Hitchhiker's Guide to Operating Systems</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=crWJEi6udNU">https://www.youtube.com/watch?v=crWJEi6udNU</a></p>
<ol>
<li>State machine as a teaching method for operating systems</li>
<li>Emphasis on rigorous mathematical object definition and understanding</li>
<li>Importance of model-checking tools in teaching concurrency and system cost</li>
<li>Building emulators to clarify concepts and understand end-to-end system behavior</li>
<li>Using Unix philosophy principles in teaching operating systems</li>
<li>Symbolic verification as a tool for verifying real systems</li>
<li>Sharing course materials publicly and receiving positive feedback from students</li>
</ol>
<h2 id="usenix-atc-23-accelerating-distributed-moe-training-and-inference-with-lina">USENIX ATC '23 - Accelerating Distributed MoE Training and Inference with Lina</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=Vsb4j6hHAaA">https://www.youtube.com/watch?v=Vsb4j6hHAaA</a></p>
<ol>
<li>Moe (Mixed Expert) is a mixture expert system that combines multiple experts to improve performance and efficiency.</li>
<li>It has been applied in various NLP tasks using Transformer models, such as Glam Moe and Deep Speed's Moe model.</li>
<li>The main idea behind Moe is to select the most suitable expert for each input data sample based on a gating network that assigns weights to each expert output.</li>
<li>Moe architecture aims to benefit from sparsity computation, which reduces the number of flops and allows for larger models with low computational cost.</li>
<li>Training involves balancing loss across experts to ensure even data distribution among them. This is achieved by introducing a low-balancing loss added during training and removing it in the inference stage.</li>
<li>Distributed systems are often used to efficiently train and infer Moe models, as they can leverage parallelism and reduce communication costs.</li>
<li>One of the main challenges with Moe models is the performance bottleneck caused by Ottawa (a communication-intensive operation) during distributed training and inference.</li>
<li>To address this issue, Lina (Linear Scheduler) has been proposed to improve resource allocation and scheduling in Moe models. It aims to balance load among devices in a cluster and minimize the impact of performance bottlenecks.</li>
<li>Lina introduces a two-phase scheduling process: phase one involves estimating expert popularity based on token selection patterns, while phase two fine-tunes resource allocation based on actual routing decisions.</li>
<li>Evaluation results show that Lina can significantly improve the efficiency of Moe models during both training and inference stages, achieving near-optimal performance with minimal overhead.</li>
</ol>
<h2 id="usenix-atc-23-smartmoe-efficiently-training-sparsely-activated-models-through-combiningand">USENIX ATC '23 - SmartMoE: Efficiently Training Sparsely-Activated Models through Combiningand...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=iGDt-nzTofY">https://www.youtube.com/watch?v=iGDt-nzTofY</a></p>
<ol>
<li>Moe (Modeling Overlapping and Excluding Networks) is a technique for training large-scale models efficiently by combining offline and online parallelizations.</li>
<li>The Pacman Lab at Qinghua University has been researching this method, with the goal of scaling extremely large models while accepting the increased computational cost.</li>
<li>Moe's key features are its ability to handle imbalanced workloads and dynamic properties in training.</li>
<li>Ideal systems should address both the imbalanced property (workload wear) and the dynamic property (ever-changing workload).</li>
<li>Smart Moe proposes a two-stage automatic parallelization approach: </li>
<li>Stage 1: Offline search for a comprehensive search space, constructing a candidate pool containing promising execution plans.</li>
<li>Stage 2: Online selection of an optimal plan from the pool, improving efficiency and pruning unnecessary options.</li>
<li>Smart Moe also supports hybrid parallelism to enhance the overall performance of training large-scale models.</li>
<li>The system's effectiveness is validated through various experiments and comparisons with other strong baselines like DeepSpeed Moe, Faster Moe, and Tell Alpha.</li>
</ol>
<h2 id="usenix-atc-23-msrl-distributed-reinforcement-learning-with-dataflow-fragments">USENIX ATC '23 - MSRL: Distributed Reinforcement Learning with Dataflow Fragments</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=f14YBz2v-OU">https://www.youtube.com/watch?v=f14YBz2v-OU</a></p>
<ol>
<li>The speaker presents the MSRL distributed reinforcement learning system using a data flow fragment approach.</li>
<li>They mention that classical RL training systems have several challenges, including large models, accelerating entire training loops, and handling diverse algorithms.</li>
<li>They introduce three types of RL systems: function-based, data flow-based, and actor-critic-based.</li>
<li>The speaker emphasizes the importance of a flexible distribution policy to support different communication requirements for various RL algorithms.</li>
<li>They introduce MSR (Microsoft Reinforcement Learning), a system that decouples algorithm specification from operational execution using three key technologies: defining computation and data flow graphs, accelerating heterogeneous devices with fragmented data flow graphs, and supporting flexible distribution policies.</li>
<li>The speaker provides examples of how MSR can be used to develop and execute RL algorithms efficiently on different hardware devices.</li>
<li>They discuss the importance of selecting appropriate distribution policies based on the user's specific needs and hardware environment.</li>
<li>Finally, they present experimental results comparing the performance of MSR with other RL systems like MSL (Microsoft CNTK), demonstrating that MSR can accelerate entire training loops and efficiently process large amounts of data in distributed environments.</li>
</ol>
<h2 id="usenix-atc-23-beware-of-fragmentation-scheduling-gpu-sharing-workloads-with-fragmentation">USENIX ATC '23 - Beware of Fragmentation: Scheduling GPU-Sharing Workloads with Fragmentation...</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=-37_clawjdc">https://www.youtube.com/watch?v=-37_clawjdc</a></p>
<ol>
<li>Introduced a novel scheduling strategy called "Fermentation Gradient Descent (FGD)" for efficient utilization of GPU clusters in machine learning tasks.</li>
<li>Existing approaches like beam packing fail to address fragmentation issues, leading to idle GPUs and underutilized resources.</li>
<li>FGD is a heuristic scheduling algorithm that minimizes fermentation (unallocated GPU resource) by allocating tasks towards the steepest descent in the fermentation gradient.</li>
<li>The proposed approach leverages a statistical measure called "fermentation likelihood" to quantify expected unallocated GPU resources given target workload distribution.</li>
<li>FGD outperforms existing scheduling strategies, reducing ultimate unallocated GPUs and improving overall cluster utilization.</li>
</ol>
<h2 id="usenix-atc-23-towards-iterative-relational-algebra-on-the-gpu">USENIX ATC '23 - Towards Iterative Relational Algebra on the GPU</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=NUIA_c-dOzI">https://www.youtube.com/watch?v=NUIA_c-dOzI</a></p>
<ol>
<li>Introduced GPU-based High Performance Hashing Table (HTBL) for Iterative Relational Algebra</li>
<li>Optimized operations by fusing join, projection, and deduplication</li>
<li>Implemented efficient GPU memory management system using CUDA</li>
<li>Overcame deduplication challenges in iterative relational algebra computation</li>
<li>Targeted single GPU architecture with potential memory overflow issues for larger graphs</li>
<li>Future directions include developing multi-node, multi-GPU solutions and exploring different parallel programming models</li>
</ol>
<h2 id="usenix-atc-23-vectorvisor-a-binary-translation-scheme-for-throughput-oriented-gpu-acceleration">USENIX ATC '23 - VectorVisor: A Binary Translation Scheme for Throughput-Oriented GPU Acceleration</h2>
<p>URL: <a href="https://www.youtube.com/watch?v=TElCneM9UxQ">https://www.youtube.com/watch?v=TElCneM9UxQ</a></p>
<ol>
<li>GPU programming has become a popular field for accelerating workloads, especially in machine learning applications.</li>
<li>The transition from CPUs to GPUs for general-purpose computing began around 2007.</li>
<li>Modern GPUs offer high performance and parallelism but come with complex programming models and limited support for certain language features.</li>
<li>GPU programming requires careful consideration of hardware characteristics, such as register file sizes and memory coalescing rules.</li>
<li>Existing CPU code cannot be directly run on GPUs without rewriting or optimizing the program for parallel execution.</li>
<li>WebAssembly provides an intermediate representation that can be used to compile and run GPU programs across multiple vendor platforms.</li>
<li>The Vector Vizor compiler uses a novel approach called Continuation Passing Style (CPS) to preemptively pause and resume GPU programs, allowing for more efficient use of hardware resources.</li>
<li>The Vector Vizor compiler can improve the throughput-per-dollar ratio of CPU-based systems by offloading certain tasks to GPUs without requiring significant modifications to existing codebases.</li>
<li>Future improvements in GPU technology and programming models will continue to drive innovation in server-side workload acceleration.</li>
</ol>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.d7c377c4.min.js"></script>
      
    
  </body>
</html>