## Developer keynote (Google I/O '23)

URL: [https://www.youtube.com/watch?v=r8T0SnwHRNI](https://www.youtube.com/watch?v=r8T0SnwHRNI)

- Google I/O 2022: Key Announcements and Updates
    - MakerSuite: A new AI-powered tool for creating customized, synthetic data sets.
    - PaLM 2: An updated version of Google's largest multimodal language model (PaLM), offering improved performance and capabilities.
    - Jetpack Compose 1.3: The latest update to Google's UI design toolkit, bringing improvements in performance, usability, and developer productivity.
    - Kotlin 1.8: An updated version of the popular programming language, with new features and enhancements for better interoperability with Java and improved tooling support.
    - Android Studio Bumblebee: A new version of Google's integrated development environment (IDE) for building apps on the Android platform, featuring improved performance, enhanced developer productivity tools, and better integration with other Google services and platforms.
    - TensorFlow 2.10: The latest update to Google's open-source machine learning framework, bringing improvements in performance, usability, and developer productivity.
    - Flutter 3.0: An updated version of Google's popular UI toolkit for building cross-platform mobile apps, featuring improved performance, enhanced developer productivity tools, and better integration with other Google services and platforms.
    - WebGPU: A new API that enables developers to leverage the power of GPU hardware directly in their web applications, improving performance and enabling more complex visual effects.
    - Baseline 2023: An initiative aimed at providing a consistent, predictable view of the web platform for developers, by establishing a set of stable, well-supported features that are widely supported across different browsers and platforms.
    - Project Gameface: A demonstration of how AI and machine learning technologies can be used to enable users with disabilities to interact more effectively with digital devices and applications, through the use of facial landmark detection and hand gesture recognition.
    - MediaPipe: A versatile, cross-platform library for building custom AI models and applications, featuring a wide range of pre-built components and modules for common machine learning tasks, as well as powerful tools for creating and training custom models.
    - KerasCV: A new library from Google Research that combines the power of TensorFlow's Keras API with the popular OpenCV computer vision library, providing developers with a powerful set of tools and resources for building advanced AI-powered image processing and analysis applications.
    - Vertex AI: An integrated platform for building, training, and deploying custom machine learning models, featuring a wide range of pre-built components and modules for common machine learning tasks, as well as powerful tools for creating and training custom models.
    - Duet AI: A new AI-powered tool from Google Research that helps developers build more efficient and productive software applications by providing real-time code suggestions and recommendations based on the developer's current coding activity and context.
    - AppSheet: A low-code development platform that enables developers to quickly and easily build custom mobile apps, without needing to write any code or have any prior programming experience.


## Google I/O 2023 Developer Keynote in 5 minutes

URL: [https://www.youtube.com/watch?v=hleLlcHwQLM](https://www.youtube.com/watch?v=hleLlcHwQLM)

- Intro by Jeanine Banks
- Google IO conference intro music and cheers
- Announcements: 
    - Deep investment in mobile platforms, web browsers, and Cloud services
    - Launch of PaLM API
    - New MakerSuite feature for creating synthetic data
    - Studio Bot, an AI-powered helper in Android Studio
    - Flippable foldable devices optimization by Google
    - Updates to Wear OS: 
        - Beautiful watch faces with customizable power efficiency
        - Rich animation tiles and new APIs for faster app launching
    - WebGPU API: 
        - Unlocks GPU hardware capabilities
        - Makes web AI-ready
        - Part of the Web DX community group, supported by major browser vendors and framework providers
    - Baseline project for stable web development across browsers
    - Showcase of MediaPipe solutions
    - Duet AI: 
        - A new generative AI collaborator for Cloud application developers
        - Provides collaboration needs within the Cloud Console, IDE, and call Google to train models
        - Part of trusted tester program
- Closing remarks by Jeanine Banks


## What's new in Android

URL: [https://www.youtube.com/watch?v=qXhjN66O7Bk](https://www.youtube.com/watch?v=qXhjN66O7Bk)

- New Platform Capability: Jetpack Compose
- Privacy and Security Updates: Credential Manager, Privacy Sandbox, Foreground Service Changes
- Personalization Updates: Grammatical Inflection API, Share Sheet Changes
- High Quality User Experience: Predictive Back, Media3 Library updates
- New Form Factors: Foldables, Tablets, TVs, Wear OS 2
- Developer Tools and Workflows: Android Studio Giraffe, Studio Bot, App Quality Insights, Baseline Profile Gradle Plugin
- Kotlin Multiplatform Updates: Jetpack Library Annotations, Collections, DataStore
- Architecture Guidance and Best Practices: Saving UI State, Testing Best Practices


## What's new in Android Accessibility

URL: [https://www.youtube.com/watch?v=w1Fqx_2SRro](https://www.youtube.com/watch?v=w1Fqx_2SRro)

- Android 14 brings new accessibility features and improvements
    - TalkBack 14: New Actions contextual for focused Edit field, Gmail allows archive/delete email, Splittap typing allows typing without lifting finger, spell check granularity added to Reading Control, Braille table 38 language support coming soon, on-screen Braille keyboard allows editing using cursor movement and text selection
    - Hearing Accessibility: Interacting with hearing aids becomes more intuitive, notification vibration sound and flash camera screen light for silent environments, harmful listening habit alerts with headphone loud sound alert, Live Caption captions audio playback on device without internet connection
- New accessibility developer updates include:
    - Android 14 supports nonlinear font scaling to avoid text cut when increasing font size
    - New APIs added: Accessibility Data Sensitive Attribute to protect sensitive data, SetRequestInitialAccessibilityFocus API for better TalkBack user experience, and SetMinDurationBetweenContentChanges API for customizing TalkBack announcement frequency
    - Updates to the Accessibility Testing Framework include improved Android Studio integration and support for Compose UI layouts
- New resources published:
    - Guide on improving accessibility in Wear OS apps
    - Code snippet catalog demonstrating best practice accessibility-related APIs

Note: The above summary is a concise version of the given transcript. It does not contain all details but highlights key points and main ideas.


## What’s new in ChromeOS | Google I/O 2023

URL: [https://www.youtube.com/watch?v=eJ5FpTltXZU](https://www.youtube.com/watch?v=eJ5FpTltXZU)

- ChromeOS has a wide range of hardware options, including laptops, 2-in-1 devices, and desktops.
- There are 77 global retail partners for ChromeOS devices.
- The HP Elite Dragonfly Chromebook Pro is available in two versions.
- Framework Laptop Chromebook edition is a customizable laptop with sustainability features.
- Acer Chromebox Enterprise CX15 and ASUS Chromebox5 are powerful desktop PC options.
- Lenovo 100E Chromebook Gen 4, Acer Chromebook Vero 712, and other budget-friendly options are available.
- ChromeOS gaming laptops offer a variety of cloud gaming options.
- ChromeOS is constantly adding new features to improve user experience.
- The Google Workspace Education platform has over 170 million users.
- Chromebooks are the number one selling device in schools across the US, Sweden, New Zealand, Japan, Brazil, Indonesia, Singapore, and more.
- ChromeOS is used by over 22,000 businesses worldwide.
- The Chrome Enterprise Recommended program helps businesses find industry solutions validated for ChromeOS.
- ChromeOS Flex allows businesses to update old PCs and Macs with minimal investment.
- Over 46% growth in activation of ChromeOS Flex has been seen since last year.
- ChromeOS can run Android apps, which can be made installable and large screen optimized.
- The Google Photos team built a movie editor app for Chromebooks that supports large screens.
- Minecraft: Education Edition is available on select Chromebook devices.
- A Linux development environment is available on ChromeOS to help developers build apps and games.


## What's new in Dart and Flutter

URL: [https://www.youtube.com/watch?v=yRlwOdCK7Ho](https://www.youtube.com/watch?v=yRlwOdCK7Ho)

- Flutter 310: Dart team announced the release of Flutter 3.10, highlighting key features such as improved support for Material 3 widgets, new Dart language features, and enhancements in developer tools.
- Material 3 Widget Support: The latest version of Flutter includes comprehensive support for Google's Material 3 design system, including a range of adaptable widget toolkits that allow developers to create visually appealing and consistent user interfaces.
- Dart Language Enhancements: Dart 3 introduces several new language features, including support for structured data records, destructuring patterns, and class modifiers. These enhancements aim to improve productivity and code maintainability for Flutter developers.
- DevTools Improvements: The latest version of Floot also includes updates to the Flutter DevTools suite, which provides powerful profiling, debugging, and performance analysis capabilities. Key features include a new Diff Snapshots tab for comparing memory usage and a redesigned Trace Viewer powered by the Perfetto open-source toolkit.
- Web Stability: With the release of Flutter 3.10, support for web has been marked as stable, enabling developers to build high-performance web applications using Flutter's cross-platform capabilities.
- Wide Gamut Image Support: The latest version of Flutter now supports wide gamut images on iOS devices, providing users with more vibrant and immersive visual experiences.
- Impeller Renderer: Introduced in Flutter 3.10, the new Impeller renderer is designed to maximize performance and quality for Flutter applications. It leverages modern GPU capabilities and optimizes rendering processes to deliver smooth animations and responsive user interfaces.
- Native Ads Integration: Google has improved support for native ads integration within Flutter applications, enabling developers to create seamless ad experiences across various platforms without requiring extensive platform-specific code.
- WebAssembly Support: Flutter 3.10 introduces experimental support for WebAssembly, a binary instruction format designed for portable compilation and execution of high-performance web applications. This new feature allows developers to build Flutter applications using WebAssembly libraries, providing improved performance and compatibility with modern web browsers.


## What's new in Firebase

URL: [https://www.youtube.com/watch?v=emIxn-f9bK0](https://www.youtube.com/watch?v=emIxn-f9bK0)

- New Firebase features introduced at Google I/O 2023
    - Second generation Cloud Function with greater request timeouts and support for even longer running tasks.
    - Python support for second generation Cloud Functions, allowing developers to write functions using a popular programming language.
    - Fivestar trigger support for second generation Cloud Functions, providing an additional powerful addition for triggering functions based on Firestore collections.
    - App Check SDKs available in C++ and Unity, enhancing security by verifying callbacks coming from legitimate apps untampered devices.
    - Firebase Authentication Identity Platform upgraded, improving security measures for app authentication.
    - Support for dynamic previews in Firebase Hosting, allowing developers to share preview versions of their sites with a specific expiration time.
    - Flutter Web support added to Firebase Hosting, making it easier for developers to deploy Flutter web apps using the same workflow as other platforms.
    - Real-time updates to Remote Config, enabling developers to make changes and see them instantly across all users without requiring app updates.


## What's new in Generative AI

URL: [https://www.youtube.com/watch?v=628ANvH1jH0](https://www.youtube.com/watch?v=628ANvH1jH0)

- Introduction to Generative AI and Google's Palm API
    - Overview of Generative AI and its applications
    - Introduction to Google's Palm API for developers
- Building Applications with Large Language Models (LLMs) using Palm API
    - Explaining LLM concepts and their use in various language tasks
    - Demonstrating how to build an application powered by a generative AI model
        + Creating a new text prompt interface
        + Using the maker Suite data prompt feature for synthetic data generation
- Prototyping Chatbots using Palm API
    - Understanding the concept of chatbots and their use cases
    - Building a chatbot prototype with conversation history using Palm's chat service
- Demonstrating the use of the Palm API in various applications
    - Showcasing different types of APIs (text endpoint, chat endpoint, etc.) and how they can be used to build diverse applications
        + Text endpoint example: Reverse dictionary application
        + Chat endpoint example: Interactive alien life conversation
- Embeddings and Search Applications using Palm API
    - Understanding the concept of embeddings in text data
    - Demonstrating how to use embeddings for search applications
- Collab Magics and Large Language Models
    - Introduction to Collab Magics, a tool that allows easy access to large language models
    - Showcasing how to use Collab Magics with Palm API in Google Colab
- Responsible Development with Large Language Models
    - Discussing the importance of responsible development when using generative AI models
    - Sharing best practices for building applications that leverage LLMs while maintaining user safety and privacy


## What's new in Google AR

URL: [https://www.youtube.com/watch?v=7dkfi4s3mvU](https://www.youtube.com/watch?v=7dkfi4s3mvU)

- Vision Augmented Reality: AR technology seamlessly blends physical and digital worlds, enabling developers to place user interact with digital content in their physical surroundings.
- New Creator Tool: Directly integrated into Unity and Adobe Aero, the new geospatial creator tool makes it easier for creators to build immersive experiences that blend virtual objects with real-world locations.
- ARCore Geospatial API: Enables developers to create world-anchored AR experiences using Google Maps platform. The new feature allows users to place digital content at specific geographic coordinates.
- Streetscape Geometry API: Provides developers with 3D models of the real world, including building geometry and terrain data. This helps in accurately placing virtual objects within the real world environment.
- Terrain Anchor: A type of anchor that allows placement of digital content on the topography of the Earth's surface. It uses latitude/longitude coordinates to create an exact pose for the anchor.
- Rooftop Anchor: A new type of anchor that enables placement of digital content on the roofs of buildings, using advanced 3D data available from streetscape geometry.
- Geospatial Depth API: Combines real-time depth measurements from mobile devices with streetscape geometry data to generate a depth map, which can be used for building more realistic geospatial experiences.
- Scene Semantics API: Uses AI to provide accurate labeling features for outdoor scenes, saving developers significant time and effort when creating AR experiences.


## What's new in Google Cloud

URL: [https://www.youtube.com/watch?v=LDbD3ioDkYA](https://www.youtube.com/watch?v=LDbD3ioDkYA)

- Google Cloud introduces new generative AI powered interface
    - The new interface transforms cloud development experience using AI foundation model
    - It surfaces contextual code completion and provides prescriptive chat-based assistance wherever Console, CLI, or APIs are used
    - This makes it easier for developers to build, deploy, and manage software applications across the globe

- Google Cloud also introduces new generative AI product solutions
    - These help easily build gen apps, create new experiences like bot chat interfaces, custom search engines, digital assistants, etc.
    - They are designed to work seamlessly with other Google Cloud services and products

- Google Cloud has optimized its infrastructure for AI workloads
    - This includes improvements in hardware, software, and network infrastructure to support demanding AI applications
    - It also provides a range of managed services, APIs, and SDKs that developers can use to build, deploy, and manage their AI applications more easily

- Google Workspace introduces new opportunities for developers using AI
    - The platform now supports the integration of generative AI capabilities into its apps like Gmail, Docs, Sheets, Slides, and Meet
    - This enables developers to create more intelligent and productive workflows across these apps, helping users get things done faster and more efficiently

- Google Workspace also offers new opportunities for developers to build workflow apps using chat APIs
    - These APIs allow developers to integrate contextual workflow data directly into chat conversations, making it easier for users to take action and respond to urgent events in real time

- Google Meet now supports the integration of third-party smart chips into its apps like Docs and Sheets
    - This enables users to access external data sources like project thumbnails, customer records, and dashboard widgets directly within their documents and spreadsheets, without having to switch between different applications or tabs
```


## What's new in Google Home

URL: [https://www.youtube.com/watch?v=CXaxMXZwaaw](https://www.youtube.com/watch?v=CXaxMXZwaaw)

## Summary of Google IO 2022 Smart Home Announcement
- Matter, the latest smart home standard, accelerates industry growth and brings ease of use, openness, and choice.
- New tools for Android developers enhance app smart home control, reducing costs and enabling focus on innovation.
- The introduction of Matter and Thread standards simplifies setup processes and allows a single hub to manage multiple devices.
- Google Home's ecosystem enables developers to create personalized experiences, with over 500 million connected devices and an active user base.
- Developing for Google Home means reaching a large audience through the Android platform, which supports Matter-enabled devices.
- The redesigned Google Home app allows users to easily discover and adopt smart home automations powered by Google's leading language model.
- New APIs simplify building Matter-enabled devices, giving users more control over their devices without complexity or cost.
- The launch of the limited developer early access program for Matter device APIs will help developers test and refine their products before full release.


## What's new in Google Pay and Wallet

URL: [https://www.youtube.com/watch?v=xJxzdqcGJQk](https://www.youtube.com/watch?v=xJxzdqcGJQk)

- Robert Dunnette, Director of Product Management at Google Pay, and Edson Yanaga, Lead Developer Relations at Google Wallet, presented a keynote on new features and updates for both products.
    - They discussed the goal of providing simple, safe, and seamless payment experiences for everyone, everywhere.
- Broadening Benefit Across Devices: Rolling out merchant liability protection for eligible online transactions using Google Pay.
- Secure Payment Authentication: A new service to help with risk compliance-based authentication needs. It uses devicebound tokens to meet two-factor authentication and boost authorization rates.
    - This feature is available for payment transactions requiring additional verification.
- Three New Online API Features for Google Pay:
    1. The introduction of the Google Pay button, which has been improved with dynamic previews and card network last four digits.
    2. A new customizable button view for Android, which includes a fresh, new look using Material 3 design principles.
    3. New customization capabilities for the Google Pay button, such as adjusting shape and corner roundness to match UI designs.
- Virtual Card Check Guide: A guide to help developers optimize their Android applications and websites for autofill, which allows customers to reuse existing payment information saved in their Google accounts. This can save time and effort during checkout, resulting in increased conversion rates.
    - Since its launch, the virtual card feature has been adopted by millions of users from American Express and Capital One.
- New Developer Features for Autofill: Two new developer features have been introduced to enhance the autofill experience:
    1. A new recommendation system provided by Chrome DevTools that helps developers improve their checkout forms' performance.
    2. Support for filling out forms across iframes, which can help facilitate payment while respecting application security decisions and architecture.
- New Way to Display Fill Form on Chrome Mobile: A new feature will be introduced later in the second half of 2023 that presents a convenient, recognizable dialog when users land on a form information page. This helps them navigate and fill out necessary information more quickly, speeding up checkout processes.
- Bank Account Number Autofill for IBAN: A new feature will be added to allow autofill of bank account numbers using the International Bank Account Number (IBAN) format, which is commonly used in Europe for direct debit and peer-to-peer transactions.
- New Developer Tool for Google Wallet: A new developer tool called "Google Wallet API Demo Mode" has been introduced to make building Google Wallet even easier. It provides a hands-on environment where developers can sign up for an API access key, integrate the code immediately, and test their applications without any restrictions or limitations.
    - This feature is especially useful for those who want to experiment with different use cases and configurations before going live with their products.
- New Features for Google Wallet: 
    1. The introduction of a new integration for RCS (Rich Communication Services) that allows users to check in for flights using Android Messages.
    2. A new Generic Private Pass API that enables support for passes containing sensitive data such as health insurance cards and ID cards. This feature is available only to select groups and countries with approved use cases.
- Privacy and Security Cornerstone: Google Wallet maintains strict privacy and security standards, ensuring that users' sensitive digital items are kept safe at all times. One important property of private passes is that they require user verification through fingerprint sensor or passcode authentication methods.
    - The Google Wallet developer documentation contains detailed steps to help developers add private passes to their applications.
- New API Rotate Barcodes: This new feature allows pre-created batch barcodes to be synced with Google Wallets, enabling a range of use cases where protect passes are needed for long-duration transit tickets or event tickets. Sometimes, these passes need to be associated with specific individuals, like sporting events or boarding passes for concerts.
    - This feature helps prevent pass theft, reselling, and transfer restrictions.
- New Feature: Identity Linked Pass: This new feature allows issuers to associate passes with Google Accounts, ensuring that only authorized users can hold certain passes and gain access to restricted areas.
    - To use this feature, simply include the user's email address when creating a pass object.
- New Pas Builder Tool: A new pas builder tool has been introduced to help developers configure styles for their passes using real-time previews. This helps them understand how their passes will look styled and connected with visual elements in the API.
    - The new pas builder also generates class objects and JSON format code snippets that can be used directly to make API calls, making it easier to configure passes without having to write any text-based configuration files.


## What's new in Google Play

URL: [https://www.youtube.com/watch?v=XP7saG2QqJA](https://www.youtube.com/watch?v=XP7saG2QqJA)

- Introduction: Deepthi Menon and Tom Grinsted discuss Google Play's focus on supporting app developers throughout the entire lifecycle of their business.
- User Acquisition: They highlight updates to store listing customization, AI-assisted content generation, and machine translation tools to help developers reach a wider audience and improve user engagement.
- Event Promotion: The team shares success stories from high-quality apps leveraging Google Play's promotional opportunities during special events, driving significant increases in user acquisition and retention.
- App Quality: They emphasize the importance of maintaining quality standards across all aspects of app development, including technical quality and adherence to Google Play's guidelines.
- Monetization Strategies: The presenters discuss new features such as featured products for selling in-app items directly through Google Play, price experimentation tools, and subscription offer improvements designed to help developers generate revenue more effectively.
- Marketing Efficiency: They talk about enhancing the effectiveness of marketing-to-sales funnels by improving deep linking capabilities and providing better insights into user behavior.
- Security & Safety: The team shares updates on Google Play's efforts to protect users and businesses from abuse, cheating, unauthorized access, and piracy, including improvements to the Play Integrity API and automatic integrity protection features.
- User Privacy & Data Protection: They discuss new data deletion features designed to give users more control over their personal information and help developers comply with privacy regulations.
- Continuous Improvement: Finally, they preview upcoming changes to the Google Play Console interface, including a redesigned app content page, customizable homepage metrics, and enhanced notification functionality. They also invite feedback on these updates through an open beta program for Android users.


## What's new in Machine Learning for Google Developers

URL: [https://www.youtube.com/watch?v=DaUtUYIn2D0](https://www.youtube.com/watch?v=DaUtUYIn2D0)

- Intro by Meenu Gaba, Engineering Lead at Google
    - Overview of the latest updates in Google's machine learning ecosystem
    - Highlighted AlphaFold, a deep learning model that predicts protein 3D structures
    - Emphasized AI becoming more commonplace in everyday life
- Kaggle Community Update
    - 13 million users and over 200,000 public datasets available
    - Data augmentation feature to expand data sets and improve model performance
    - New APIs for computer vision tasks like image classification and object detection
- TensorFlow Ecosystem Updates
    - Keras, a high-level API for building machine learning models, now supports more types of models and simplifies workflows
    - KerasNLP, a library for natural language processing tasks, makes it easy to build transformer-based models like BERT and GPT
    - TensorFlow 2.x Quantization API helps scale models by parallelizing data and model splitting across multiple machines
- JAX Update
    - A research framework used to train large-scale models, now has better integration with TensorFlow ecosystem for deploying models in production
- Kaggle Models Search
    - A platform that allows developers to find, use, and modify pre-trained machine learning models
- Google's Machine Learning Deployment Options
    - Browser: TensorFlow.js enables model training and deployment directly within the browser
    - Android: TensorFlow Lite simplifies the process of shipping ML models on mobile devices by leveraging existing Google Play Services infrastructure
    - WebGPU support in Chrome allows for faster, more efficient machine learning operations without needing a backend server
- MediaPipe Update
    - A low-code solution for incorporating machine learning into on-device applications
    - Extensive collection of cross-platform APIs encapsulate common ML tasks like object detection, text search, and audio classification
    - Task-based approach allows developers to plug in pre-trained models or build their own custom models for specific tasks
- TensorFlow Extended (TFX) Update
    - An open-source framework for building end-to-end machine learning pipelines that are scalable, reliable, and explainable
    - Supports data governance through ML metadata libraries which track data lineage and help debug issues in the pipeline
    - Many community-built add-ons available to customize TFX components and models
- OpenXLA Update
    - An accelerated infrastructure for training and deploying machine learning models across various hardware platforms like TPUs and GPUs
    - Allows developers to migrate workloads seamlessly between different hardware types


## What’s New in Material Design

URL: [https://www.youtube.com/watch?v=vnDhq8W98O4](https://www.youtube.com/watch?v=vnDhq8W98O4)

- Material 3 APIs now stable and production ready
- New carousel component for expressive, unique design
- Motion system updates for more natural, spirited movement
- Dynamic color update with color fidelity and content-based dynamic color
- High contrast themes for better accessibility and personalization
- Monochrome variant in Material Color System
- Figma-integrated M3 Design Kit for streamlined design workflow


## What's new in Web

URL: [https://www.youtube.com/watch?v=x9rh0Du4Czg](https://www.youtube.com/watch?v=x9rh0Du4Czg)

- Overview of web platform features and their use
- Mention of new HTML element: Dialogue element
- CSS transform properties for better performance
- New viewport units for mobile website design (vh, vw)
- Deep copy JavaScript objects using Structured Clone function
- Focus Visible pseudo class for accessibility improvements in web design
- Transform Stream API for streaming data between two locations
- Importing external modules in JavaScript using ES6 import statement

The speaker discussed various features of the web platform, including new HTML elements, CSS transform properties, viewport units for mobile websites, deep copying JavaScript objects, accessibility improvements with focus visible pseudo class, streaming data with Transform Stream API and importing external modules using ES6 import statement. The presentation aimed to provide a glimpse into the evolving landscape of web development, highlighting how these features can be used to build modern, efficient, and accessible web applications.


## Building more skin tone inclusive computer vision models

URL: [https://www.youtube.com/watch?v=vuv_r3iGM14](https://www.youtube.com/watch?v=vuv_r3iGM14)

    - Auriel Wright, Skin tone plays a significant role in how users are treated and interact with technology.
      - Numerous examples exist where industries have failed to create products that work well for various skin tones.
    - Google's Skin Tone Team aims to help build products that function effectively for everyone, regardless of their skin tone.
      - The team consists of UXRs, a colorism expert, research scientists, and engineers who leverage their expertise to minimize skin-tone inequity in ML products.
    - Google uses ML Fairness Evaluation to understand common human biases manifested in ML algorithms ultimately affecting the product.
      - Over the past year, they've been working with Dr. Monk to validate the MST scale and apply it in their work.
    - Teams within Google use the MST scale for ML labeling tasks, ethnographic research, fairness testing, and as a guiding principle for computer vision fairness workflow.
      - They've broken down four key principles to practice inclusive product development: Testing, Annotating Data Responsibly, Including Skin Tone in Image Processing, and Following Google's AI Principles.
    - When developing an inclusive product, it is essential to ensure that the training and evaluation data sets include representation across the entire MST scale.
      - This means accounting for a diverse range of people and tone categories within the dataset.
    - Inclusive evaluations involve testing algorithms and products with various skin tones to ensure they perform similarly.
      - In practice, this looks like using metrics that measure performance consistently across different skin-tone groups.
    - Intersectional slicing allows for a more nuanced understanding of how different attributes interact within the product development process.
      - This involves testing whether models perform well in specific subgroups based on multiple factors such as gender, age, and perceived expression.
    - To annotate data responsibly, practitioners should use the MST scale to capture nuances in skin tone accurately.
      - They should avoid grouping points into categories like light, medium, or dark, which can hide important nuances.
      - It's also helpful to enlist geographically diverse sets of annotators within one region and get at least two raters per region to reduce variation and achieve a global consensus.
    - Lighting plays an essential role in rating data and annotating skin tone, as it impacts perception significantly.
      - Practitioners should consider this factor when working with data sets and ensure they account for variations in lighting conditions.
    - Finally, practitioners should avoid equating race with skin tone when developing ML fairness efforts.
      - It's crucial to remember that skin tone and race are two distinct concepts, and models must account for differences within ethnic groups.


## Optimize your app performance using the new deep linking management tools in Play Developer Console

URL: [https://www.youtube.com/watch?v=GeyvIbBS7s8](https://www.youtube.com/watch?v=GeyvIbBS7s8)

## Deep Links in Apps: Benefits and Implementation

- **What are deep links?** Deep links are URLs that open an app and send the user to a specific piece of content or action within the app. They can improve user experience, drive traffic to apps, create personalized experiences, and boost long-term engagement.

- **Why implement deep links in apps?** Implementing deep links can:
  1. Provide smoother user experience compared to mobile websites.
  2. Drive targeted traffic to the app through various channels like marketing emails or SMS.
  3. Create personalized experiences by including URL parameters.
  4. Help users accomplish goals within the app with minimal effort.
  5. Boost long-term engagement and return on investment.

- **How to implement deep links?** Implementing deep links involves two main steps:
  1. Creating an intent filter in the app's manifest file that defines URLs for opening the app and launching the right content or activity.
  2. Uploading a JSON file containing the domain, subdomains, and verification status to Google Play Console for website validation.

- **New Tools for Deep Link Management**
  - **Google Ads iOS Validator**: A new tool launched by Google Ads that performs a series of checks on an iOS app's deep link configuration based on the link entered and the app's Apple App Site Association file. It provides recommendations to fix misconfigured links.
  
  - **New Deep Links Page in Google Play Console**: A new page dedicated to deep link setup per app version, which tracks active versions and highlights potential issues requiring fixes.

- **Examples of Successful Deep Link Implementation**
  - Fashion retailer Boozt saw a 45x higher conversion rate, 2x higher average order value, and 33% higher return on ad spend after implementing deep links to create a seamless web-to-app experience.
  - Fashion retailer Boohoo achieved a 25% increase in revenue, a 55x higher conversion rate compared to the web, and an 8% increase in net conversion volume by using Web App Connect and implementing deep links within their mobile app.


## Developers guide to BigQuery export for Google Analytics 4

URL: [https://www.youtube.com/watch?v=c0mqBuXPrpA](https://www.youtube.com/watch?v=c0mqBuXPrpA)

- Three individuals from Equicorp discussed their experience using Google Analytics 4 (GA4) and BigQuery Export.
- Kishore, a marketing analyst, explored GA4 implementation for value extraction through Google Analytics data.
- Musa, a data engineer, built a robust granular marketing reporting solution using GA4 data.
- Robin, an advertising expert, focused on implementing advanced use cases like creating custom audiences with GA4's first-party CRM data.
- Key features of GA4 and BigQuery Export were highlighted:
    - BigQuery Export feature allows users to export event-level data from a web app property or a Firebase app.
    - Users can explore demo datasets, understand schema structures, and run basic queries before starting their own projects.
    - The BigQuery Sandbox provides free trial access to the tool for 60 days with limited storage capacity.
    - GA4 properties can be linked directly to Google Ad accounts for conversion audience targeting purposes.
- Cost implications of using BigQuery Export were discussed:
    - Google Analytics users get 10 GB of free data storage every month, and additional gigabytes are charged at $0.02 per gigabyte.
    - Users can enable auto-scaling for streaming export from GA4 properties, but this feature comes with a cost per 200 MB point.
- Proposal creation tips were shared:
    - Kishore recommended setting up a Google Cloud Platform (GCP) project and enabling billing accounts to get started quickly.
    - Musa emphasized the importance of implementing custom cost controls at both project and user levels to manage costs effectively.
- The benefits of using BigQuery Export were highlighted:
    - It allows users to analyze large datasets without programming skills or SQL knowledge.
    - It provides a flexible platform for building custom dashboards, models, and audiences based on GA4 data.
- Robin shared his experience working with internal data science teams to build advanced use cases using GA4 data:
    - He built a traffic attribution model using BigQuery Export data combined with first-party CRM data.
    - He also created custom audiences using predictive metrics like lifetime value, propensity to purchase, and churn rates.
- Future enhancements to the GA4 and BigQuery Export features were discussed:
    - New event-level traffic source data will be available for collection and export in Q2 2023.
    - A separate table will be created daily to export user data that has changed within the last day, along with summary statistics.



## How to build great Android apps for large screens and foldables

URL: [https://www.youtube.com/watch?v=5JQjk3ZqPWc](https://www.youtube.com/watch?v=5JQjk3ZqPWc)

- Android Developer Team focused on large screen tablet and foldable app experience
- 280 million active large-screen foldable devices running Android
- Practical overview to improve Android apps for large screens
- Large screens include tablets, foldables, Chromebooks, desktop running Android
- Improving app device can lead to significant improvements in core business metrics
- Developer guide available to help with improvements
- 50 apps have been updated since the introduction of new features
- Key areas for improvement include layout, UX continuity, multitasking, and input support
- Large screen devices are often used in different orientations, with multiwindow mode, and freeform windowing
- Users expect large-screen apps to work seamlessly across these configurations
- Apps should be designed to take advantage of the larger real estate available on large-screen devices
- Novel form factors like foldables can provide unique opportunities for app experiences
- Android 12L introduced updates that help improve large-screen and foldable device support
- UI affordances allow users to customize app experiences
- New layout gallery provides inspiration for building great large-screen apps
- Key areas of focus when designing for large screens include fixed orientation, adaptive responsiveness, and differentiating the app experience
- Large-screen layouts should be designed considering window size classes and breakpoints
- Apps should support continuity across device configurations to prevent loss of state
- Camera sensor orientation can impact app performance on foldables and other large-screen devices
- New features like drag-and-drop multiwindow mode can help make apps more optimized for large screens
- Input support is key for large screen productivity, with support for keyboard, mouse, trackpad, and stylus
- Stylus support should consider motion events, position, orientation, tilt, pressure, and palm rejection
- Android Studio's reference devices can help developers ensure their apps work well across a variety of device configurations
- Testing automation is also important for large screen app development


## Optimize activity-based apps for large screens

URL: [https://www.youtube.com/watch?v=zx20-3GSdHw](https://www.youtube.com/watch?v=zx20-3GSdHw)

- Introduction by Ran Nachmany
- Activity embedding enhances app experience on large screens
- Two container activity stacking: primary and secondary
- Secondary always considered top
- Consistent ordering of activities regardless of device (phone, tablet, foldable)
- XML configuration file for defining split rules
- WindowManager library dependency
- Adding "activity embedding split enabled" property in application element
- Creating a rule controller component
- Parsing the XML configuration file to make the rule available system
- Customizing behavior: launching nested split and horizontal split
- Detecting device's posture (tabletop mode, folding feature)
- Programmatically creating and registering split rules for tabletop mode
- Cross-application activity embedding allows tight visual integration
- Opt features require specifying SHA1 certificate for host application
- Conclusion by Jon Eckenrode: Large screens are growing in popularity; optimize apps for large screens today.



## How to test for all screen sizes

URL: [https://www.youtube.com/watch?v=YHeWQ9MNuWg](https://www.youtube.com/watch?v=YHeWQ9MNuWg)

- The talk is about testing Android apps across different screen sizes.
- Different types of devices have different configurations and screen shapes.
- It's important to test various device types, orientations, and sizes to ensure compatibility.
- Testing should cover app crashes, UI state saving, and handling orientation changes.
- The speaker recommends using emulators and physical devices for testing.
- Android Studio has a feature called "Device Mirroring" which allows developers to interact with their physical device directly from the IDE.
- Jetpack Compose has a new multipreview tool that lets developers see how their UI looks on different screen sizes simultaneously.
- Automated testing is important, and tools like Gradle Managed Devices can help run tests on various devices.
- The WindowTesting library allows developers to test features related to foldable devices.
- State Restoration Tester API in Jetpack Compose helps test how composables retain their state during configuration changes.
- TestHarness is a tool that lets developers define different scenarios for testing, such as changing the size of composables or testing UI navigation on large screens.


## How to build high quality experiences on Wear OS

URL: [https://www.youtube.com/watch?v=82HB6ziP1QY](https://www.youtube.com/watch?v=82HB6ziP1QY)

- Introducing new features in Wear OS 3
    - Enhanced notification system with local and bridged notifications
        - Local notifications: displayed exclusively on the device, created using notification APIs
        - Bridged notifications: originates from a mobile app, delivered via Bluetooth or internet connection
    - New watch face format: declarative XML format, built using Jetpack watch face library, allows for easier maintenance and fewer updates
        - Watch Face Studio: innovative design tool helps designers create watch faces without writing code, includes an editor part and supports localization, TalkBack, and health data querying
- Enhancing Google Assistant integration on Wear OS
    - Improved plugin for Android Studio, allows developers to test shortcuts conveniently and preview app updates locally
    - New tile partner integrations: Spotify, Peloton, WhatsApp
        - Tiles: live one swipe away watch face, provide quick access to helpful information and actions
- Upcoming features in Wear OS
    - Platform data binding support for tiles, allowing real-time updates per second from data sources like heart rate or step count
    - Animation support for tiles, enabling smooth transitions between layout changes


## What's new in Health on Android

URL: [https://www.youtube.com/watch?v=hlyC0I6v9ic](https://www.youtube.com/watch?v=hlyC0I6v9ic)

- Google's Android Health team announced updates to two key features: Health Services and Health Connect.
- Health Services, a set of APIs that allow developers to access sensor data from wearable devices such as heart rate, steps, and sleep, will continue to evolve with new features like improved batching rates for fitness metrics and support for reading and writing reproductive menstrual health data. 
- Health Connect, an on-device health data store that provides APIs for storing and sharing health and fitness data across Android apps, is now available in open beta on the Play Store. It will be a core part of Android starting with the Android 14 release and will be updated frequently via Google Play System Updates.
- Health Connect reduces fragmentation by providing a single place where users can control their data permissions and share health fitness data across favorite integrated apps. Developers benefit from a streamlined, unified API surface that supports 50 different data types with single permission management.
- The Android Health team has been working closely with partners like Fitbit and Samsung Health to ensure compatibility and seamless integration with their respective platforms.
- In the future, Health Connect will support new features such as exercise route tracking, which will allow users to share GPS maps of their workouts across different apps.
- Google is also continuing to improve its own health and fitness app, Google Fit, by expanding its API surface to include more data types and better support for reading and writing health data. The company plans to provide developer migration guides and documentation to help developers transition from the current Google Fit APIs to Health Connect APIs.
- The Android Health team emphasized that they are committed to working closely with the developer community to create a seamless, interconnected health experience for users across all Android devices. They encourage developers to adopt Health Connect and leverage its powerful features to build innovative health and fitness apps.


## Best practices for game development across platforms

URL: [https://www.youtube.com/watch?v=I5ubmoSgaXE](https://www.youtube.com/watch?v=I5ubmoSgaXE)

- Introduction by Daniel Levy
- Importance of large screen devices for game distribution and revenue growth
- Technical best practices for building games for large screen devices
    - Input handling:
        - Keyboard support
            - Handling configuration changes
            - Preserving input text when switching keyboard types
        - Mouse handling
            - Ensuring smooth cursor movement with different input methods
            - Toggling input emulation
    - Gamepad support
        - Using Android Game Development Kit library for robust game controller implementation
- Flexible input switching approach
- Windowing and resizing considerations
    - Defining minimum and maximum aspect ratios supported by the game
    - Handling rotation and resizing configuration events
    - Letterboxing or scaling content to fit new window sizes
- Large screen experience optimization tips
    - Declaring main activity as resizable
    - Explicitly handling screen layout configuration events
    - Testing the game's performance on larger screens
- X86_64 architecture support for better performance on Chrome OS and Google Play Games PC
- Best practices for building, releasing, and distributing games for large screen devices
    - Using a single release artifact for different form factors
    - Leveraging Android App Bundle and Play Asset Delivery features for efficient game updates and asset distribution


## What's new with TV and intro to Compose

URL: [https://www.youtube.com/watch?v=_X4tswgV67Y](https://www.youtube.com/watch?v=_X4tswgV67Y)

    - Shobana Radhakrishnan, Senior Director of Engineering at TV Google, discussed Android TV OS's growth and the introduction of new features for a more personalized experience.
    
    - The latest update to the home screen introduced four new content pages: US movie show, family, entertainment, and Español. These updates aim to help users find content they are interested in while also giving partners new ways to surface their content.
    
    - The company announced a major update to Google TV, which includes redesigned TV Guide and the addition of a free channel tab featuring live TV channels like Tubi, Plex Haystack News, and more. This feature will make it easier for users to find what they want to watch without needing a subscription.
    
    - Paul Lammertsma, Director of Developer Relations at Google, introduced Compose TV, a new tool that allows developers to build world-class TV apps efficiently using code. The new framework is designed to make it easier for developers to create engaging and visually appealing TV experiences by providing prebuilt components and an easy-to-use interface.
    
    - Sidd Gudipati, Staff Designer at Google, demonstrated how to use Compose TV to build a TV app using various prebuilt components like TabRow, Carousel, and ImmersiveList, as well as customizing the focus system and creating unique animations.
    
    - The presentation also provided an overview of the focus system in TV apps, which is crucial for visually indicating the active element and providing a clear starting point for navigation.
    
    - Finally, Lammertsma emphasized that Compose TV is built on Jetpack Compose, making it easy for developers to build TV apps using familiar tools and techniques. The team encourages developers to start adopting Compose Android TV app development today by checking out the release notes and submitting feedback or feature requests.


## What’s new with Android for Cars

URL: [https://www.youtube.com/watch?v=ii7PGDG0G5g](https://www.youtube.com/watch?v=ii7PGDG0G5g)

- Introduction: Jennifer Tsau, Product Management Lead, and David Dandeneau, Engineering Lead, discuss the vision of bringing a safe, seamless, and connected experience to cars.
- Android Auto Updates: The new Android Auto provides a split-screen design that adapts to various screen sizes and orientations for better functionality like smart suggestions, missed call reminders, quick ETA sharing, and instant access to music and podcasts.
- Digital Car Key: This feature allows users to lock, unlock, and even start their cars using their Android phones with ultrawideband support.
- Android Automotive OS: The open-source platform has seen strong adoption from car manufacturers, providing an embedded solution that integrates Google Assistant, Google Maps, and Google Play directly into the car system.
- New Apps Experience: The flexibility of the Android platform allows developers to create unique experiences for drivers and passengers, helping them navigate their destinations, stay connected with the world around them, and access various entertainment options.
- Weather and Smart Home Integration: Developers can build weather apps that provide real-time information on road conditions, while smart home integration lets users join scheduled meetings without fumbling with their phones.
- Multiscreen Support: Android Automotive OS will soon support multiscreen displays, enabling passengers to enjoy personalized apps on the rear seat screen while the driver focuses on driving.
- Video and Gaming Apps: The expansion of video and gaming apps in cars is being accelerated by partnerships with popular game developers like FRVR, Vector Unit, and Outfit7.
- Car App Library: This tool helps developers build safe, drivingsafe apps compatible with Android Auto and Android Automotive OS, making it easier for them to scale their implementations and maintain codebases across different platforms.
- Waze Updates: The revamped Waze app built using the Car App Library enables users to submit real-time reports about traffic jams, road closures, and potholes directly from their car's display. It also supports electric vehicle charging station integration, allowing drivers to find relevant plug types and access detailed information about operating hours.
- Future Opportunities: Developers can bring tablet apps directly into cars via Google Play, enabling them to distribute updates more efficiently and take advantage of large screens for better user experiences.


## What great quality looks like on Google Play

URL: [https://www.youtube.com/watch?v=QaTvA-rDny8](https://www.youtube.com/watch?v=QaTvA-rDny8)

- Intro: Lauren and Scott from Google Play introduce themselves as product managers.
- Goal of the talk: To inspire developers to build high quality experiences on the Google Play Store.
- High Quality Framework: Lauren introduces a framework that consists of four pillars - Core Value, User Experience, Technical Quality, and Privacy & Security. Scott then goes on to illustrate these pillars with examples.
    + Core Value: The first pillar emphasizes fulfilling user needs. Canva is used as an example of an app that does this well by providing thousands of templates for users to create content.
    + User Experience: This pillar focuses on how easy and intuitive the app is to use. Spotify's multi-window support and Peloton's seamless integration with wearables are cited as good examples.
    + Technical Quality: The third pillar highlights the technical aspects of an app, including its performance across different devices. Left Survive's control translation for different input methods is given as a good example.
    + Privacy & Security: The final pillar deals with ensuring user data is safe and secure. Google's new Android version features enhancements to help apps design safer experiences.
- Play Presence: Scott highlights that having a strong presence on the Google Play Store can increase discoverability and promotion of an app. Asset showcasing, promotional content, and store listing accuracy are all important factors in this regard.
    + Asset Showcase: Using Spotify as an example, he emphasizes the importance of uploading high-quality images and videos that accurately represent the app's features and benefits.
    + Promotional Content: He also discusses how promotional content can help tell a compelling story about an app or game, such as MyFitnessPal's announcement of their new app widget feature.
    + Store Listing Accuracy: Lastly, he stresses the importance of having accurate and clear descriptions in the store listing to avoid disappointing users upon installation.
- Closing: Lauren wraps up by thanking Scott for his insights and encourages developers to check out Google's resources on building high-quality experiences on the Play Store.


## Boost your revenue with Play Commerce

URL: [https://www.youtube.com/watch?v=JILMOPUIBiQ](https://www.youtube.com/watch?v=JILMOPUIBiQ)

- Google Play Commerce Platform:
  - Evolves to meet user needs
  - Supports 300+ local payment methods across 65 markets
  - Prevents 2 billion fraudulent transactions yearly
  - Explores expanded billing options like user choice billing pilot
  
- Pricing Optimization:
  - New tool to test local pricing for in-app products and subscriptions
  - Flexible subscription plans with multiple price points per billing period
  - Increasing price for existing subscribers with advanced notification
  
- Promotional Strategies:
  - Featured Product: A new sale channel enabling direct in-app product sales on the Play Store
  - Google Play Points: One of the largest reward programs globally, now available in India and Mexico
  - In-App Offers: Allows users to apply available Google Play promotions without leaving the app
  
- Additional Tools & Features:
  - Upgraded Play Billing Library 60 coming soon
  - Support for major releases of Billing Library until November 2023
  - Expansion into new markets like India and Japan for Play Pass


## What's new in Android privacy and security

URL: [https://www.youtube.com/watch?v=MsNraIQ6mUI](https://www.youtube.com/watch?v=MsNraIQ6mUI)

- Introduction by Ronnie Falcon and Terrence Todd
- Focus on privacy security in Android app development
- Privacy changes and approaches to building privacy-centric apps
- Optimizing security and performance with special permissions
- Latest security changes and actions to prevent common pitfalls
- Android team's commitment to user privacy and transparency
  - Transparency features introduced in Android 12 and 13
  - Data safety section on Google Play for app developers
  - New data deletion feature in Android 14
- Importance of responsible data collection and usage
- Developers' role in the Android privacy story
- Discussion of storage improvements and best practices
  - Granular permission access in Android 13
  - User selected permissions in Android 14
  - Privacy preserving API detecting user screenshots
- Adoption process of new features and improvements
- Latest special app access permissions and security enhancements in Android 14
  - Fullscreen intent (FSI) permission and notification flags
  - Target SDK version requirement for app distribution on Google Play
  - Safe browsing feature to protect users from phishing attacks
  - Security changes affecting unintended interactions between apps and devices
    - Broadcast receiver exported flag in Android 13
    - Implicit intent availability in Android 14
- Importance of SDK dependency updates for developers
- Introduction of the Play Integrity API for app integrity verification
  - Standard request for faster response times
  - Support for Kotlin, Java, and Unity programming languages


## Creating high-quality Android media experiences

URL: [https://www.youtube.com/watch?v=sv9ICtooWBc](https://www.youtube.com/watch?v=sv9ICtooWBc)

- Introducing new premium medium experience:
  - Spatial Audio: A feature launched in Android 13 that creates immersive listening experiences by placing sound around a virtual space. It's designed to feel like you're part of the action.
  - Lossless audio: Android 14 is adding better support for playback of lossless high-resolution audio formats, such as MQA and DSD, through wired USB headsets. This ensures pristine quality audio without compromising on bit depth or introducing distortion.
  - HDR technology: Provides realistic and delightful viewing experiences with vibrant colors and greater range of brightness. Android 13 has standardized support for HDR video, while Android 14 is bringing HDR support to images.
- Latest Jetpack Media3 release:
  - A comprehensive set of APIs designed to simplify the development of medium experiences on Android devices. It abstracts away much of the complexity associated with device and OS fragmentation, making it easier for developers to create reliable, performant applications that take full advantage of modern hardware capabilities.
- Transformer API: Part of the Media3 suite of tools, this API allows developers to easily transcode one medium format into another using optimal parameters, handle special formats like slow-motion video and HDR video, apply tone mapping between SDR and HDR assets, and even edit videos by adding custom effects.
- Future enhancements:
  - The team is working on expanding the capabilities of Transformer to support multi-asset editing, allowing developers to create more complex and engaging media experiences for their users.


## Building high quality Android camera experiences

URL: [https://www.youtube.com/watch?v=rNe2xGKjtvc](https://www.youtube.com/watch?v=rNe2xGKjtvc)

    - Importance of Camera Experience
        * 34 million people use Android camera
        * High quality experience is key for user satisfaction and purchase
    
    - Polished Camera Preview
        * Maintain aspect ratio across different devices and orientations
        * Jetpack preview classes: `PreviewView` (CameraX) and `CameraViewfinder` (Camera2)
        * Configure preview display properly on every screen size
        * Use fit scale type to ensure proper display of preview
    
    - Premium Visual Quality
        * HDR Video Capture introduced in Android 13
        * 10-bit color capture for higher contrast and color fidelity
        * Check device support for 10-bit output using `CameraManager`
        * Use key REQUESTAVAILABLECAPABILITIES DYNAMIC RANGE TEN BIT to check if camera supports HDR video
    
    - Stream Use Cases
        * Optimize performance and usability based on stream use case (e.g., preview, still capture, video record)
        * Query supported stream use cases using `CameraManager`
        * Configure output configuration for selected stream use case
    
    - Camera Extensions
        * Device manufacturers expose special capabilities like night mode and bokeh effect as extensions
        * Check if camera supports specific extension based on camera ID
        * Implement `CameraExtensionSessionStateCallback` to handle session states
    
    - What's Coming in Android 14
        * Ultra HDR still capture with 10-bit color and P3 color space support
        * Zoom optimization for faster, crisper zoom experience


## How to reduce reliance on passwords in Android apps with passkey support

URL: [https://www.youtube.com/watch?v=36peNZUlgzU](https://www.youtube.com/watch?v=36peNZUlgzU)

    - Introduction: Niharika and Diego introduce the topic of reducing reliance on passwords by adding passkey support to Android apps.
    - Passkeys: New form of passwordless authentication, based on public cryptography, simpler and safer than phishable authentication methods.
    - How it works: When creating a passkey, user's device generates private-public key pair. Public key sent to server; user keeps the private key. User then uses their device's screen lock (like fingerprint or face unlock) to sign in. Passkeys are synchronized across user's Android devices and can be used wherever needed.
    - Benefits: Simpler, more secure authentication process with multifactor authentication, single tap for quicker sign-in, and less chance of phishing attacks.
    - Credential Manager API: New unified API that consolidates support for passkeys, traditional authentication methods, and federated sign-in. It brings together multiple sign-in methods into a single interface, making the login experience simpler and more user-friendly.
    - Password Managers: Several leading password managers like 1Password, Dashlane, Keeper, and Okta are available on Android 14 launch, with many expected to be available shortly after.
    - Developer Benefits: Credential Manager supports multiple sign-in mechanisms within a single API, making it easier for developers to integrate different authentication methods and maintain ongoing maintenance. It also helps in transitioning towards a passwordless future.
    - Real-world example: Kayak implemented passkey support using Credential Manager Android, making account creation lightning fast and removing the need for traditional password creation processes.
    - Integration process: To register a passkey, users must first sign into an app using traditional authentication methods (like passwords or federated identity). Once signed in, they are prompted to create a passkey by authenticating biometrics. The generated passkey is then saved in the user's preferred password manager and public key credential information returned to the app.
    - Sign-in process: Similar to registration, users must first sign into an app using traditional authentication methods. Once signed in, they are prompted to authenticate their passkey (including signature) using biometrics or another secure method.
    - Future plans: Firebase Authentication and Google Cloud Identity Platform plan to bring passkey support for developers soon, providing them with a modern, secure out-of-the-box authentication solution for apps built on major mobile web platforms.


## Building for the future of Android

URL: [https://www.youtube.com/watch?v=WMMPXayjP8g](https://www.youtube.com/watch?v=WMMPXayjP8g)

- Future of Android Development:
    - Seamless interoperability
        + Apps work great together across device leveraging Android's multitasking capability and resource management.
    - Privacy and Security
        + Built on the foundation of app sandboxing, open-source code, and an open app development platform.
        + Continuing advancements in state-of-the-art technology.
    - Personalization 
        + Android experience best adapts to user needs and wants.
    - Device Hardware Evolution
        + User expectations for apps take advantage of device hardware improvements.

- Android 14 Release:
    - Use purpose-built APIs
        + Share resource, move data much.
    - WorkManager API
        + Helps manage background tasks efficiently.
    - Foreground Service Type
        + New Android 14 Service permission type and new Apps target Android 14 need to declare foreground service permission based on foreground service type.
    - Cached State
        + Occurs when an app is no longer active and can be moved within seconds without getting CPU time.
    - Broadcast Dynamically Registered
        + Queued and/or merged delivery.
    - New Jetpack Library Support
        + Simplifies telecom stack API, service brings backwards compatibility, supports call-related background work.
    - Exact Alarm
        + Precise time, exact alarm.
    - Predictive Back Migration
        + Optical activity dependency to latest stable version.
    - Opt-in Animation
        + New Android 14 Material components.
    - Privacy and Security
        + Focus on improving privacy and security features in Android apps.
        + Google Play requires new apps and app updates target recent SDK versions.
        + Data auditing APIs provide transparency for private data access.

- Widget Development: 
    - Jetpack Glance Beta
        + Simplifies widget development using Composes declarative syntax.
    - Themed App Icon
        + Add third monochrome layer to point drawable in adaptive icon XML.

- Language and Locale Support:
    - Per-app Preference
        + Central place for multilingual user set preferred app language device setting.
    - Grammatical Inflection API
        + Create personalized UI for users who speak gendered languages.

- Accessibility Improvements: 
    - Nonlinear Font Scaling
        + Scale larger text slowly, avoid static SP calculation.
    - App Actions
        + Enable voice command and deep linking.

- Hardware Evolution: 
    - Larger Screens and Input Devices
        + Support for stylus, touchpad, mouse, keyboard, barcode scanning, translation, smart reply, etc.

- Machine Learning: 
    - ML Kit
        + Turnkey feature support in Android apps.

- Quality Framework: 
    - Pillars of Google Play's new Quality Framework
        + Performance, Stability, and Compatibility.


## 10 things to know about the Privacy Sandbox on Android

URL: [https://www.youtube.com/watch?v=rOtYqxaSnco](https://www.youtube.com/watch?v=rOtYqxaSnco)

    - Intro by Rob Clifford:
        - Explanation of Privacy Sandbox Android initiative
        - Four proposal topics (Protected Audience, SDK Runtime, Attribution Reporting, and Topics)
        - Progress updates on milestones reached so far
        - Importance of industry partnerships in shaping approach and adoption
    - Presentation by Erin Walsh:
        - Overview of beta version release for Privacy Sandbox Android
        - Usage of Jetpack library for easy integration with APIs
        - Introduction to SDK Extensions for accessing APIs
        - Detailed walkthrough of SDK Runtime and its shim library
        - New feature: Attribution Reporting API for cross-app web measurement
        - Simulation Library tool for testing and refining Privacy Sandbox features
        - Topics Classifier Jupyter Notebook for understanding topic categorization
    - Presentation by Chris Schmerling:
        - Steps for app developers to get involved with Privacy Sandbox Android beta
        - Importance of real-world testing and parallel use of existing solutions
        - Engaging ad tech partners in the ecosystem
        - Encouragement to stay updated on latest developments and provide feedback
    - Closing remarks:
        - Invitation for developers to build solutions using Privacy Sandbox Android
        - Reference to sample app repository on GitHub


## What's new in Android development tools

URL: [https://www.youtube.com/watch?v=7lubRrkxagk](https://www.youtube.com/watch?v=7lubRrkxagk)

- Introduction by Jamal Eason and Tor Norbye
    - Flamingo release highlights: Network Inspector, Themed App Icon Preview, SDK Extension
- Android Studio Giraffe and Hedgehog updates
    - Target SDK migration assistance
    - Jetpack Compose improvements
        + New project template with Material 3
        + Groovy Kotlin DSL for build scripts
    - Live Edit integration
    - Crash reporting and debugging enhancements
        + Studio Bot AI-powered chatbot
    - IDE modernization
    - Local smart actions
    - Code snippet generation
- App Quality Insights and Android Vitals integration
- Firebase Test Lab and Gradle Managed Devices support
- Wear OS updates
    - Wear OS emulator with Bluetooth support
    - New XML-based watch face format
- Build performance improvements
    - Gradle configuration caching
    - Build Analyzer
    - Baseline Profiling
- Log diagnostic tool enhancements


## Debugging Jetpack Compose

URL: [https://www.youtube.com/watch?v=Kp-aiSU8qCU](https://www.youtube.com/watch?v=Kp-aiSU8qCU)

- Define problem: Expect a certain behavior, but the actual outcome is different.
- Assumption making: Different types of recomposition (direct, indirect, and unskippable) can cause problems.
- Test assumptions: Write tests that verify your code doesn't break after you make changes.
- Debugging tools: Use Android Studio Hedgehog, a new feature for debugging Jetpack Compose. It helps diagnose recomposition issues by showing the composition state when a composable breaks.
- Logging: Use logging to help debug frequent recompositions. Wrap logs in SideEffect to avoid causing recompositions.
- Linting: Use linting to catch potential issues and optimize your code. Visual Lint support in Android Studio helps you see how your layout looks across multiple screen sizes.
- Baseline profiling: Enable baseline profiling to measure performance improvements over time.
- System tracing: Trace the system to see which parts of your app are spending the most time, allowing you to identify potential issues and optimize your code.
- Composition tracing: Use composition tracing to see how long a composable function takes to execute. This can help you identify bottlenecks in your code and make improvements.
- Benchmarking: Use benchmarking tools to quantify performance optimization. This helps you ensure that any changes you make actually improve performance.


## What's new in Kotlin for Android

URL: [https://www.youtube.com/watch?v=QGtB--ABiNM](https://www.youtube.com/watch?v=QGtB--ABiNM)

- Kotlin is a supported language for Android since 2017, with over 95% of top thousand apps in the Play Store using it directly or indirectly.
- The latest version of Kotlin (Kotlin 20) has a new compiler called K2, which offers twice as fast compilation speed compared to the previous compiler.
- Gradle Kotlin DSL is a way to define build scripts in a more readable and maintainable format. It's now the default option in the latest Android Studio preview (Android Studio Giraffe).
- Kotlin Symbol Processing (KSP) is an alternative to KAPT that analyzes Kotlin code directly, resulting in faster build times and better understanding of language constructs. It's recommended to migrate dependencies from KAPT to KSP whenever possible.
- Kotlin Multiplatform allows developers to share business logic across platforms like Android, iOS, and web by writing Kotlin code that compiles into platform-specific binaries. Google Workspace is experimenting with this technology for their internal projects.


## Best practices for saving UI state on Android

URL: [https://www.youtube.com/watch?v=V-s4z7B_Gnc](https://www.youtube.com/watch?v=V-s4z7B_Gnc)

- Introducing UI state loss scenarios
    - Configuration change (e.g., device orientation, light/dark mode)
    - System needs resources (app backgrounded, low memory)
    - User system destroys application abruptly
- Saving and restoring UI state using ViewModel API
    - ViewModel instance cached in memory
    - Keeps data intact during configuration changes
    - Used as a screen-level state holder
- Persistent storage options for app data
    - Jetpack DataStore (for small, simple datasets)
    - Room (for structured, large, complex datasets)
- Saving and restoring UI state using Compose APIs
    - `rememberSaveable` API for state needed in UI logic
    - `SavedStateHandle` API for state needed in business logic
- Advanced use cases
    - Contributing saved state with a custom saver (e.g., complex objects)
    - Controlling lifecycle of rememberSaveable values (e.g., long-lived composables)
    - Using `SavedStateRegistry` for controlling long rememberSaveable values in navigation composables


## Advanced state and side effects in Jetpack Compose

URL: [https://www.youtube.com/watch?v=TbxCz5AljQk](https://www.youtube.com/watch?v=TbxCz5AljQk)

1. Introduce the speakers and their roles.
   - Alejandra Stamato, Android Developer Relations team member at Google.
   - Manuel Vicente Vivó, also a developer relations team member at Google.

2. Explain that this is a conference about Jetpack Compose, a new way to build user interfaces for Android apps.

3. Introduce the workshop's format and goals:
   - A live-coding workshop focused on advanced state management, side effects, and using various APIs in Jetpack Compose.
   - The goal is to create an app that searches for flights and hotels, and then navigate through different screens, handle loading states, and deal with edge cases.

4. Explain the content of each section of the workshop:
   - Section 1: Setup and codelab introduction.
     - Set up the development environment and download the sample app.
     - Introduce the basic concept of handling state in Jetpack Compose using MutableStateFlow, Remember, and make sure check one first codelab.
   - Section 2: Implementing landing screen and loading data.
     - Discuss the importance of a splash screen and how to implement it with Kotlin Coroutines.
     - Use LaunchedEffect to call Suspend functions safely inside composables, and rememberUpdatedState to guarantee usage of latest lambda value.
   - Section 3: Implementing navigation drawer and creating new state holder.
     - Create a new state holder class for handling user input in an editable text field.
     - Use rememberSaveable to store instance state that survives activity recreation.
     - Discuss the importance of using saver classes when implementing rememberSaveable.
   - Section 4: Implementing ToDestinationUserInput composable and using observeStream API.
     - Refactor code to improve readability and reusability.
     - Use observeStream API to listen for changes in the UI state and update the UI accordingly.
   - Section 5: Building a better Details screen with loading states.
     - Discuss the importance of showing loading states when fetching data from the network.
     - Use produceState API to produce UI states and consume them using collectAsStateWithLifecycle API.
   - Section 6: Adding scroll top button in Explore section composable.
     - Implement a floating action button that scrolls the list back to the top when clicked.
     - Use derivedStateOf API to efficiently derive a new state from an existing one.

5. Conclude by summarizing the main takeaways from the workshop:
   - The importance of using Lambda inside side effects and considering whether they need to be restarted when their parameters change.
   - The use of rememberUpdatedState, rememberCoroutineScope, and derivedStateOf APIs for handling state and side effects in Jetpack Compose.
   - The benefits of creating reusable, modular components using composables and remembering to always think about the lifecycle of your coroutines when working with Suspend functions.


## How to build a data layer

URL: [https://www.youtube.com/watch?v=P125nWICYps](https://www.youtube.com/watch?v=P125nWICYps)

- Introduction to Android app data layer
- Role of data layer in Android application architecture
- Creating data source: local and network
- Data model for task management application
- Implementing repository: connecting data sources and exposing API
- Handling data update, complex tasks, and synchronization
- Testing the data layer with local instrumented tests
- Connecting UI layer to data layer in TasksViewModel and AddEditTaskViewModel


## How to build smarter Android apps with on-device Machine Learning

URL: [https://www.youtube.com/watch?v=BpthRGc3bM0](https://www.youtube.com/watch?v=BpthRGc3bM0)

- Thomas Ezan, Android Developer Relations Engineer, discussed the latest developments in machine learning for Android.
- He highlighted the benefits of on-device machine learning, including low latency, processing user data offline, and reducing cloud bills.
- Two examples of on-device ML experiences were shown: Lens AR Translate and an Indian team's solution to digitize important documents using OCR.
- David Lopis, Product Manager for ML Kit, detailed updates on the platform, including improvements in barcode performance and adding new features like gesture classification and digital ink recognition.
- He also mentioned that ML Kit now supports 100 languages through its Text Recognition API.
- A new Android API called Document Scanner Digitizing has been created to help users digitize physical documents easily and consistently across the Android ecosystem.
- Thomas Ezan then discussed Androids custom ML Stack, which includes TensorFlow Lite Google Play Services, hardware acceleration delegate, and Acceleration Service APIs.
- He emphasized that using these tools can make developing custom ML features faster and easier for developers.
- Finally, he encouraged developers to try out the new features and continue learning about machine learning on Android by visiting dandroid.com/ml.


## Introducing Geospatial Creator powered by ARCore and Google Maps Platform

URL: [https://www.youtube.com/watch?v=kjo40RHSQ4k](https://www.youtube.com/watch?v=kjo40RHSQ4k)

    - Stevan Silva, Product Manager at Adobe, announces the launch of Geospatial Creator, a new tool that simplifies creating augmented reality (AR) geospatial experiences.
- Geospatial API allows developers to create world-anchored AR experiences in 87 countries using Visual Positioning Service technology.
- Google Maps' decade-plus ground-level imagery helps build immersive, richer, and useful AR experiences.
- The new tool includes a client-side Geospatial API, terrain anchor, and scene semantics capabilities to help developers build world-anchored cross-platform experiences on Android and iOS devices using ARCore.
- Geospatial Creator allows developers to access Google's 3D photorealistic map tiles via the Map Tile API, enabling them to create customized immersive 3D visualization experiences for web, mobile, desktop, automotive applications, etc.
- Adobe Aero, a real-time 3D authoring tool, is integrated with Geospatial Creator, allowing creators and developers to seamlessly build immersive apps using their preferred platform (Android, iOS) without needing coding skills.
- The partnership between Unity and Adobe enables developers to create end-to-end workflows for building world-scale AR experiences.
- Geospatial Creator's integration with Adobe Aero allows creators to visualize real-world spaces using Google Maps Platform 3D tiles, navigate around them, and place digital assets in precise locations.
- The tool simplifies the process of creating interactive 3D stories anchored at specific real-world locations such as parks, museums, or neighborhood stores.
- Geospatial Creator is available today for Adobe Aero's Geospatial prerelease and Unity across ARCore-supported devices (Android, iOS).


## Get started with Geospatial Creator in Unity

URL: [https://www.youtube.com/watch?v=MDcyG9MAMAo](https://www.youtube.com/watch?v=MDcyG9MAMAo)

- Introduce Dereck Bridie, his role and the topic of the conference
    - Dereck Bridie is a developer relations engineer at ARCore. He will be introducing the new Geospatial Creator powered by the ARCore Google Maps platform.

- Explain what Geospatial Creator does
    - Geospatial Creator is a tool that allows designers and developers to build 3D content for real-world locations, providing a preview of a digital twin world.

- Describe how to use Unity with Geospatial Creator
    - To use Unity with Geospatial Creator, download the ARCore extension from the Google Drive link provided in the description video. Once installed, import the 3D asset and position it at the desired location on the map. Add realistic animation to make the object more lifelike. Finally, view the augmented reality experience on a device located at the specified latitude and longitude.

- Mention some use cases for Geospatial Creator
    - Use cases for Geospatial Creator include designing immersive shopping experiences, creating rich and powerful augmented reality games, and building 3D representations of real-world locations.

- List the dependencies needed for the project
    - Dependencies required for the project are ARCore Extensions, Cesium Extension, TextMeshPro resource, and a sample Geospatial Creator scene.

- Explain how to set up Google Cloud Project
    - To set up a Google Cloud Project, go to the Google Cloud Console, create a new project if one doesn't already exist, and enable the required APIs (ARCore API and Tiles API). Generate an API key for the project and use it in the Unity project.

- Outline the steps to set up the ARCore XR Plugin Management
    - To set up the ARCore XR Plugin Management, go to the Android tab of the plugin settings, enable ARCore, and input the generated API key from Google Cloud. Enable optional features like Geospatial Creator.

- Describe how to use the Cesium Georeference Component in Unity
    - To use the Cesium Georeference Component, add it as a component in the XR head AR Geospatial Origin object in the Inspector panel and input the generated API key from Google Cloud.

- Discuss how to find the correct latitude and longitude for a location
    - To find the correct latitude and longitude for a desired location, use Google Maps. Right-click on the map at the desired location, and copy the latitude and longitude coordinates. Paste them into the appropriate fields in the Unity project.

- Explain how to import and position 3D assets in Unity
    - To import and position a 3D asset in Unity, download the asset from the provided link, and use the Import Package function. Position the object at the correct location on the map using the Move tool and scale it as needed using the Scale tool.

- Describe how to add animation to a 3D asset in Unity
    - To add animation to a 3D asset in Unity, select the object in the hierarchy panel, click Add Component, find the Animator component, and click Add. Import an animation controller for the object.

- Explain how to view the augmented reality experience on a device
    - To view the augmented reality experience on a device, build and export the APK file from Unity. Use ADB or another program to install the APK on the device. Open the application on the device and locate it at the specified latitude and longitude.


## Get started with Geospatial Creator in Adobe Aero

URL: [https://www.youtube.com/watch?v=YFzvinINgZ4](https://www.youtube.com/watch?v=YFzvinINgZ4)

- Introduce the Geospatial Creator powered ARCore Google Maps platform
- Explain the use of Adobe Aero in creating an augmented reality experience with Geospatial Creator
- Show how to create a new project and import 3D assets into Adobe Aero
- Demonstrate how to position and scale objects within the scene
- Teach users how to add interactivity using behavior actions, such as spinning or bouncing an object when tapped
- Walk through the process of exporting and sharing the augmented reality experience via a QR code link
- Explain the importance of syncing projects in Adobe Aero before sharing
- Discuss the benefits of using Geospatial Creator to create world-anchored AR experiences


## How to develop kiosk apps for ChromeOS

URL: [https://www.youtube.com/watch?v=gQ6arLknnMY](https://www.youtube.com/watch?v=gQ6arLknnMY)

- ChromeOS kiosk mode allows apps to run in a fullscreen, locked environment without user login.
- ChromeOS supports Android, web, Linux apps, and Chrome extensions.
- Kiosk mode is used for secure, standardized testing environments, digital signage displays, customer-facing kiosks, and self-service utility stations.
- To set up a kiosk device, you need a compatible device, management license, and a kiosk app.
- ChromeOS offers a variety of hardware options from OEM partners like HP, Acer, and Lenovo.
- ChromeOS Flex is designed for niche use cases that require specialized hardware or extending the life of existing hardware.
- To deploy a kiosk app, you need to have a Chrome Enterprise or Chrome Education license.
- The Google Admin Console allows you to manage user accounts, device stats, fleet management, and workflow configuration.
- Key policy controls in the Admin Console include device monitoring, alerting, URL blocking, virtual keyboard behavior, and energy efficiency settings.
- Web apps can take advantage of advanced Chrome features like the web USB API, File System Access API, and offline content storage using service workers.
- Chrome extensions can be used to enhance kiosk app functionality by accessing sensitive device information and capabilities through ChromeEnterprise APIs.
- To deploy a kiosk app, you need to add it to the Kiosk Settings page in the Google Admin Console, where you can also set autolaunch options for specific organizational units.
- Arreya Digital Signage software service is recommended by ChromeOS as a partner for digital signage deployment due to its alignment with ChromeOS values and features.


## Building communications and contact center apps for the Web

URL: [https://www.youtube.com/watch?v=yd7uNIQVHhM](https://www.youtube.com/watch?v=yd7uNIQVHhM)

- Introduction to Google IO Tech Talk: Communication Contact Center Application Optimized for Web Differentiation
- 77% of service organizations have adopted or are accelerating remote work options in the past year.
- Cloud technology migration has allowed customers to embrace automation and rethink business continuity.
- Progressive web apps (PWAs) provide a great capability, similar to native installed applications, with reach across desktop and mobile devices.
- WebHID allows web applications to request access to device features like mouse, keyboard, touchscreen, etc., enabling more seamless integration of hardware accessories in communication contact centers.
- ChromeOS Disk Connector helps create a new desk for each customer interaction, isolating apps windows related to specific interactions and simplifying desktop management.
- ChromeOS Telemetry API enables monitoring device operation health and application performance, providing valuable insights for troubleshooting and optimizing the user experience in communication contact centers.


## How to build and integrate education apps with Google

URL: [https://www.youtube.com/watch?v=feAahFti-BA](https://www.youtube.com/watch?v=feAahFti-BA)

- Chromebooks are being used as the primary device in many classrooms.
- Technology is transforming education, making it more dynamic and engaging.
- Adaptive learning tools use AI to create personalized learning experiences for students.
- Reading mode in Chrome browser helps students with different reading preferences.
- Video conferencing features like live caption and background noise cancellation are essential for remote and hybrid learning environments.
- Screencast is a powerful tool that enables teachers and students to share their screens, annotate, and record lessons.
- Google Classroom is the central hub for education tools, where teachers can assign apps and resources directly within the platform.
- Classroom add-ons like Adobe Express and Pear Deck enhance creativity and engagement in the classroom.
- The App Licensing Program makes it easier for schools to purchase and deploy large quantities of licenses for educational apps.
- The Google Classroom API provides developers with tools to create valuable applications that integrate seamlessly with the platform.
- Progressive Web Apps (PWAs) are a recommended way to build modern classroom applications, as they offer offline functionality and can be accessed through browsers or installed standalone.
- Extensions like ReadWrite can enhance web experiences for students with disabilities.
- Android apps continue to be popular in education, but schools need to enable Google Play on their managed Chromebooks to install them.
- Developers should optimize their apps for the classroom by considering factors such as compatibility, content filtering, and effective use of Google Drive.


## Extend Google Workspace using apps, APIs, and workflows

URL: [https://www.youtube.com/watch?v=oNSRCJLG3Ps](https://www.youtube.com/watch?v=oNSRCJLG3Ps)

## Overview of Google Workspace Platform
- Addressing challenges faced by users and businesses, such as hybrid work and cloud collaboration.
- Building a platform that brings apps, data, and business processes together in one place.
- Making Google Workspace the primary hub for connecting and getting work done.
- Providing an open platform for developers to integrate their solutions with Workspace.
- Supporting a growing ecosystem of apps and addons.

## What's New on Google Workspace Platform
### Workspace Add-ons:
- A new integration framework that allows developers to build deep app integrations in Gmail, Docs, Sheets, Slides.
- Developer can create productive and collaborative experiences by engaging users with two applications simultaneously.
- Independent software vendors (ISVs) and solution developers can now integrate their solutions into Google Docs.

### Third-Party Smart Chips:
- Developers can now integrate data sources like tasks, customer records, dashboards from various favorite applications using smart chips.
- Users get a quick preview of information linked to the data source.

### Updates in Google Chat:
- A new chat API is being introduced for developers and appmakers to create chat apps, automated workflows, send alerts, and share critical data within conversations.
- The chat REST APIs are currently in preview mode and will be generally available by summer.

### Updates in Google Meet:
- A new Live Sharing SDK has been introduced for developers to create shared experiences across apps, mobile web, and synchronize medium content.
- Support for iOS and web previews is planned, with Android support currently previewing and expected to be generally available by the end of Q3.
- The Meet Add-ons SDK allows developers to integrate apps into Google Meet, enhancing user acquisition and engagement.

### Updates in Google Calendar:
- A new feature called "Working Location" is being added to Google Calendar that synchronizes third-party tools like desk booking applications or resource management tools with a user's calendar.
- The Calendar API now supports the working location field, allowing developers to manage and coordinate in-office day plans and meeting locations.

### Workspace APIs:
- A unified Workspace API Dashboard has been created on Google Cloud Console, providing a single centralized location for managing views, metrics, credentials, and quotas.
- The new Workspace API Explorer allows developers to browse, discover, make real-time API requests, see API responses, and create code samples within the browser.

### Nocode/Low Code Solutions:
- AppSheet, Google's nocode platform, now supports building chat apps, which are becoming generally available.
- The new AppSheet Databases feature offers a collaborative structure for organizing data in an easy-to-understand format.
- An AI-powered app creation tool is coming soon to the Developer Preview program, enabling developers to build solutions quickly by simply describing their app idea using natural language.

### Enhancements to Apps Script:
- The latest update includes a new feature called "Project History" or version control, which allows scriptwriters to track and manage changes in their scripts, including seeing past versions and changes made.
- Admins can now allowlist URLs per domain, providing safer access control and managing data sent externally.
- Support for the V8 runtime has been added to Apps Script, which is used by Chrome and Node.js. This allows developers to migrate legacy scripts from the old Rhino runtime if needed.

### Google Workspace Marketplace:
- A new category called "Intelligent Apps" has been introduced in the marketplace, showcasing AI-enabled apps that help users work smarter and more productively.
- With over 5200 apps available on the marketplace, developers can easily distribute their Workspace integrations or custom internally built apps to enhance user workflows and market test their solutions.

## Getting Started with Google Workspace Platform:
1. Visit the developer docs for detailed information and example code labs.
2. Join the Developer Preview Program to get access to preview features announced during the session.
3. Engage with the community by joining engaging discussions and learning from others' experiences.


## How to build no-code AI powered apps for Google Workspace with AppSheet

URL: [https://www.youtube.com/watch?v=VCo2NpDmKGg](https://www.youtube.com/watch?v=VCo2NpDmKGg)

- Introduction by Rachel Goodman Moore
- Overview of Google AppSheet: a no-code app builder that allows anyone to rapidly build applications in the Google Workspace platform without coding.
  - Reduces development lifecycle for simple apps
  - Allows creating powerful data-driven apps with flexibility and expansive expression engine
- Fit of Google AppSheet within the Google Workspace platform:
  - Extensibility power of the Google Workspace Platform allows developers to extend, integrate, customize, and collectively build solutions that cover a wide range of use cases.
- New features introduced in recent updates:
  - **AppSheet Database**: A built-in data source for nocode app creation, allowing easy and secure management of data. Features include concurrency, built-in integration, and structured column reference.
  - **Chat Apps**: Nocode Chat apps have been made available for Google Workspace users to create chatbots without writing a single line of code. These can be used for workflow automation, approval processes, task management, etc.
    - Chat Apps are designed to help teams keep everyone in the loop by allowing updates in real-time and completing tasks with important intelligence and decision-making context.
- Demo of a new feature: **Generative AI Feature (AppSheet Private Preview)**: This allows users to build customized applications using natural language, making it easier for business process owners without technical knowledge to create well-designed apps that follow app development best practices.
  - The demo showcased how the AI feature can understand user requirements and propose a general app schema based on the conversation with the user. It also demonstrated how users could refine their ideas using natural language, and the platform would generate an initial prototype of the app.
- Wrap up: A call to action for attendees to try out Google AppSheet and explore its resources and community support.


## 10 ways to use machine learning with Google Cloud, in 15 minutes

URL: [https://www.youtube.com/watch?v=oQMgqMRR-io](https://www.youtube.com/watch?v=oQMgqMRR-io)

- Teachable Machine: A browser-based tool to build image/audio classifiers in a few minutes.
- AutoML Vertex AI: A machine learning tool developer by Google Cloud that allows training custom enterprise-grade models based on data.
- Pretrained ML APIs: Powerful pretrained models that allow embedding machine learning capability directly into an application with a single API call.
- Generative AI Studio: A managed environment within Vertex AI to deploy, interact, and tune generative models in production.
- Vertex AI Model Garden: A unified environment for searching, discovering, and interacting with curated models on Google Cloud.
- Reinforcement Learning Human Feedback (RLHF): An upcoming capability that allows tuning a model by learning from direct positive/negative feedback.
- BigQuery ML: A tool to train, evaluate, and make predictions using SQL within BigQuery without needing to move data.
- Colab: A free Jupyter notebook environment in the cloud for playing around with machine learning libraries in Python.
- Vertex AI Workbench: A customizable JupyterLab instance on Google Cloud that provides great integration with other Google Cloud services.
- Vertex AI Custom Training: An enterprise-grade way to train custom models and scale them using managed, repeatable workflows.
- Vertex AI Matching Engine: A highly optimized vector database for storing and searching fast lookup embeddings of various data types like text, image, video, audio, etc.


## Securing web applications without slowing your pace of innovation

URL: [https://www.youtube.com/watch?v=7ss6J_lnucM](https://www.youtube.com/watch?v=7ss6J_lnucM)

## Introduction
- Google's comprehensive solution to prevent application fraud and abuse: Cloud Armor, Apigee, and reCAPTCHA Enterprise
- Layer 3/4 DDoS protection, WAF protection, geofencing, and bot management using Cloud Armor
- API security, governance, data security, and analytics with Apigee
- Advanced API security add-on with machine learning capabilities and real-time incident detection
- reCAPTCHA Enterprise provides comprehensive fraud prevention solution for web and mobile applications

## Cloud Armor
- Global scale defense against DDoS attacks and WAF protection
- L3/4 DDoS protection: SYN flood, UDP amplification attack
- Preconfigured WAF rule using ModSecurity
- Custom rules language, filtering policy, rate limiting, and connection management
- Geofencing capabilities to define allowed/denied regions or countries by IP address
- Layer 7 adaptive protection with machine learning functionality for detecting volumetric DDoS attacks

## Apigee
- API security, governance, data security, and analytics platform
- Manages APIs as a product, offering visibility, control, and security configuration
- Integrates with GCP's IAM, logging, and monitoring services
- Developer-focused service with an API catalog, API product marketplace, and transformation capabilities for serving multiple runtime environments (multicloud SaaS on-prem)
- Advanced API security add-on provides real-time incident detection, abuse detection, access control, and data encryption functionality

## reCAPTCHA Enterprise
- Comprehensive fraud prevention solution for web and mobile applications
- Analyzes user behavior to detect anomalies and prevent fraudulent activity
- Supports device signal integration, account defender workflow, and real-time payment instrument data analysis
- Offers a low-friction user experience while maintaining robust security measures


## Securing development environments with Google Cloud

URL: [https://www.youtube.com/watch?v=hVTnW-rH5Vc](https://www.youtube.com/watch?v=hVTnW-rH5Vc)

- Google Cloud Product Manager, Marcos Grappeggia, presented on secure development environment and overall software supply chain.
- The presentation introduced the Google Cloud Software Delivery Shield product, which provides a comprehensive solution for securing the entire software supply chain.
- Two recently introduced products were also discussed: Cloud Workstations and Assured Open Source Software.
- The main challenge addressed by Cloud Workstations is to provide a consistent, customizable development environment that can be accessed from anywhere via a browser.
- Cloud Workstations offers various features such as GPU acceleration for data processing tasks, support for multiple regions, and integration with popular IDEs like IntelliJ, PyCharm, and Rider.
- BeyondCorp Enterprise is integrated into Cloud Workstations to provide additional security measures against data exfiltration and unauthorized access.
- Assured Open Source Software provides a way to improve software supply chain security by incorporating trusted open source packages and enriching metadata around known vulnerabilities.
- The presentation also discussed the importance of DevOps tuning support, which allows organizations to incorporate existing tools and processes without disrupting their current workflows.


## How to build better conversational experiences with generative AI

URL: [https://www.youtube.com/watch?v=50EJft0ILUI](https://www.youtube.com/watch?v=50EJft0ILUI)

- Introducing Kris Overholt as a Developer Advocate for Google Cloud
- The focus of the talk is on boosting conversational experiences using generative AI
- Current chatbot capabilities and their limitations are discussed
- Three types of conversational capabilities that can enhance customer experience: informational, transactional, and generative
- Large Language Models (LLMs) are introduced as a powerful tool for generating content in real time
- The timeline of Google's investment in AI technology is presented
- Gen App Builder is introduced as an easy way to create chatbots and other conversational experiences using LLM-powered conversation design principles
- Informational, transactional, and generative capabilities are discussed in the context of Gen App Builder
- The benefits of using Gen App Builder for faster chatbot development and improved customer experience are highlighted
- Generative fallback response is introduced as a way to handle unpredictable customer interactions
- The importance of natural language prompts in chatbot development is emphasized
- Complex graph state transitions and efficient conversation flow management are discussed as key features of Gen App Builder
- The need for a balance between domain-specific information and general knowledge in AI-powered chatbots is highlighted
- Resources for learning more about Google Cloud's generative AI solutions are provided


## How to build data-driven apps with Google Cloud's latest services

URL: [https://www.youtube.com/watch?v=1oNKCEAqdvY](https://www.youtube.com/watch?v=1oNKCEAqdvY)

## Summary
- Google Cloud's database services like Cloud SQL, AlloyDB, and Spanner offer different benefits for various use cases.
- The three representative examples discussed were fraud detection in credit card transactions, reporting on DVD sales, and pricing analysis for coupons.
- Federated query allows developers to run queries across databases and data warehouses without moving data.
- Streaming replication continuously sends changes from one database to another, making it easier to create reports with fresh data.
- Integrated analytics enables users to skip the data warehouse step and analyze data directly within the database.
- AlloyDB Omni is a downloadable version of Google's Postgres-compatible database that can run anywhere, including on-premises environments or other cloud providers.
- The columnar engine in AlloyDB Omni makes analytical queries run 100x faster compared to traditional row-based storage.
- Machine learning algorithms help manage memory usage and optimize performance in AlloyDB Omni.


## How to build web applications with Firebase and Google Cloud

URL: [https://www.youtube.com/watch?v=zW86dgr85NE](https://www.youtube.com/watch?v=zW86dgr85NE)

- Introducing Firebase and its powerful features
- Demonstration of deploying a blog application using Google Cloud Firebase and Cloud Run
- Explanation of the process to integrate Firebase Hosting with Cloud Run in one click 
- Discussion on how global CDN can improve performance of web applications
- Addition of user authentication using Firebase Authentication
- Implementation of sending weekly digest emails by scheduling a recurring job using Google Cloud Scheduler and Firebase Extension
- Explanation of the process to set up SMTP server for email delivery in Firebase console.


## Image data classification with BigQuery ML

URL: [https://www.youtube.com/watch?v=uErqSYCiBls](https://www.youtube.com/watch?v=uErqSYCiBls)

- Introduce the purpose of the talk: To demonstrate how to store, process and analyze image data using Google Cloud's BigQuery.
- Explain that BigQuery can now handle unstructured data types like images and videos.
- Mention that analytics and ML operations can be performed directly on this data using SQL queries or BQML.
- Provide a step-by-step guide to setting up the required infrastructure: 
  1. Create a dataset in BigQuery.
  2. Set up a BigLake connection.
  3. Create a Google Cloud Storage bucket and upload an image.
  4. Grant permission for BigQuery to access the image in the storage bucket.
  5. Create an external object table in BigQuery, referencing the images stored in the GCS bucket.
- Show how to query the external object table and visualize results.
- Explain how to create a model using pretrained TensorFlow models, upload it to Google Cloud Storage, and use it for prediction with BQML.
- Demonstrate how to combine structured and unstructured data in a single SQL query, and query them together.


## Harnessing the power of generative AI to deliver next-gen search experiences

URL: [https://www.youtube.com/watch?v=HD_xreaLKb4](https://www.youtube.com/watch?v=HD_xreaLKb4)

- Kaz Sato and Holt Skinner introduce the concept of enterprise search, highlighting its three main challenges: handling large amounts of data in various formats at scale, implementing traditional keyword search restricted especially with AI-powered recommendations, and taking advantage of generative AI LLMs for information retrieval.

- The speakers discuss Google Cloud's Gen App Builder, which allows developers to create enterprise search services easily without having to build the entire DevOps team or manage infrastructure themselves. It supports creating search engines, chat interfaces, and integrating with existing applications.

- They demonstrate how to set up a basic search engine using structured and unstructured data sources such as websites and cloud storage files.

- The speakers then discuss semantic search technology, which allows users to find content based on its meaning rather than just keywords. This is achieved by converting raw data into embeddings that represent the content's meaning in a high-dimensional space. These embeddings can be used across various Google services like Google Search and YouTube for better recommendation systems.

- Finally, they showcase how LLM capabilities can enhance enterprise search experiences by providing summaries of search results and enabling multimodal searches (e.g., searching for an image or text related to a specific product). These features improve information retrieval productivity in various industries such as retail, finance, and healthcare.


## Material You for large screens guidance

URL: [https://www.youtube.com/watch?v=wP-xAPIyqLY](https://www.youtube.com/watch?v=wP-xAPIyqLY)

- Intro by Anna Pfoertsch, Product Manager for Material Design
- Focus on adaptive design for large screens (tablets, foldables, desktops)
- Overview of window classes: compact, medium, expanded
- Window class examples: Pixel 7 in portrait mode is considered "compact"; when rotated to landscape mode, it becomes "expanded"
- Large screen optimization involves leveraging established print design concepts like hierarchy, typographic expression, contrast, composition, tension, and wayfinding
- Material 3 encourages designers to express themselves more boldly and vividly, especially on larger screens
- Design best practices for large screens:
    - Use navigation bar, rail, or drawer based on window class
    - Utilize panes (one, two, or exclusive) to organize content within the body region
    - Consider phone landscape orientation as an expanded window class
    - Incorporate landscape mobile adaptation when necessary
- Hardware considerations for folding devices:
    - Display cutouts may obstruct design elements
    - Offset body regions if part of UI is obscured by hardware
- Sneak peek into planned Google Apps large screen updates in line with Material 3 design language
    - Expressive, spirited UIs that feel human and alive
    - Shapes and motion add emotion and liveliness to the UI
    - Personalization options allow users to tailor their environment based on ergonomic placement and control
- Adaptable UIs enable users to customize environments according to their needs and preferences.


## How to build a complete app with Relay and Compose

URL: [https://www.youtube.com/watch?v=vBNmeiHlDHE](https://www.youtube.com/watch?v=vBNmeiHlDHE)

**Summary of transcript:**
- Justin and Travis will guide you through building a complete app using Relay Compose
- They will create an app called Reflect, which is a daily habit tracker meant to promote mindfulness and good habits.
- Justin has designed the app in Figma, using UI packages and importing them directly into Jetpack Compose projects.
- Travis will share his screen and follow along with the coding process.
- The codelab will help you understand how to integrate Relay workflow, UI package imports, pattern state behavior, and map theme component in Figma Compose.
- You'll learn about terms like Figma Compose, design spec, visual bug thing past, and more.
- The workshop includes a demonstration of the complete app building process using Android Studio.
- They will share their experiences working on the project and discuss various aspects of designing an app with UI packages.



## Making beautiful products more accessible with Material research

URL: [https://www.youtube.com/watch?v=k-nG86tp8oQ](https://www.youtube.com/watch?v=k-nG86tp8oQ)

- Introduction to Accessible Design
    - Julia Feldman, Researcher at Material Design, explains the importance of designing products that work for everyone, including people with disabilities.
    
- Overview of Accessibility Standards and Guidelines
    - Dahlia Shoukry, Program Manager at Google's Product Inclusion Equity Accessibility team, presents a brief overview of accessibility standards and guidelines that enable creativity in the product development process.
    
- Designing for Accessibility: A Case Study
    - Julian Gonzalez, UX designer at Material Design, shares his work on making things accessible while maintaining an expressive design aesthetic. He discusses designing a focus state for a thermostat interface and how research played a crucial role in refining the design.
    
- Testing Accessibility: A Real-Life Example
    - Julia Feldman presents a case study where they tested different variations of a focus state button to determine which design performed best in terms of accessibility and usability.
    
- Conclusion
    - The speakers emphasize the importance of designing products that are accessible, beautiful, and relevant for everyone. They encourage designers to embrace accessibility standards and guidelines, conduct research, iterate designs, and work collaboratively with their teams to create inclusive experiences.


## Build modern Android apps with Material You for Compose

URL: [https://www.youtube.com/watch?v=tu0UtDGC31A](https://www.youtube.com/watch?v=tu0UtDGC31A)

- Gurupreet Singh introduces Material Design 3, a radical way of designing personalized and accessible user experiences in apps.
- Material You empowers developers to create expressive and spirited personal apps using dynamic theming, flexible components, and motion updates.
- Material 3 is the latest design system that provides customization options for color, shape, typography, and more.
- It is recommended to use Material 3 as the default design system when creating new projects in Android Studio.
- The Material 3 Compose library offers a wide range of components needed to build production-ready apps.
- New additions to the Material 3 Compose library include bottom sheet, date selection component, and time picker.
- The Material 3 design system also includes various topic bars like centerline, small, medium-large, and large, which provide flexibility in choosing the right product.
- Material 3 components come with built-in motion animation that provides interactive user experiences through separation, color changes, highlighting switch states, etc.
- Migrating apps from Material 2 to Material 3 can be done using either one-shot migration or phase migration strategies based on the complexity of the codebase.
- Key differences in designing systems between Material 2 and Material 3 include additional color options, slight changes in typography naming, and increased number of parameters for shape customization.
- To help with planning migrations from Material 2 to Material 3, developers can use the Material Team Builder tool to generate color schemes based on brand colors.
- Dynamic theming is a key feature of Material 3 that allows apps to adapt to light or dark color schemes depending on the device wallpaper and system theme.
- Some components in Material 3 have been redesigned, renamed, or introduced to embody the new Material You design language, such as buttons, navigation components, and chips.
- Accessibility is a standard built into all Material 3 components, ensuring that products are designed inclusively for everyone.


## How to enhance your app security with new features in Firebase App Check

URL: [https://www.youtube.com/watch?v=iYA0QYP9ocw](https://www.youtube.com/watch?v=iYA0QYP9ocw)

    - Introduces Firebase App Check, a new feature to enhance app security
      - Prevents unauthorized client access and API resource abuse
      - Works in conjunction with strong authentication key part of the Firebase Security suite
      
    - Firebase App Check token lasts for short time to avoid expensive attestation process
    
    - New replay protection feature provides a single-use token for specific endpoints without sacrificing performance
      - Ensures tokens cannot be stolen or reused, future requests are denied
      - Can be used in conjunction with Firebase Cloud Functions and other Firebase services
      
    - App Check is now integrated with Firebase Auth Identity Platform
      - Upgrade to the latest Firebase SDK for automatic forwarding of context
      - Provides stronger security when protecting user authentication flow
      
    - Custom attestation provider support allows federating attestation providers, ensuring endpoint authentication across platforms
      - Works using API and supports multiple platforms including Android, iOS, web, C#, and Unity
```


## What's new in Firebase Crashlytics

URL: [https://www.youtube.com/watch?v=UmAFhPeCNF4](https://www.youtube.com/watch?v=UmAFhPeCNF4)

- Crashlytics update at Google I/O
    - Strengthening mobile ecosystem: Integrating with popular developer products like Firebase and Android Studio
    - Addressing crash troubleshooting journey: Finding, fixing, and identifying offending lines of code in the crash report
    - New feature: App Quality Insights window in Android Studio for native apps
        + Addresses tricky memory errors
        + Multiple strategy improvement to help map crash reports to exact lines of code
- Exciting updates on crash grouping algorithm
    - Variant subgrouping: One step closer to identifying root cause of pervasive crashes
        + Indicates user impact in the issue list
        + Provides stack trace grouped together for multiple variants
- Improved stack trace across popular languages:
    - Symbolication engine enhancements for better mapping of crash reports to code
    - GWPASan support for finding and fixing memory bugs in production environments
- New searchable metadata added to issue list:
    - Exception type, package name, etc., allow developers to easily search and filter issues
- App Quality Insights feature update:
    - Support for navigating version app and blamed commit point time crash happens
    - Compare current blamed commit version with previous versions to get insights into the state of codebase at the time of crash


## Ship faster with feature flags using Firebase Remote Config

URL: [https://www.youtube.com/watch?v=vWJ8wDzeEg0](https://www.youtube.com/watch?v=vWJ8wDzeEg0)

- Introduction to Feature Flagging
    - Importance of feature flagging in app development
    - Decoupling app version release from feature launch
    - Reducing production risk by enabling gradual rollout and quick bug fixes
- Firebase Remote Config and Feature Flags
    - Overview of Firebase Remote Config
    - Using feature flags with Remote Config to control new feature exposure
    - Targeting specific users or app versions with feature flags
- Realtime Updates in Remote Config
    - Enabling real-time updates in Remote Config for dynamic configuration changes
    - Monitoring and responding to real-time updates in the app
    - Using real-time updates to quickly adjust feature exposure based on user feedback
- Gathering User Feedback with Google Analytics
    - Integrating Google Analytics with Firebase apps
    - Collecting user data and insights for better decision making
    - Monitoring key metrics such as daily active users, crash rates, and more
- Crashlytics for Bug Detection and Tracking
    - Using Crashlytics to analyze app crashes in real time
    - Identifying bug patterns and trends for faster resolution
    - Integrating Crashlytics with Firebase apps for seamless bug tracking
- Conclusion
    - Recap of the benefits of feature flagging and Remote Config usage
    - Encouragement to adopt best practices for app development and release management
    - Emphasis on continuous learning and improvement through real-time monitoring and feedback.


## Connect millions of developers to your product by building a Firebase Extension

URL: [https://www.youtube.com/watch?v=PLOw63DvZdc](https://www.youtube.com/watch?v=PLOw63DvZdc)

- Introducing Firebase Extensions
    - Easier integration for developers
    - Provides code running closest to developer data and database logic
    - Simplifies the process of writing, testing, and deploying custom functions
- Building an Invoice Generation Extension with Firebase Extensions
    - Create a Node.js cloud function that responds and writes to Firestore
        + Triggered by insert events in Firestore collections
        + Calls backend service to generate invoice
    - Wrapper Cloud Function for user installation
        + Configurable values (e.g., Firestore collection, API key)
- Deploying an Extension
    - Sign and publish the extension on extensionsdev
    - Upload the extension registry using Firebase CLI
- Benefits of Building Extensions with Firebase
    - Charges only for installation beyond underlying cloud resource cost
    - Provides offline mode support
    - Helps distribute server load evenly
- Firebase Developer Community and Extension Ecosystem
    - Many companies are building extension APIs to integrate with Firebase services
        + Algolia, Elastic, Meilisearch for search functionality
        + Stripe, RevenueCat, Purchasely for subscription payment integration
    - Dozens of extensions available on extensionsdev
- Future of Extensions with Firebase
    - Google is launching multiple new extensions and encouraging developers to build more
    - Leverages PaLM API, Vertex AI, and other Google Cloud's AI technologies


## Manage Firebase projects with Terraform

URL: [https://www.youtube.com/watch?v=32SKh-jGXI4](https://www.youtube.com/watch?v=32SKh-jGXI4)

- Intro: Lingkai Shen introduces Sally, a software engineer who wants to adopt best practices for setting up staging and production environments.
- Current Approach: Sally currently uses the Firebase Console to create and manage two separate Firebase projects for staging and production.
- Problems with Current Approach: It's time-consuming, not scalable, and there's a risk of accidentally breaking the production environment.
- Introducing Terraform for Firebase Integration: Lingkai explains that Terrafire is an open-source tool from HashiCorp that allows managing infrastructure code. It can be used to describe and provision resources in the cloud, including those provided by Firebase.
- Setting Up a Prod Project with Terraform: Sally creates a Terraform configuration file for her production project. She uses the `googlebeta` provider and sets up authentication, Firestore database, etc.
- Adding Firebase Resources: Sally adds platform variants (Android, iOS, web) to her production project using Terraform resource blocks. She also sets up authentication and Firestore database.
- Enabling Services: To enable certain services for the production project, Sally uses the `Service Usage API` and `Firebase Management API`.
- Creating a Staging Project with Terraform: Sally copies her production project's Terraform configuration file, tweaks it slightly to create a staging project. She can reuse most of the code from the production project without copying it.
- Managing Changes in Infrastructure: If Sally wants to make changes to the infrastructure (e.g., update security rules for Firestore), she simply modifies her Terraform configuration files and runs `terraform apply` again. This ensures that both staging and production environments stay synchronized.
- Benefits of Using Terraform: By using Terraform, Sally can manage multiple similar Firebase projects confidently. It allows her to track changes in the infrastructure, test new features on the staging environment before deploying them to production, and easily replicate resources across different environments.


## Faster, cheaper serverless APIs for your Firebase app

URL: [https://www.youtube.com/watch?v=EIA58FKrA8Y](https://www.youtube.com/watch?v=EIA58FKrA8Y)

- Jeff Huleatt and Daniel Lee introduce the Second Generation Cloud Functions Firebase, which allows users to run serverside code in a trusted environment without managing scaling servers.
- The SecondGen infrastructure has graduated from public preview to general availability.
- Python programming language is now supported by Cloud Functions Firebase.
- Firebase CLI tools and library support for writing functions in Python.
- Concurrency settings can be adjusted to optimize performance and reduce costs for function instances.
- The new generation of Cloud Functions provides powerful instance types with 32GB memory, longer request timeout options, and the ability to deploy functions in new regions like London and Delhi.


## How to run dynamic web apps on Firebase

URL: [https://www.youtube.com/watch?v=MhkDpZA_Ciw](https://www.youtube.com/watch?v=MhkDpZA_Ciw)

- Firebase Hosting can deploy dynamic full stack web apps with atomic deployment.
- The new support for atomic deployment ensures server and client-side code always sync.
- Dynamic preview channel allows testing changes before deploying to production.
- Experimental support for Nextjs, Angular Universal, React Server Components, Nuxt 3, and SvelteKit is added.
- Firebase Deploy supports Flask and Django Python backends.
- Firestore CLI detects Flutter projects and calls the web target for deployment.
- Enhanced support for Nextjs, Angular Universal, and Nuxt 3 has been added.
- Fast dev mode is available for local development with Nuxt Vite.


## What's new in Firebase's Android SDK

URL: [https://www.youtube.com/watch?v=nYGg_8Ro5VE](https://www.youtube.com/watch?v=nYGg_8Ro5VE)

## Summary of Firebase Talk by Miguel Ramos
* Last year improvements:
  + Crashlytics - crash reporting and analysis service
  + Performance Monitoring - real-time quality insights for app performance metrics like start time, user device network latency
  * New App Quality Insights window in Firebase Web Console
  * Integration with Android Studio for faster bug fixing
* This year's focus:
  + Improving performance and developer experience
  + Making Kotlin the default programming language for building Firebase apps
    * Redesigning APIs to offer idiomatic support for Kotlin
    * Using asynchronous functions (suspend functions) in Kotlin first design
* Other updates:
  + App Distribution - general availability of app distribution feature for early testing and feedback collection
  + App Check - enhanced security measures to protect backend infrastructure from tampering or hacking attempts
    * Integration with Play Integrity API for Android device anti-abuse signals
* Upcoming improvements:
  + Faster Android app startup times (3 seconds faster on average) due to optimized thread pool, background workers, and delayed product initialization
    * Java APIs will still be available for those who prefer them.
```


## What’s new in Cloud Firestore at Google I/O 2023

URL: [https://www.youtube.com/watch?v=rW9MkxD5318](https://www.youtube.com/watch?v=rW9MkxD5318)

- Firestore's mission is to unlock application innovation, simplicity, speed, and confidence.
- It is a serverless document database built on Google's infrastructure.
- Features include serializable asset transaction, elastic scalability, high availability, five nines availability, external consistency, multiversion concurrency control, real-time sync, offline caching, ecosystem integration, and native support for Google Cloud Firebase and third-party services via extension.
- The new query feature enables developers to find the data they need more easily and make it easier to get insights from their data.
- A count function has been introduced to perform cost-efficient scalable count aggregation.
- OR operator support has been added, allowing developers to compare one field with two values or use nested ORs for multiple conditions.
- The IN operator's limit has been extended to 30 values, enabling broader scale lookups.
- Eventarc integration is being introduced, providing a more integrated end-to-end developer experience and allowing Firestore to generate cloud events that can be delivered to various destinations like limited Cloud Functions Gen 2, Cloud Run, GKE, etc.
- Terraform infrastructure-as-code support has been added for creating new Firestore databases and managing resources declaratively. Developers can now use Terraform to create a provisioning plan file with gcloud commands and run scripts using the command line.
- Multiple database per project feature is also available, enabling developers to create multiple named databases within one project.


## Evolving Flutter's support for the web

URL: [https://www.youtube.com/watch?v=PY42FysQTgw](https://www.youtube.com/watch?v=PY42FysQTgw)

- Introducing Kevin Moore, Product Manager for Flutter Dart team at Google
- Flutter web growth: double the number of domain deploying Flutter last year
- Three new improvements in Flutter 3.10:
    - Faster application load time
        - Optimized CanvasKit binary size by 50%
        - Reduced font size from 400KB to 10KB for icon fonts
        - Decreased total load time of a sample app by 40ms
    - Better integration with traditional web experiences:
        - Element embedding allows Flutter apps to be hosted within an arbitrary HTML element, making it easier to combine content and interact with JavaScript.
    - Improved support for Firebase integration
- Preview features in Flutter 3.10:
    - WebAssembly (Wasm) support:
        - Dart's garbage collection is compatible with WasmGC, which will improve performance, fidelity, and runtime consistency of Flutter web apps.
        - A fallback to JavaScript will be provided for browsers that do not yet support WasmGC.
- Upcoming features in future releases:
    - WebAssembly compilation of Dart code
        - The goal is to shrink load time, improve frame rate, and enhance runtime consistency of Flutter web apps.
```


## Deep dive into Flutter deep linking

URL: [https://www.youtube.com/watch?v=6RxuDcs6jVw](https://www.youtube.com/watch?v=6RxuDcs6jVw)

    - Zoey, a Flutter product manager, and ChunHeng, a software engineer on the Flutter team, discuss the importance of deep linking in mobile apps.
    - Deep links point to specific resources within an app.
    - Universal links and App Links are types of deep links that work best for implementing deep linking.
    - Benefits of adding app deep links include driving traffic from other channels (like marketing emails or SMS), creating personalized experiences, and improving long-term engagement.
    - Implementing deep linking involves setting up both the web and the app sides.
        + Web setup: Host a specific web file (assetlinks.json for Android, apple-app-site-association for iOS) that contains metadata about the app, including its package name or ID and allowed paths.
        + App setup: Modify your project parameters and write Dart code to handle redirects from the web.
    - For Android, update the manifest file with intent filters and associate it with the hosted assetlinks.json file.
    - For iOS, add an associated domain in the Info.plist file and host the apple-app-site-association file on your website, allowing specific paths.
    - After setting up deep linking, monitor its effectiveness by tracking conversion rates and engagement metrics.


## Introducing Impeller - Flutter's new rendering engine

URL: [https://www.youtube.com/watch?v=vd5NqS01rlA](https://www.youtube.com/watch?v=vd5NqS01rlA)

- Intro: Brandon and Leigha introduce themselves as a software engineer and product manager for Flutter.
- Impeller: They discuss the new Flutter renderer, Impeller, which is designed to improve rendering performance by focusing on Flutter's specific needs. The goal is to eliminate jank and stuttering in applications.
- Overview of Rendering Process: Leigha explains how a renderer translates UI code into pixel display. They describe the render pipeline, shaders, vertex and fragment shaders, and how these elements work together to draw pixels on the screen.
- Impeller Architecture: Brandon dives deeper into Impeller's architecture, explaining how it uses Aiks, entity framework, hardware abstraction layer, and shader compilation to communicate with GPUs efficiently.
- Benefits of Impeller: Leigha highlights some key benefits of using Impeller, such as improved performance through precompiled shaders, efficient antialiasing techniques, and fast clipping operations.
- Demo Widget: They showcase a demo widget called "Scene Box" that allows developers to create 3D scenes using assets exported from 3D modeling software like Blender or Maya. This widget is built on top of Impeller's hardware abstraction layer and supports hot reload functionality.
- Announcement: Leighan announces that Impeller is now the default renderer for iOS in a stable version of Flutter, and they look forward to future updates that will bring Android support as well.


## What's new in Firebase's Flutter SDK

URL: [https://www.youtube.com/watch?v=PsV-wSZ2BCo](https://www.youtube.com/watch?v=PsV-wSZ2BCo)

## Key Points:
- Flutter SDK Firebase update: Improved documentation, 100 code samples, new onboarding flow, easier integration with Crashlytics, improved crash reporting and faster issue resolution.
- Feature parity with native Android & iOS SDKs for Firebase in Dart language.
- Windows development support for Flutter apps using Firebase: Faster iterative cycles, stateful hot reload, and hot restart features.
- Extended Firebase C SDK support for desktop development workflow.
- Continued collaboration with the Flutter community to evolve and complete support for all platforms.


## Rethinking Dart interoperability with Android

URL: [https://www.youtube.com/watch?v=ZWp2FJ2TuJs](https://www.youtube.com/watch?v=ZWp2FJ2TuJs)

- Introduced Dart Native Interop and Flutter team's new approach to system interoperability.
- Demonstrated how Flutter app plugin developers can call Android libraries directly from Dart using JNIgen tool.
- Explained the difference between Pigeon, FFIgen, and JNIgen tools in generating typesafe interop code for communicating with platform APIs.
- Showcased a simple example of using JNIgen to generate Dart bindings for a Java class and demonstrated how to use these bindings in a Flutter app.
- Provided an overview of the Health Connect Kotlin library and its integration with JNIgen to build a pedometer app.
- Discussed the four steps involved in using JNIgen, including adding dependencies, creating config files, generating bindings, and updating build configurations.
- Mentioned that while JNIgen is still considered experimental, it has the potential for significant improvements in Flutter development workflows.


## How to build next-gen UIs in Flutter

URL: [https://www.youtube.com/watch?v=HQT8ABlgsq0](https://www.youtube.com/watch?v=HQT8ABlgsq0)

- Introducing a new Flutter project: Next Gen UIs
- Building the UI using Flutter's widgets and layout tools
- Implementing animations using Flutter Animate package
- Adding hover effects, color changes, and custom shaders to enhance the UI
- Creating a particle field around the orb for added visual interest
- Writing custom handlers for button interactions and state changes
- Using ticking builders and animation controllers to create dynamic effects
- Finalizing the UI with all features implemented


## How to build a package in Dart

URL: [https://www.youtube.com/watch?v=8V_TLiWszK0](https://www.youtube.com/watch?v=8V_TLiWszK0)

- Intro: John Ryan talks about how to build and publish Dart Flutter packages.
- Creating a package: The process involves creating a new Dart package, setting up the directory structure, writing code and tests, and specifying dependencies and versions in the pubspec.yaml file.
- Dependency management: Jonas Jensen explains how to manage dependencies, including understanding semantic versioning, using constraints to specify dependency requirements, and dealing with breaking changes.
- Continuous integration: John Ryan discusses setting up continuous integration for packages using GitHub actions and Dart's tools for running tests, checking code formatting, and analyzing code quality.
- Publishing a package: Jonas Jensen covers the process of publishing a package, including creating a license file, reviewing the changelog, and confirming the package details before publishing. He also explains how to retract a published package version if necessary.
- Package homepage: John Ryan shows what a typical package homepage on pub.dev looks like, including the package description, usage example, topic tags, and API documentation links.
- Group packages and publisher accounts: Jonas Jensen mentions that organizations can create group packages and assign publisher accounts to maintain them, which helps developers find relevant packages more easily.
- Package health scores: The talk also introduces the concept of package health scores based on popularity, maintenance, and testing quality, which helps developers evaluate the suitability of a package for their projects.


## Flutter, Dart, and Raspberry Pi

URL: [https://www.youtube.com/watch?v=0CCVB31feO0](https://www.youtube.com/watch?v=0CCVB31feO0)

    - Introduction to the project: Flutter Dart Raspberry Pi game console project
        + Camille and Khanh introduce themselves as software engineers working on the Flutter team.
        + They talk about the Raspberry Pi, a single-board computer, and its popularity for various use cases.
    - Project requirements:
        + A gamepad is required for a proper gaming experience.
        + The gamepad should be connected via USB to access GPIO pins.
        + The code should be reusable and extensible so that the community can contribute.
    - Setting up the Raspberry Pi:
        + They installed Debian-based operating system, Raspberry Pi OS 64 bit, using an SD card.
        + Flutter was installed on the Pi via Canonical's app store called Snap.
    - Assembling the game hat:
        + The game hat is a module that attaches to the Pi and includes a preinstalled screen, joystick, and GPIO pins.
        + They connected the game hat to the Pi using an HDMI cable and screw support pieces.
    - Reading GPIO signals in Dart:
        + They used the rpi_gpio package to read input values from GPIO pins.
        + Each button on the gamepad was mapped to a specific GPIO pin, which they represented with an enum.
    - Implementing the Gamepad class:
        + The Gamepad class listens for changes in GPIO pin states and triggers events when a button is pressed or released.
    - Integrating the gamepad input into Flame:
        + They extended the Flame work game hat by creating a mixin called GamepadInput.
        - This mixin listens to gamepad broadcast streams and triggers callbacks when new events are detected.
    - Implementing game console menu navigation using Flame widgets:
        + They used a GamepadHandler class with five callback methods (left, right, confirm) for navigating through the menu.
    - Adapting the UI for smaller screens and testing the final product:
        + The UI was adapted to fit smaller screens, and the game console was tested on the Raspberry Pi.
    - Future plans for the project:
        + They plan to share the source code with the community and continue building upon it.
        + The Flutter Samples repository will be updated soon with this project.


## Design for every device with Flutter and Material 3

URL: [https://www.youtube.com/watch?v=CfOlY36GWYU](https://www.youtube.com/watch?v=CfOlY36GWYU)

    - Intro:
        Liam Spradlin and Rody Davis introduce the topic of building a new app, Pesto, using Flutter Material 3. They emphasize that the conference will provide insights into designing adaptive layouts, customizing brand parameters, and creating responsive user interfaces across different platforms.
    
    - Designing Adaptive Layouts:
        - The importance of adaptive structure is highlighted since apps need to be usable and recognizable across various devices and contexts.
        - Flutter supports six platform variations, which may result in significant white space differences.
        - Testing is essential for ensuring that the app functions correctly on different platforms and screens.
    
    - Customizing Brand Parameters:
        - Understanding how to adapt a brand's parameters within an interface is crucial for maintaining consistency while still catering to each platform's specific needs.
        - The Material design system helps map relationships between color components, ensuring that the app remains consistent and respects priority when translating the brand system.
    
    - Creating Responsive User Interfaces:
        - Grid-based layouts are an effective way to conceptualize interface designs, as they provide a convenient structure for aligning elements and organizing content.
        - The 12-column grid is introduced as a common method of creating responsive layouts in Flutter, with the help of packages like Flutter Layout Grid.
    
    - Personalizing the App's Aesthetic:
        - Pesto's brand color palette is showcased and integrated into the app's design using Material 3's dynamic color capability.
        - The use of M3's color utility method to create a new color scheme based on an image provider is demonstrated, allowing for personalized content-based dynamic colors that respect the brand aesthetic.
    
    - Typography and Text Design:
        - The importance of consistent typography across all platforms is emphasized, with Montserrat and Lekton being used as the primary fonts in Pesto.
        - Type scale is introduced as a way to represent different types of information through varying typefaces and sizes.
    
    - Customizing Material App Components:
        - The concept of preserving base usability while customizing components to emphasize or de-emphasize certain visual elements is discussed.
        - The use of Material tokens for referencing colors, surfaces, and other design elements is highlighted as a way to maintain consistency across the app's various components.
    
    - Conclusion:
        - Liam Spradlin and Rody Davis conclude by emphasizing the importance of considering context when designing apps that will operate on multiple platforms and input modalities. They encourage attendees to explore further resources, such as GitHub repositories and tool guides, to help them build great experiences across devices.


## Announcing Photorealistic 3D Tiles and the Aerial View API to create immersive map experiences

URL: [https://www.youtube.com/watch?v=Yj11hdq2jgA](https://www.youtube.com/watch?v=Yj11hdq2jgA)

- Introduction by Travis McPhail, Engineering Lead for Visualization at Google Maps Platform
- Announcement of new 3D experience capabilities with the Photorealistic 3D Tiles Map Tiles API and Aerial View API
- Explanation that Google Maps Platform will offer a comprehensive 3D map using 3D Tiles OGC standard, currently available in 2500 cities across 49 countries
- Discussion of the Photorealistic 3D Tiles Map Tiles API, which enables developers to create immersive experiences without having to produce photorealistic 3D maps themselves
- Collaboration with Cesium for rendering performance improvements and support for tile geofencing capabilities
- Demonstration of how the Photorealistic 3D Tiles Map Tiles API can be used in conjunction with ARCore, Unity Geospatial Creator, Adobe Aero, and other tools to create immersive experiences
- Introduction of the new Aerial View API, which allows developers to easily generate cinematic 3D videos without needing to capture drone images themselves
- Call for developers to explore the new capabilities and provide feedback through bug reports, feature requests, and collaboration on improving the 3D offering
- Encouragement to continue pushing the boundaries of immersive interactive 3D experiences in the real world


## How to use data-driven, advanced styling capabilities to build immersive geospatial experiences

URL: [https://www.youtube.com/watch?v=5dAqwpNJbnw](https://www.youtube.com/watch?v=5dAqwpNJbnw)

- Google Maps Platform team introduced three new customization and styling features for building engaging map experiences.
- The first feature is the general availability of datadriven styling boundary, which allows users to customize the appearance of Google's polygon maps based on region, country, state or county.
- The second feature is the launch of a new preview release called "datadriven styling datasets", which enables users to visualize and style geospatial data sets such as airports, subway lines, national parks etc., using their own, open-source or purchased data.
- The third feature introduced today was an advanced marker capability, which allows for more customization options like changing the outline color, glyph, SVGs, and HTML elements to create a richer user experience.
- These features are aimed at providing users with more control over their map designs, enabling them to tell stories and provide insights through visualizations.


## What's new in TensorFlow

URL: [https://www.youtube.com/watch?v=EPBBUT4Q2Fg](https://www.youtube.com/watch?v=EPBBUT4Q2Fg)

## TensorFlow Keras Google IO 2023 Highlights
- **KerasCV, KerasNLP:** New additions to the TensorFlow ecosystem provide pretrained state-of-the-art models for computer vision and natural language processing. They offer cutting-edge backbones with line code, fully integrated into the TensorFlow ecosystem.
- **DTensor (Distributed Tensor):** This toolkit enables scaling ML models unparalleled by providing a simple way to build models that can be distributed across multiple devices. It supports both data parallelism and model parallelism, making it easy for developers to scale their models without worrying about rewriting code or using different parallelism strategies.
- **JAX2TF (JAX to TensorFlow):** This feature allows researchers and developers to leverage the powerful JAX framework within the diverse TensorFlow ecosystem. With JAX2TF, users can deploy JAX models on servers using TF Serving or devices using TF Lite, fine-tune pretrained JAX models with TensorFlow, and even fuse additional layers and components into a single model.
- **Quantization API:** This upcoming feature will enable making models more efficient and easier to deploy across various devices by reducing the size of the model while maintaining accuracy. It offers flexibility in quantizing at different levels such as per layer, operation, or tensor, ensuring optimal performance for each use case.


## Easy on-device Machine Learning with MediaPipe

URL: [https://www.youtube.com/watch?v=yOP_FY2KTm8](https://www.youtube.com/watch?v=yOP_FY2KTm8)

    - Introduction to MediaPipe
        - On-device machine learning allows running machine learning models on user devices without sending user data to servers.
        - Google Pixel's Magic Eraser and Live Caption features are examples of on-device machine learning.
        - Developing on-device machine learning solutions can be complex, so MediaPipe abstracts the complexity with low-code/no-code tools for easy integration into mobile, web, and IoT apps.
    - MediaPipe's 14 On-Device Machine Learning Solutions
        - Each solution contains a TensorFlow Lite model and is designed to chain together in a pipeline.
        - The user only needs to interact with simple APIs to fit input images into the system and get predictions.
    - Integrating MediaPipe Solutions into Apps
        - To integrate hand gesture recognition, developers need to use the MediaPipe Tasks library and call specific functions.
        - Support for iOS is coming soon.
    - Platforms Supported by MediaPipe
        - Currently supported platforms include Android, web, and Python.
    - Demonstration of MediaPipe's Solutions in Action
        - MediaPipe Studio allows users to try on-device machine learning solutions directly within their web browsers.
    - Customizing MediaPipe Solutions with Data Sets
        - Developers can use the MediaPipe Model Maker Python library to customize existing MediaPipe solutions or create new ones by training custom models using their own data sets.
    - Future Directions for On-Device Machine Learning with MediaPine
        - MediaPipe is exploring techniques like model distillation and diffusion-based image generation to enable on-device generative AI applications.
        - They are also working on deploying larger language models on devices for tasks such as natural language summarization and email writing.


## Applied ML with KerasCV and KerasNLP

URL: [https://www.youtube.com/watch?v=K2PKZS1fPlY](https://www.youtube.com/watch?v=K2PKZS1fPlY)

- Introducing KerasCV and KerasNLP: two easy-to-use, modular libraries for computer vision (KerasCV) and natural language processing (KerasNLP).
- KerasCV features include a unified NLP modeling API, support for image classification tasks, object detection, data augmentation, and integration with the TensorFlow ecosystem.
- KerasNLP offers text sentiment analysis, text generation using GPT2/OPT models, and customizable transformer-based classifiers.
- Both libraries provide high-level APIs for building state-of-the-art models quickly while offering flexibility to build custom workflows and models from scratch.
- KerasCV and KerasNLP are part of the TensorFlow ecosystem and support TF Lite, XLA compilation, TPU accelerators, and DTensor multi-accelerator inference.


## Simple ML for Sheets: ML without coding

URL: [https://www.youtube.com/watch?v=GYCORMNr_IU](https://www.youtube.com/watch?v=GYCORMNr_IU)

- Introduced Simple ML Sheets, a tool that allows users to use machine learning without coding or knowledge of machine learning.
- Demonstrated two main features: predicting missing values and forecasting future values.
- Explained how the tool works by using examples in Google Sheets.
- Emphasized that the tool is easy to use, with most tasks requiring just a few clicks.
- Discussed the benefits of using Simple ML Sheets for decision making and planning.
- Mentioned that the tool can also be used for training models, evaluating them, and exporting them to other platforms like TensorFlow.
- Highlighted that data remains on the user's computer when using Simple ML Sheets.
- Encouraged users to try out the tool by visiting their website and starting with tutorials.


## Safe and responsible development with generative language models

URL: [https://www.youtube.com/watch?v=oAc0ZhbCfi8](https://www.youtube.com/watch?v=oAc0ZhbCfi8)

- Thi and Shivani discuss responsible AI and safe product development using large language models (LLMs)
- They mention the need for developers to be aware of safety responsibility when building applications with LLMs, as it differs from traditional software development.
- They talk about the importance of understanding risk in use cases and how it can impact user experience negatively or even harmfully.
- The seven Google AI principles that guide product development are discussed: Be socially beneficial, avoid creating or reinforcing unfair bias, be built and tested for safety, be accountable to people, incorporate privacy design principles, uphold high standards of scientific excellence, and be made available for uses that accord with these principles.
- They emphasize the importance of building a foundation model that is responsibly developed, including using diverse data sets, incorporating safeguards into the model, and thoroughly testing and evaluating it.
- They also discuss additional steps developers can take to measure and mitigate specific harms in their applications, such as detecting potential harm through input/output detection systems and tuning models for specific product needs.
- Finally, they encourage continuous iteration and testing throughout the development process to ensure that the final product meets its goals while adhering to responsible AI principles.


## How to scale Large Language Models on Google Cloud

URL: [https://www.youtube.com/watch?v=t74WVC6L5wU](https://www.youtube.com/watch?v=t74WVC6L5wU)

- Intro to Generative AI Studio and Model Garden
    - Google Cloud's Vertex AI offers a unified platform for machine learning workflows, including generative AI models.
    - Model Garden provides access to Google's latest foundation models like PaLM, while the Generative AI Studio simplifies experimenting with these models.
- Prototype Application Using Large Language Models
    - The Generative AI Studio allows developers to quickly prototype and customize large language models for a variety of use cases.
    - Prompt design is crucial in generative AI, as it involves a lot of experimentation to find the best prompt for a given task.
- Working with Large Language Models in Google Cloud
    - In the Generative AI Studio, developers can write prompts and interact with large language models directly through the user interface.
    - The platform supports both zero-shot and few-shot learning scenarios, allowing developers to provide examples or instructions for the model to follow.
- Customizing Large Language Models in Google Cloud
    - Generative AI Studio allows users to customize their own large language models by providing a structured prompt template that includes context, word restrictions, topic focus, and output format requirements.
    - Users can also specify parameters such as temperature, topP, and topK to control the randomness of the model's responses and adjust its behavior accordingly.
- Integrating Large Language Models into Applications
    - Once a large language model has been prototyped in the Generative AI Studio, it can be integrated directly into Cloud applications using APIs provided by Vertex AI.
    - Developers can use common Google Cloud components like Google Cloud Storage and BigQuery to store data, run processing workflows, and update databases automatically whenever new summaries are generated.
- Scaling Generative AI Applications with Google Cloud
    - Google Cloud's large language models are designed for enterprise-level applications, offering SLAs and scalability features that make them suitable for production environments.
    - By using the Vertex AI platform, developers can easily experiment with different prompts and parameters to optimize their large language models for specific use cases.


## How developers can get started with AI and ML

URL: [https://www.youtube.com/watch?v=3K1414RwNDU](https://www.youtube.com/watch?v=3K1414RwNDU)

- Defining AI: Artificial intelligence (AI) is a concept that refers to programming computers to react to data in an intelligent, human-like manner.
- Machine Learning: Machine learning (ML) is a technique used to help computers understand the content of data. It involves showing the computer many examples and allowing it to learn how to classify new data on its own.
- Data Processing: Before training a machine learning model, developers often need to process, cleanse, and filter the data to make it more useful for the subsequent steps.
- Model Creation: Developers use high-level frameworks like TensorFlow and Keras to create and train their models. These tools allow them to define the architecture of their model without needing a deep understanding of the underlying mathematical theories.
- Deployment: Once a model has been trained, it needs to be deployed so that people can use it. This often involves integrating the model with existing systems and handling data interfacing between different platforms.
- Ongoing Improvement: After deploying a model, developers must continually gather data and work on improving the model based on user feedback. This process is called Operations (Ops) or MLOps in the context of machine learning.
- Accelerated Infrastructure: Advanced hardware like GPUs and TPUs can speed up the training and deployment of machine learning models by making them more efficient.
- Responsible AI: Developers must also consider the ethical implications of their work when creating AI systems, as these technologies have the potential to impact society in significant ways.


## On-device Large Language Models with Keras and Android

URL: [https://www.youtube.com/watch?v=pNWNMPi0Mvk](https://www.youtube.com/watch?v=pNWNMPi0Mvk)

- Introduction to Large Language Models (LLMs)
- LLM Architecture and Training Overview
- Importance of KerasNLP for NLP Tasks
- Loading and Finetuning GPT2 Model with KerasNLP
- Generating Text Using GPT2 Model
- Converting GPT2 Model to TensorFlow Lite Format
- Quantizing the TensorFlow Lite Model for Size Reduction
- Android Studio Setup and Deploying LLM on Android Device
- Introduction to Google's Colab Platform
- Building a Visual Blocks Machine Learning Pipeline using TFLite Model
- Responsible AI Considerations when Using LLMs in Production Environments


## Supercharge your web app with Machine Learning and MediaPipe

URL: [https://www.youtube.com/watch?v=axBwpKnVr4M](https://www.youtube.com/watch?v=axBwpKnVr4M)

- Introduction to custom ML solutions using MediaPipe
- Overview of different tasks available in MediaPipe such as image classification, gesture recognition, and text classification
- Importance of using MediaPipe for web applications
- Overview of the codelab project: building an object detector that detects the presence of a dog photo or video
- Steps to follow in the codelab project
    - Setting up the starter app on CodePen
    - Adding the necessary code snippets from the MediaPipe library using Skypack CDN import package
    - Initializing the object detector and setting its running mode (image or video)
    - Handling clicks to enable webcam detection and run inference with the provided image
    - Displaying the detected objects on the screen
- Switching from the default EfficientDetLite model to a custom dog detection model using MediaPipe Model Maker
    - Downloading the required dataset for training the custom model
    - Setting up the training options and starting the training process
    - Evaluating the performance of the trained model
    - Exporting the trained model and uploading it to Google Cloud Storage
- Testing the web app with the custom dog detection model using MediaPipe Studio
- Conclusion: encouraging developers to explore further possibilities with MediaPipe Tasks, especially in building custom object detection models using JavaScript frameworks like TensorFlowjs.


## Hands-on Responsible AI with the People and AI Guidebook

URL: [https://www.youtube.com/watch?v=wsW9-jjY5Z0](https://www.youtube.com/watch?v=wsW9-jjY5Z0)

- Introduced MakerSuite, a tool that allows rapid prototyping of LLM-based AI systems.
- Discussed the PAIR Guidebook, which provides best practices and methodologies for designing responsible AI systems.
- Showcased an example of using MakerSuite to generate stories based on prompts.
- Emphasized the importance of responsive design in AI systems, including the ability to handle a wide variety of inputs.
- Demonstrated how to use MakerSuite's "test input" feature to prototype different user scenarios and test system performance.
- Explained the concept of "scalability" in LLM models, which allows for easy customization of output based on user prompts.
- Introduced the PAIR Guidebook's User Needs chapter, which provides guidance on designing AI systems that meet real human needs.
- Demonstrated how to use MakerSuite to rewrite and edit text based on user prompts.
- Discussed the importance of evaluating AI systems using both usefulness and usability metrics.
- Highlighted the need for earning and maintaining user trust in AI systems, particularly those that generate content or make decisions.
- Encouraged attendees to try out open-source editors created using MakerSuite and other Google tools.


## What's new with Web ML in 2023

URL: [https://www.youtube.com/watch?v=r7hOoCY6uGo](https://www.youtube.com/watch?v=r7hOoCY6uGo)

- Jason Mayes, Web ML Lead at Google, presents on the growth and future of Web ML.
  - Web ML usage has grown exponentially in the past three years.
  - TensorFlow GS NPM downloads have increased by over ten times since 2020.
  - The number of cumulative downloads is now at 50 million.
- Adobe plans to use Web ML in their products, such as Photoshop.
  - This will enable users to access the smart selection tool directly within their browser.
  - This feature will allow for faster prediction and responsive user interfaces.
- The future of Web ML includes:
  - Increased adoption of machine learning technologies across industries.
  - Continued growth in the number of developers using JavaScript for building AI applications.
  - Improved performance through optimizations like those made to TensorFlowjs's WebGL backend.
  - Support for new web standards, such as Web GPU.
- Visual Blocks ML is a new open-source framework that allows developers to create low-code and no-code machine learning applications within their product ecosystem.
  - It enables users to drag and drop input/output nodes compatible with various models.
  - This tool can be used to create custom graphs and perform desired tasks in real-time.
- MediaPipe Studio provides a solution for hand gesture recognition that runs entirely within the browser.
  - Users can integrate this solution into their web apps by simply adding code snippets.
  - The platform allows for customization via transfer learning.
- A new ML solution called Face Blend Shape Classification has been announced, which could be used to create virtual avatars that match a user's facial expressions in real-time.
- TensorFlowjs now supports the latest feature variants and offers varied performance levels depending on the selected backend (WebGL vs WebAssembly).
  - This allows users to choose the best performing backend based on their device capabilities.
- Chrome is adding support for Web GPU, which brings powerful graphics processing capabilities to web applications.
  - This will enable developers to run larger and more complex models directly within the browser.
- A visual debugger has been released to help users compare output operations across different TensorFlowjs backends (WebGL vs WebAssembly).
  - This tool allows users to find potential bugs and performance issues by comparing results from different environments and devices.
- The latest version of TensorFlowjs now offers up to 159 times faster performance compared to previous versions due to significant optimizations made at the individual operation level.
- A new AI-powered feature called "Near Exercise" has been demonstrated, which uses machine learning models to generate an animated dance partner based on user movements captured by a camera.
- RoboFlow is being used by Cardinal Health to improve backroom pharmacy operations by adding pill counting features to existing video feeds.
  - This enables pharmacists to remotely manage multiple locations more efficiently.
- The "Yoha" hand tracking engine has been introduced, which can detect hand movements in real-time and generate slight variations in output based on user input.
- A new visual workflow engine called "Mariel Pettee's Workflow Engine" allows users to build complex solutions quickly by dragging and dropping different components into place.
  - This tool enables developers to define the type of process they want without needing coding skills.
- The future of Web ML will involve further expansion into areas such as healthcare, where machine learning models can be used directly within web browsers to provide personalized care plans for patients.
- Finally, Jason Mayes emphasizes that anyone can now build powerful AI applications using just JavaScript and a web browser, thanks to the growth and development of Web ML technology.


## Level up your Smart Home integration with Matter

URL: [https://www.youtube.com/watch?v=FI0V1YiGx3M](https://www.youtube.com/watch?v=FI0V1YiGx3M)

- Introduction to the session
    - Saraj Mudigonda, a Nest Partner Engineering solution consultant at Google, will lead the session.
    - The talk will cover end-to-end developer journey building Matter integration for Google Home.
    
- Cloud Matter Integration Overview
    - Matter is an IoT connectivity standard that enables seamless communication between devices from different manufacturers.
    - The development process involves creating, launching, and monitoring a comprehensive analytics suite.
    
- Creation of the Project in Google Home Developer Console
    - Developers can create a new cloud project or import an existing one into the Google Home Developer Console.
    - Device dual stack support allows for both Matter and Cloud Matter integration.
    
- Matter Development with Yeet Tool
    - Yeet is a comprehensive suite of tools designed to improve developer productivity and quality.
    - The tool offers a fast path to building Matter products by providing a step-by-step guide to creating, launching, and testing Matter integrations.
    
- Matter Integration Testing and Certification
    - Developers must implement Matter device deduplication to avoid duplication of devices in the Home Graph.
    - The test suite tool allows developers to run development tests and certification tests within the Google Home Developer Console.
    
- Launching and Managing Matter Integrations
    - Once a developer has completed their integration, they must submit it for certification through the Google Home Developer Center.
    - After passing the certification process, developers can launch their Matter integrations in the Google Home ecosystem.
    
- Level Project Quality Initiative
    - The initiative aims to improve the quality of smart home projects by providing detailed analytics dashboards and troubleshooting tools.
    - Developers can use the dashboard to monitor metrics such as camera quality, general success rate, latency, and error breakdown.
    
- Troubleshooting Matter Integrations
    - The Colab sample application demonstrates how developers can use WebRTC to implement camera streaming in their smart home projects.
    - Developers can use the Logs Explorer tool to troubleshoot issues by monitoring logs and identifying potential points of failure.
    
- Monitoring Over-the-Air Updates
    - The rollout process allows developers to monitor the progress of software updates for their devices.
    - By tracking metrics such as device field release, new update check number, and rollout progress, developers can ensure that their updates are being delivered successfully.
    
- Reaching Out for Help
    - Developers can seek help from various channels, including the smart home developer forum, Google issue tracker, and Stack Overflow.
    
- Conclusion
    - Saraj thanks everyone for joining the session and wishes them luck in their smart home development journey.


## How to optimize web responsiveness with Interaction to Next Paint

URL: [https://www.youtube.com/watch?v=KZ1kxzsJZ5g](https://www.youtube.com/watch?v=KZ1kxzsJZ5g)

- Introduction to Interaction Next Paint (INP)
    - Replaces First Input Delay as Core Web Vital
    - Measures time from tap/click/keypress to next update on screen
    - Captures user experience better than FID
- Demonstration of INP using a simple app
    - Original implementation: slow and sluggish due to long-running tasks blocking UI updates
    - Optimized version: uses async functions, debouncing, and efficient state management for improved responsiveness and INP scores
- Case studies on improving INP in real-world websites
    - The Economic Times: reduced total blocking time, optimized DOM size, and used request title callbacks to improve interaction times and business metrics
    - Redbus: added real user monitoring, removed unnecessary scripts/styles, and optimized API calls to cut INP by half and increase conversion rates


## The 9 most effective Core Web Vitals opportunities of 2023

URL: [https://www.youtube.com/watch?v=mdB-J6BRReo](https://www.youtube.com/watch?v=mdB-J6BRReo)

- Introduction: Barry Pollard talks about top Core Web Vitals optimization strategies for 2023.
- Largest Contentful Paint (LCP):
    - Make image source discoverable by using static HTML.
    - Prioritize downloading of LCP images using the Fetch Priority API.
- Cumulative Layout Shift (CLS):
    - Ensure content is explicitly sized initially rendered.
    - Use aspect ratio property or minheight to ensure responsiveness.
    - Utilize Backward and Forward Cache (bfcache) for faster page loads.
- Responsiveness:
    - Identify and break long tasks using Chrome DevTools and Lighthouse.
    - Code splitting to load JavaScript code later when needed.
    - Regularly audit tag managers for unnecessary scripts.
    - Use CSS containment to separate unaffected areas of the webpage from rendering updates.
- Conclusion: Barry encourages listeners to consider these recommendations and tools to improve website performance and user experience.


## What's new in Web on Android -  updates to WebView, Custom Tabs, and more

URL: [https://www.youtube.com/watch?v=sLn3wszcnGU](https://www.youtube.com/watch?v=sLn3wszcnGU)

## Summary of Web Android Conference Talk
- Latest updates to Android's WebView tool improve privacy, better support large screens, and enhance custom tab and PWA features.
- Privacy improvements include deprecating the use of the XRequestedWith header in requests sent from WebView, which could leak user identity and app online service data.
- Large screen support includes updates for image drag and drop functionality in split-screen view mode.
- Custom tab enhancements allow developers to control height and create a more customized multitasking experience.
- PWA (Progressive Web Apps) features include expanded access installation, richer install UI, and integration with the Play Billing API for managing digital goods sales and subscriptions.
- Trusted Web Activity (TWA) allows developers to wrap first-party web content within another application, providing a full-screen browsing experience with URL bar display.


## What's new in Chrome Extensions

URL: [https://www.youtube.com/watch?v=QYd2XiUYNlE](https://www.youtube.com/watch?v=QYd2XiUYNlE)

- Intro: Chrome Product Manager, David Li, gave a brief overview of the recent changes in Chrome Extensions.
- Education Extension: Examples include Read&Write for reading and writing assistance and Honey for finding coupon codes and offering rewards.
- Pinterest Save Button: This extension allows users to save images and ideas from the web onto their Pinterest boards.
- Platform Changes: Last year, Google invested heavily in adapting the Chrome extension platform to meet changing user expectations and privacy standards.
- Manifest V3: The next version of the extension platform is aimed at improving performance and better protecting sensitive user data. It also addresses issues with extension access to user data.
- Extension Service Worker: This feature replaces the background page, improves performance, and allows for better resource management.
- DeclarativeNetRequest API: The new version of this API supports many use cases and helps improve performance by allowing extensions to block network requests without needing access to sensitive user data.
- Deprecation Timeline: Chrome has announced a timeline for deprecating Manifest V2, with a commitment to address platform gaps, close critical bugs, and gather developer feedback before making any changes.
- Privacy Controls: New controls are being introduced to give users more transparency and control over the data they share with extensions.
- Side Panel API: This new feature allows extensions to display contextual information directly alongside a webpage. Extensions must request side panel permission in their manifest file to use this functionality.
- Future Improvements: Google is working on redesigning the Chrome Web Store, which will include enhancements like personalized recommendations and a broader user journey. The company also plans to introduce powerful new APIs that will allow extension developers to create more customizable experiences for users.


## Learn passkeys for simpler and safer sign-in

URL: [https://www.youtube.com/watch?v=SF8ueIn2Nlc](https://www.youtube.com/watch?v=SF8ueIn2Nlc)

- Introduction to Passkeys
    - Passkeys are a simpler, much safer authentication method that replaces passwords and second factors.
    - They work across devices and browsers such as Mac OS, iOS, iPadOS, Windows, Android, Chrome, Edge, and Safari.
    - Passkeys protect users from phishing attacks.
- Benefits of Passkeys
    - Users can sign in using their device's screen lock or biometric authentication (e.g., fingerprint or facial recognition).
    - Creating and remembering unique complex passwords is a big pain for many users, but passkeys make this process simpler with just one touch.
- How Passkeys Work
    - Developers store public keys instead of passwords, making it harder for bad actors to obtain sensitive information.
    - Passkeys are compatible with web apps and can be used on ecommerce websites like Shopify.
- User Journey Example: Shopping using a passkey in the Shop app
    - Users set up fingerprint unlock and upgrade their password to a passkey.
    - When shopping online, users sign in using their saved passkey without needing to enter an email address or password.
- Cross-device Synchronization
    - Passkeys are automatically synchronized across devices via password managers like Google Password Manager.
- Implementing Passkeys
    - Web developers can use the standard API called Web Authentication (WebAuthn) for web apps.
    - For Android apps, developers should use the Credential Manager library and associate their app with a web domain using JSON files.
- Passkey vs. Traditional Authentication Methods
    - Passkeys are more secure than traditional password-based authentication methods because they use public key cryptography to generate signatures that verify user identities.
    - Two-factor authentication (2FA) is another alternative, but it's less secure than passkeys and can be inconvenient for users who need to enter codes sent via SMS or email.
    - Identity federation allows websites to delegate authentication-related complexity and security challenges to identity providers like Google or PayPal. This method is complementary to passkeys and doesn't require third-party cookies, unlike traditional methods.
- Future of Passkeys
    - As more developers adopt passkey technology, users will experience a simpler, safer world without passwords. The speaker encourages everyone to start integrating authentication systems soon.


## Preparing for the end of third-party cookies

URL: [https://www.youtube.com/watch?v=gm8O8-b2B8c](https://www.youtube.com/watch?v=gm8O8-b2B8c)

- Helen Cho and Kaustubha Govind are the speakers at a conference discussing Privacy Sandbox.
- Chrome will soon deprecate support for third-party cookies, and developers must prepare their websites accordingly.
- The Privacy Sandbox suite proposal aims to address the issue of pervasive tracking on the web.
- Tools like Web HTTP Cache Partitioning, User Agent String Reduction, Network State Partitioning, and Storage Partitioning have already been implemented to help with this transition.
- The Chromium team has proposed CHIPS (Cookies Having Independent Partition State) as an alternative to third-party cookies.
- FirstParty Sets is another proposal that helps facilitate cookie access support for user-impacting functionality across a site's first-party domain.
- Developers can use the Storage Access API to request storage access for specific domains within their sets.
- Users will be able to control their privacy settings in Chrome, allowing them to toggle access to FirstParty Sets and see which websites are part of their visited set.


## Getting started with Attribution Reporting

URL: [https://www.youtube.com/watch?v=mYVi3yL-GNI](https://www.youtube.com/watch?v=mYVi3yL-GNI)

- Introduction to Attribution Reporting API in Privacy Sandbox
    - Jolyn Yao, Georgia Franklin, and Erin Walsh introduce themselves and their roles in the development of the Privacy Sandbox.
- Vision and goals of Privacy Sandbox
    - The vision involves rethinking building together for privacy on Chrome and Android platforms.
    - Key use cases are supported by purpose-built technology, transparent, open collaboration, and consulting stakeholders across industries to ensure everyone's vision is realized.
- Three main advertising-focused APIs developed for web and Android platforms
    - The three main advertising-focused APIs are: 1) Attribution Reporting API, 2) FLEDGE (Federated Learning of Cohorts), and 3) TURTLEDOVE.
- Core measurement goal aligned with privacy
    - The core measurement goal is to enable conversion measurement without needing data on individual users. Instead, it focuses on large patterns and robust outliers to provide business decision-making insights.
- Platform support for identifier tracking
    - Platforms like Chrome and Android previously used identifiers such as third-party cookies and device IDs to track user behavior across websites and apps. However, these methods have been criticized for privacy concerns.
- Purpose-built platform APIs like Attribution Reporting API
    - The Privacy Sandbox supports purpose-built platform APIs like the Attribution Reporting API as a privacy-preserving alternative to legacy technology such as third-party cookies and device IDs.
- Workflow of Attribution Reporting API
    - When an ad is clicked, the advertiser registers the event using the Attribution Reporting API. If the user later engages with the advertiser (e.g., makes a purchase), the advertiser registers this conversion event. The API sends an event-level summary report with randomized delay and structural limits to prevent tracking of individual users.
    - Two types of reports are generated: 1) Event-level reporting, which provides high-fidelity line-item data for each ad event that occurs; and 2) Summary reporting, which is used for higher-level insights such as ROI analysis and campaign performance reporting.
- Privacy protection measures in Attribution Reporting API
    - The privacy protection measures implemented in the Attribution Reporting API include: 1) Structural limits to prevent the identification of individual users; 2) Noise rate limit applied to prevent the use of limited data for tracking purposes; and 3) Reporting windows, which delay sending reports on data older than 30 days.
- Summary report generation process
    - The summary report is generated by aggregating data from multiple sources and adding noise to prevent tracking of individual users. The aggregated data is then processed in a Trusted Execution Environment (TEE) before being returned to the advertiser as a summarized value.
- Cross-web app attribution support in Privacy Sandbox
    - The Privacy Sandbox natively supports cross-web app attribution, allowing advertisers to track conversions across both web and app environments on Chrome and Android platforms.
- Developer enrollment process for Privacy Sandbox APIs
    - Developers must enroll themselves using a new process that includes verifying their identity, providing contact details, and inputting necessary API server configuration information. This process helps ensure that developers are properly authorized to use the Privacy Sandbox APIs and limits the risk of misuse or improper usage beyond intended purposes.



## Web experiences that drive business growth

URL: [https://www.youtube.com/watch?v=-d3grIUVwCU](https://www.youtube.com/watch?v=-d3grIUVwCU)

## Summary of the transcript

- Cocos Creator leverages WebAssembly and WebGPU to enable game developers build online games with improved cross-platform rendering and advanced graphics features.
- Notion, a productivity tool, uses Project Fugu APIs like Async Clipboard and Persistent Storage to deliver an engaging and easy-to-use web experience that has led to increased user growth and engagement.
- Tencent Docs, an online documentation tool, utilizes the Async Clipboard API to provide near instantaneous pasting capabilities, leading to higher user engagement and session duration.
- Figma, a design platform, uses WebAssembly to improve load times and enhance its web-first solution, which has contributed to increased engagement and growth in its user base.
- LEGO Education's First Spike app leverages Project Fugu APIs like Web Bluetooth and Web Serial to enable students to write code, connect it with LEGO bricks, and create interactive projects.
- Snapchat Web allows users to stay connected across devices and leverage WebAssembly for faster time-to-market, increased session times, and new user acquisition through augmented reality experiences.


## WebAssembly: A new development paradigm for the web

URL: [https://www.youtube.com/watch?v=RcHER-3gFXI](https://www.youtube.com/watch?v=RcHER-3gFXI)

- WebAssembly: A low-level, binary format that enables maximized performance and portability.
- Four main advantages of WebAssembly: 1) Offers reliable, maximized performance. 2) Enables great portability since it compiles languages into a single language. 3) Provides greater flexibility for developers who write web languages like JavaScript. 4) Allows for efficient and cost-effective development by reusing code across deployment.
- C: One of the earliest areas where WebAssembly has been transformative, enabling large applications to be ported from platform to platform.
- Swift: A language that can now be compiled into WebAssembly, allowing developers to bring their iOS applications to non-iOS users and minimize maintenance costs.
- Rust: Another language that has been adapted for use with WebAssembly, offering better support for functional programming languages and providing C programmers with coroutines.
- Kotlin and Dart: Two new languages being introduced by Google as part of their plan to bring new languages like Java and Kotlin to the web.
- WebAssembly GC (Garbage Collector): A new extension that allows for efficient memory management in WebAssembly, reducing the need for developers to manually free up object heap memory.
- Flutter: An open-source framework developed by Google for building cross-platform applications. With the introduction of WebAssembly support, developers can now compile Dart code directly into fast, efficient WebAssembly code that runs in browsers.


## Partnering for a stable web

URL: [https://www.youtube.com/watch?v=eZa3BgGaAeA](https://www.youtube.com/watch?v=eZa3BgGaAeA)

- Rachel Andrew, Chrome Web Developer Relations, discussed the importance of keeping track of new features on the web platform and ensuring they work across different browsers.
- She highlighted that with evergreen browsers, new features are being introduced at a rapid pace, making it challenging for developers to keep up.
- However, she also noted that these features are becoming interoperable much more quickly than before, meaning they can be used consistently across different browser engines.
- Andrew emphasized the need for clear communication and guidance on browser support for new web platform features.
- She mentioned that Chrome has a "What's New" series of posts to help developers stay informed about new features and their compatibility across browsers.
- The Interop Project, which involves collaboration between major browser vendors, aims to fix issues and ensure better interoperability of new features.
- Andrew also discussed the concept of a "Baseline", which is a set of widely available web platform features that are considered safe to use without needing extensive research or testing.
- The Baseline helps developers understand which features are supported across different browsers, making it easier for them to build consistent and reliable web applications.
- Finally, Andrew highlighted the importance of working together with browser vendors and other stakeholders to ensure that new web platform features become part of the Baseline as quickly as possible.


## What's new in web UI

URL: [https://www.youtube.com/watch?v=buChHSdsF9A](https://www.youtube.com/watch?v=buChHSdsF9A)

- New responsive UI capability: Build dynamic, responsive interfaces with platform support for customization and personalization.
- Container query: A new feature that allows developers to style elements based on the size of their parent element, making it easier to create layouts that adapt to different screen sizes.
- Styled container queries: Use CSS custom properties to apply styles to a container based on its size or the presence of specific child elements.
- Logical CSS: A new feature that allows developers to style elements based on their logical position within a document, rather than just their physical position.
- Advanced nth-child selector: A powerful new CSS capability that enables developers to select and style elements based on their position within their parent element.
- Textwrap balance: A new property that balances the length of text elements by adjusting their font size.
- Initial letter: A new CSS property that allows developers to specify a specific letter at the beginning of an element, providing more control over typography.
- Dynamic viewport units: New units for specifying viewport dimensions that adapt to changes in the user's device orientation or the presence of browser chrome.
- Widegamut color space: A new color space that provides a wider range of colors than traditional sRGB, allowing developers to create more vibrant and accurate color schemes.
- Color mix: A new feature that allows developers to blend two colors together to create a third color with adjustable opacity.
- Venn diagram icon: A new feature in DevTools that helps developers visualize the intersection of different CSS properties and values.
- Cascade layer: A new feature that gives developers more control over the order in which CSS styles are applied, allowing for more precise styling.
- Scope: A new CSS property that allows developers to create namespaced stylesheets, preventing style conflicts between different parts of a web application.
- Trigonometric functions: New CSS math functions that allow developers to create complex layouts using sine, cosine, and tangent functions.
- Individual transform functions: Separate subfunctions for scaling, rotating, and translating elements within a CSS transformation, allowing for more precise control over element positioning and appearance.
- Popover attribute: A new HTML attribute that allows developers to create popup menus that can be opened and closed with a single click, without needing to write any JavaScript code.
- Fallback positions: New features in the popover API that allow developers to specify alternative positions for popup menus when they cannot be placed at their preferred location.
- Scroll-driven animation: A new feature that allows developers to create animations that are triggered by scrolling, providing a more interactive and engaging user experience.
- Vue transition: A new feature in Vue that makes it easier for developers to create smooth transitions between different pages or elements within a web application, without needing to write any additional JavaScript code.


## What's new in web animations

URL: [https://www.youtube.com/watch?v=oDcb3fvtETs](https://www.youtube.com/watch?v=oDcb3fvtETs)

- Introduced new web animation features in Chrome
- View Transitions: A full feature within the CSS working group, allows developers to create a single page application with smooth transitions between different states.
- Page transitions are now supported by customizable CSS.
- The demo provided showcases the ability of the new API to transition parts of a page independently while other parts remain static.
- View Transitions can be used alongside popular web frameworks like React, Vue, and Svelte.
- A new feature called "scoped transitions" is being explored, which will allow developers to create separate animations for different components within a page.
- The demo also showcases the use of the CSS customize transition set pseudoelements, allowing developers to animate independently.
- The View Transitions feature is currently available in Chrome Canary and is expected to be stable soon.
- A new scroll-driven animation has been introduced, which allows developers to control play ahead animations based on scroll position and container size.
- The demo provided shows a progress bar that adjusts as the user scrolls down the page.
- This feature does not require any JavaScript code and is entirely powered by HTML, CSS, and Web Animations API.
- A new tool has been built to help developers create complex easing functions using simple input parameters.
- The individual transform property feature has been introduced in Chrome 104, allowing developers to easily manipulate parts of a transform animation without needing to duplicate the entire transformation function.


## How to create personalized web experiences

URL: [https://www.youtube.com/watch?v=JiVQBqAkkac](https://www.youtube.com/watch?v=JiVQBqAkkac)

- Intro: Adam Argyle introduces himself and talks about creating personalized web experiences.
- Moment 1: Font preference tailoring - User can set font size preferences in their browser, which are supported by CSS relative units. This helps create a unique reading experience for users.
- Moment 2: Relative unit introduction - Three new relative units are introduced: rem (root em), rlh (root line height), and rex (root x height). These units help developers work with user preferences more effectively.
- Moment 3: Read Anything sidebar page view mode - A new feature that allows users to customize their font color, similar to an eReader. This is perfect for those who consume content in a specific way.
- Moment 4: OS color scheme preference matching - Users can set their preferred color schemes on their operating systems, which can be matched by websites using CSS accent color features.
- Moment 5: Reduced motion tailoring - Websites can detect if users prefer reduced motion and adjust animations accordingly to prevent discomfort or distraction.
- Moment 6: Device capability detection - Tailor experiences based on device capabilities such as hover elements, high definition color displays, etc.
- Moment 7: Language preference tailoring - Websites can detect user language preferences and automatically load content in that language.
- Moment 8: Keyboard accessibility - Allowing users to complete tasks using keyboard input instead of a mouse or touchscreen.
- Moment 9: Micro tailored UX moments - Container queries allow developers to adapt smaller sections of a page based on user preferences, device capabilities, and layout styles. This leads to a more personalized experience for the user.


## WebDriver BiDi: Future of browser automation

URL: [https://www.youtube.com/watch?v=6oXic6dcn9w](https://www.youtube.com/watch?v=6oXic6dcn9w)

- Jecelyn Yeen introduces the concept of WebDriver BiDi, a new standard browser automation protocol.
- She explains the history and evolution of web automation tools like Selenium, Cypress, Puppeteer, and Nightwatch.
- The presentation compares two major categories of automation: those that execute JavaScript within the browser (e.g., Cypress) and those that issue remote commands to control the browser via protocols such as WebDriver Classic and Chrome DevTools Protocol (CDP).
- She highlights the strengths and weaknesses of each approach, emphasizing how CDP is faster and more powerful than WebDriver Classic but lacks some features.
- The presentation then introduces WebDriver BiDi, a new protocol that combines the best aspects of both approaches while offering even greater flexibility and control over browser automation.
- Jecelyn Yeen discusses the collaborative development process behind WebDriver BiDi, which involves input from various stakeholders including browser vendors and open source projects.
- She demonstrates how to use WebDriver BiDi with tools like Selenium WebDriver IO and Puppeteer by adding support for the new protocol incrementally.
- The presentation concludes with an overview of future improvements in automation, such as Chrome's improved headless mode and the Recorder panel in Chrome DevTools.


## What’s new in Angular

URL: [https://www.youtube.com/watch?v=uqWUv0dpib0](https://www.youtube.com/watch?v=uqWUv0dpib0)

- Minko and Madleina introduce themselves as Developer Relations Product Lead and Engineering Manager for Angular, respectively.
- They discuss the philosophy behind developing and improving Angular, focusing on developer experience.
- The annual survey plays a significant role in understanding user feedback and prioritizing improvements.
- Two main areas of focus are simplifying the initial learning journey and addressing frequent sharp edges or complexities encountered by developers.
- Key features introduced in recent versions include standalone APIs, component upgrades, and improved developer tooling.
- The new reactivity system, Angular Signals, is designed to simplify code development and improve performance.
- Minko emphasizes the importance of backwards compatibility and interoperability across the Angular ecosystem.
- They discuss examples of large companies using Angular for their projects, such as Cisco and Naologic.
- The team has been working closely with Chrome DevTools to enhance stack traces and improve source maps.
- Jest is now available as an experimental option for unit testing in Angular version 16.
- They discuss the importance of server-side rendering (SSR) and its impact on Core Web Vitals.
- A new hydration strategy has been introduced to reduce flicker and improve Largest Contentful Paint (LCP).
- Partial Hydration is also being explored as a way to enable developers to load and render parts of their component tree.
- They mention the introduction of Host Directive, which allows for composing multiple components without needing inheritance.
- The team has made improvements to automatic import functionality, router configuration, and directives like image loading.
- Minko highlights the partnership with the Material Design Team and the upcoming support for Angular Material in version 16.


## Rethinking reactivity with Angular Signals

URL: [https://www.youtube.com/watch?v=EIF0g9LDHcQ](https://www.youtube.com/watch?v=EIF0g9LDHcQ)

- Reactivity in Angular:
  - Understanding reactivity as a declarative way to express propagation of change.
  - Example: A spreadsheet where the full name updates when either first or last name is updated.
  - Value states and app node graphs rely on update-based one another, with derived values depending on value signals.
  
- New in Angular V16:
  - Introduction of Signals for reactivity everywhere.
  - Three primitive components: signal, computed effect, and signal effect.
  - Signal value notifies reactive context when read, tracking dependencies and updating derived values accordingly.
  
- Cipher game example using Angular Signals:
  - A demo application where a secret message is decoded by dragging and dropping clues.
  - The app uses three signals: cipher, decodedCipher, and superSecretMessage.
  
- Computed value with Signals:
  - Deriving dependent values based on signal values.
  - Updating the derived value when a change in the dependent signal is detected.
  
- Future plans for Angular Signals:
  - Continued experimentation and feedback collection.
  - Interoperability support with other state management libraries.
  - Updates to component APIs, including input/output query support.


## Debugging modern web applications

URL: [https://www.youtube.com/watch?v=3lNkq264nkM](https://www.youtube.com/watch?v=3lNkq264nkM)

- Chrome DevTools: 15 years of evolution
- Modern tech stack change requirement debugging tool
- Interactive debugging technique used to make debugging easier
- Debugging with Chrome DevTools in modern web applications
    - Authored deployed view
        + Focus on code written
        + Ignore third-party scripts and libraries
    - Conditional breakpoints
        + Pause execution conditionally
        + Logpoints as noninvasive alternative to console.log
    - Recording and sharing debugging sessions
        + Chrome DevTools recorder for reproducing bugs
        + Export and import debugging session flows
- Future of Chrome DevTools: RFC process, community involvement, and bug fixes


## Introducing WebGPU: Unlocking modern GPU access for JavaScript

URL: [https://www.youtube.com/watch?v=m6T-Mq1BPXg](https://www.youtube.com/watch?v=m6T-Mq1BPXg)

- Corentin Walletz, WebGPU lead at Google, introduces the new WebGPU API for unlocking massive performance gains in graphic machine learning workloads.
- WebGPU is a successor to WebGL and has been developed in collaboration with Apple, Google, Mozilla, Microsoft, and Intel since 2017.
- The API brings advanced GPU programming possibilities to the web platform, reflecting modern GPU hardware and workloads.
- WebGPU is now available on Chrome 113 across Windows, macOS, and Chrome OS platforms, with support for other platforms like Android and Linux coming soon.
- A key feature of WebGPU is compute shaders, which enable new classes of algorithms to be ported from CPU to GPU, resulting in significant performance improvements.
- WebGPU also accelerates complex visual effects and practical applications such as BabylonJS, an open-source game engine library.
- In addition, WebGPU can significantly improve the efficiency of machine learning computations on GPUs by leveraging their massively parallel nature and offering flexible programming models.
- The API includes features like error handling mechanisms, lean draw loops, and new APIs such as render bundles that allow for recording large numbers of drawing commands in advance.
- WebGPU is designed to be easy to learn and use, relying on existing web platform features and patterns while minimizing boilerplate code.
- The ecosystem around WebGPU is already thriving, with many popular JavaScript/WebGL libraries embracing the new API, and support for additional hardware features like 16-bit floating point numbers in shaders is planned for future releases.


## Advanced Web APIs in real world apps

URL: [https://www.youtube.com/watch?v=Y40vMQap9fs](https://www.youtube.com/watch?v=Y40vMQap9fs)

- Thomas Steiner, Chrome Developer Relations Engineer, presented on advanced APIs and real-world apps at a conference.
- Project Fugu effort aims to close the web capability gap by enabling new applications to run on the web.
- Some exciting examples of applications using Project Fugu's Fugu APIs include:
  - Photopea: an online photo editor that uses file handling API for image editing.
  - Blockbench: a 3D model editor that utilizes eyedropper API to pick colors from the screen.
  - Construct 3: a game-making software that leverages file system access API for saving and loading games.
  - Boxy SVG: an SVG editing app that benefits from local font access API for text customization.
  - Adobe Photoshop: an image creation and photo editing software that uses origin private file system API for high-performance access to metadata files.
- The future of Project Fugu includes more amazing apps built using its APIs, with developers encouraged to check the Project Fugu API showcase for inspiration and join the collection of amazing patterns.


## Find form issues with Chrome DevTools

URL: [https://www.youtube.com/watch?v=Rb-LALxgsfY](https://www.youtube.com/watch?v=Rb-LALxgsfY)

- Autofill helps users avoid reentering data and increases data accuracy.
- Browsers interpret form code, provide an easy-to-use interface, and enable users to store autofill data.
- Chrome helps users by storing address, payment, and login information for different sites.
- Chrome offers suggestions in input fields related to address, credit card, and login data.
- To make Autofill work better, developers can use the Chrome DevTools Issues tab and the new Autofill debugging tool.
- The DevTools Issues tab highlights problems with form fields without IDs, name attributes, or autocomplete attributes.
- New features in Chrome DevTools help developers understand how Autofill works and find glitches that prevent Autofill from filling out form fields correctly.
- Developers can use the DevTools Issues tab to test new functionality and provide feedback on bugs and improvements.
- Accessibility is a priority for Chrome's Autofill feature, with features like improving the way Chrome TalkBack works and making forms more accessible for users with cognitive, motor, or visual impairments.
- Chrome's Autofill feature can be improved by using standardized values in form fields and avoiding nonstandard values that may break Autofill functionality.
- Label elements help browsers understand the meaning of form fields and assistive devices like screen readers.
- Forms should be designed with clear, logical units, and individual form tags should not be mixed together.
- Chrome TalkBack can handle valid addresses and passwords, making forms more accessible for users with visual impairments.


## How to implement passkeys with form autofill in a web app

URL: [https://www.youtube.com/watch?v=_qSCYiU_Yr4](https://www.youtube.com/watch?v=_qSCYiU_Yr4)

- Introduction to Workshop
- Implementing Passkeys in a Web App
    - Creating a basic web app with username and password fields
    - Adding the ability to create passkeys using WebAuthn API
        - Explanation of important parameters like userid, challenge ID, excludeCredentials array, authenticatorSel ection, platformAuthenticatorSel ection, etc.
        - Creating credential using navigator.credentials.create()
    - Register and manage passkeys
        - Displaying registered passkeys in a list
        - Adding a Create Passkey button to register new passkeys
        - Checking device support for passkeys
        - Implementing renderCredentials function to render the registered passkey list
- Authenticating with Passkeys
    - Using navigator.credentials.get() to authenticate user
        - Sending credential server endpoint to verify credential
    - Adding conditional UI option for browser autofill
        - Adding webauthn token attribute to username input field
- Conclusion and Next Steps


## Getting started with Angular Signals

URL: [https://www.youtube.com/watch?v=EEzDLpIbW9w](https://www.youtube.com/watch?v=EEzDLpIbW9w)

- Emma and Alex introduce Angular Signal, a new feature in Angular 16
- They explain the three reactive primitives: writable signal, computed signal, and effect.
- The speakers demonstrate how to build an app using these primitives by creating a cipher game
- The game involves decoding secret messages, dragging and dropping clues, and solving ciphers
- They emphasize that Angular Signal simplifies development and helps build faster apps
- The speakers also encourage attendees to try out the new feature and share their experiences on social media using a specific hashtag.


## Discover pre-trained models with Kaggle Models

URL: [https://www.youtube.com/watch?v=N53QMkCuwGY](https://www.youtube.com/watch?v=N53QMkCuwGY)

- Kaggle introduces Kaggle Models hub, a platform that allows users to discover and use pretrained machine learning models directly integrated with the Kaggle platform.
- The platform hosts over 2000 canonical pretrained models from Google Research TFHubdev, organized by tasks like text classification, object detection, etc., and can be filtered by data type (image, text, audio) or model implementation (TensorFlow, PyTorch, TensorFlow Lite).
- Users can access code examples and community discussions related to each model.
- By using pretrained models on the platform, users can save time, money, and resources while building their machine learning models.
- The demonstration showed how to find an open-source model, use it in a Kaggle competition submission, and understand the code notebooks provided for guidance.
- The talk also discussed transfer learning, which involves fine-tuning pretrained models on new data sets to improve prediction accuracy and adapt to new tasks or domains.


## Build more secure apps with Go and Google

URL: [https://www.youtube.com/watch?v=HSt6FhsPT8c](https://www.youtube.com/watch?v=HSt6FhsPT8c)

    - Cameron Balahan introduced the problem of software supply chain security and how it can be addressed using Go programming language.
    - The talk highlighted the complexity of dependency graphs and the need for robust tools to detect vulnerabilities early in the development process.
    - Julie Qiu discussed Go's dependency management system, Go module, and its integration with vulnerability management tools like Govulncheck and pkggodev.
    - Nicky Ringland emphasized the importance of understanding dependencies across multiple languages and introduced depsdev, a tool that helps manage and monitor open source software security.
    - The speakers also mentioned other Google tools such as OSSF Scorecard and OSV Scanner which aid in evaluating dependency health and identifying known vulnerabilities.


## Scalable UI testing solutions

URL: [https://www.youtube.com/watch?v=L6CSaH0kDnI](https://www.youtube.com/watch?v=L6CSaH0kDnI)

- Adarsh Fernando, Android Developer Tools Product Manager, discusses scaling automated UI testing for apps.
- New Espresso Device API allows synchronous configuration change testing in virtual devices running API level 24 and above.
- APIs support device rotation, folding, and unfolding screen changes.
- Firebase Test Lab can be used to scale continuous testing across multiple physical and virtual devices.
- Android Gradle Plugin 82 supports using Compose preview for host-side screenshot testing to catch potential UI regressions.


## Building with Firebase webframeworks

URL: [https://www.youtube.com/watch?v=YUwJqZLLjQ0](https://www.youtube.com/watch?v=YUwJqZLLjQ0)

- Introduction to Firebase Web Frameworks Codelab with Angular and Firebase
- Demonstration of a travel blog app built using the latest integration of AngularFire and Firebase
- Overview of key Firebase features: Authentication, Cloud Storage, Cloud Firestore, Cloud Functions, Hosting
- Step-by-step guide to setting up the project, including installing necessary packages and initializing Firebase project
- Explanation of the role of AngularFire in integrating the app with Firebase services
- Introducing Firebase Emulator Suite for testing changes locally without pushing everything to production
- Breakdown of how data is stored and retrieved from Cloud Firestore and Cloud Storage
- Discussion on security rules, indexes, and other useful features provided by Firebase
- Hands-on implementation of authentication using Google provider in the travel blog app
- Integration of Firestore database to store and retrieve travel data, including stops along the way
- Explanation of how nested collections within documents work in Firestore
- Walkthrough of implementing CRUD operations (Create, Read, Update, Delete) on the Firestore database using AngularFire services
- Introduction to the concept of storage and its role in uploading and downloading images in the travel blog app
- Implementation of an authguard service to redirect users based on their authentication status
- Deployment of the travel blog app using Firebase Hosting
- Conclusion and encouragement to explore more about Firebase Web Frameworks Codelab with Angular and Firebase.


## Add ML to your applications via simple database queries

URL: [https://www.youtube.com/watch?v=oGQgf4Q8tdI](https://www.youtube.com/watch?v=oGQgf4Q8tdI)

- Intro to using database for adding ML applications
    - Many developers want to use AI/ML to enhance their apps but struggle with operationalizing AI initiatives.
    - The traditional approach often results in a skill gap, significant cost, and complexity that leads to security issues.
    
- AlloyDB and BigQuery integration with Vertex AI
    - AlloyDB is a fully Postgres compatible database engine with superior performance, availability, and scalability. It can unify analytical workloads and generate real-time insights from data.
        + Native integration with Vertex AI allows high throughput low latency augmented transaction.
    - BigQuery is an industry-leading cloud data warehouse that processes 110 TB of data per second. It's serverless, meaning users don't worry about managing scale.
        + ML capabilities are built into the platform, removing the need to manage data infrastructure for machine learning.
    
- Use case: Fraud Detection using AlloyDB and Vertex AI
    - Users can train models in BigQuery using SQL code without needing to move data around or host additional resources.
        + The trained model can be registered in the Model Registry within Vertex AI, making it easy for enterprise users to manage their ML models.
    
- Inference with AlloyDB and Vertex AI
    - After training and deploying a model, users can invoke predictions directly from their existing applications using SQL queries.
        + The Vertex AI Prediction API allows users to specify serving infrastructure options like regional endpoints or multi-region endpoints.
    
- Demonstration of invoking prediction endpoint using AlloyDB and BigQuery
    - The demonstration showed how to create a table containing fraud detection data, load the data into the table, and then use SQL queries to invoke predictions from Vertex AI's fraud detection model.
        + The demo also demonstrated how to batch multiple rows together when invoking predictions, which can improve performance significantly.


